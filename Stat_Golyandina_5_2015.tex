%% LyX 2.1.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[oneside,english,russian]{scrbook}
\usepackage[T2A,T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=2cm,bmargin=2.5cm,lmargin=2cm,rmargin=2cm}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{color}
\usepackage{babel}
\usepackage{array}
\usepackage{verbatim}
\usepackage{latexsym}
\usepackage{float}
\usepackage{booktabs}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{esint}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=true,bookmarksopenlevel=1,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true]
 {hyperref}
\usepackage{breakurl}
\usepackage{bm}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\DeclareRobustCommand{\cyrtext}{%
  \fontencoding{T2A}\selectfont\def\encodingdefault{T2A}}
\DeclareRobustCommand{\textcyr}[1]{\leavevmode{\cyrtext #1}}
\AtBeginDocument{\DeclareFontEncoding{T2A}{}{}}

%% Special footnote code from the package 'stblftnt.sty'
%% Author: Robin Fairbairns -- Last revised Dec 13 1996
\let\SF@@footnote\footnote
\def\footnote{\ifx\protect\@typeset@protect
    \expandafter\SF@@footnote
  \else
    \expandafter\SF@gobble@opt
  \fi
}
\expandafter\def\csname SF@gobble@opt \endcsname{\@ifnextchar[%]
  \SF@gobble@twobracket
  \@gobble
}
\edef\SF@gobble@opt{\noexpand\protect
  \expandafter\noexpand\csname SF@gobble@opt \endcsname}
\def\SF@gobble@twobracket[#1]#2{}
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
 \theoremstyle{definition}
 \newtheorem*{defn*}{\protect\definitionname}
  \theoremstyle{remark}
  \newtheorem*{rem*}{\protect\remarkname}
  \theoremstyle{plain}
  \newtheorem*{cor*}{\protect\corollaryname}
  \theoremstyle{definition}
  \newtheorem*{example*}{\protect\examplename}
  \theoremstyle{remark}
  \newtheorem*{claim*}{\protect\claimname}
  \theoremstyle{definition}
  \ifx\thechapter\undefined
    \newtheorem{example}{\protect\examplename}
  \else
    \newtheorem{example}{\protect\examplename}[chapter]
  \fi
  \theoremstyle{plain}
  \newtheorem*{prop*}{\protect\propositionname}
  \theoremstyle{definition}
  \newtheorem*{xca*}{\protect\exercisename}
  \theoremstyle{plain}
  \newtheorem*{thm*}{\protect\theoremname}
 \newenvironment{solution}
   {\renewcommand\qedsymbol{$\lrcorner$}
    \begin{proof}[\solutionname]}
   {\end{proof}}
  \theoremstyle{definition}
  \ifx\thechapter\undefined
    \newtheorem{xca}{\protect\exercisename}
  \else
    \newtheorem{xca}{\protect\exercisename}[chapter]
  \fi
\newenvironment{lyxcode}
{\par\begin{list}{}{
\setlength{\rightmargin}{\leftmargin}
\setlength{\listparindent}{0pt}% needed for AMS classes
\raggedright
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\normalfont\ttfamily}%
 \item[]}
{\end{list}}
  \theoremstyle{plain}
  \newtheorem*{assumption*}{\protect\assumptionname}
  \theoremstyle{remark}
  \newtheorem*{notation*}{\protect\notationname}

\@ifundefined{date}{}{\date{}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\usepackage{nicefrac}
%\usepackage{colortbl}
%\usepackage[noend]{algpseudocode}
%\usepackage{xypic}
\usepackage{eso-pic}

%\renewenvironment{example*}
%                  {\renewcommand\qedsymbol{$\lrcorner$}
%                   \begin{proof}[Пример]}
%                  {\end{proof}}

%\usepackage[columns=1,itemlayout=singlepar,totoc=true]{idxlayout}

\usepackage[columns=1,itemlayout=singlepar,totoc=true]{idxlayout}

\@addtoreset{chapter}{part}
\DeclareMathOperator{\Int}{Int}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\cdf}{cdf}
\DeclareMathOperator{\ecdf}{ecdf}
\DeclareMathOperator{\qnt}{qnt}
\DeclareMathOperator{\pdf}{pdf}
\DeclareMathOperator{\pmf}{pmf}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\bias}{bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\med}{med}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Ber}{Ber}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Pois}{Pois}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\cor}{cor}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Lin}{Lin}
\DeclareMathOperator{\SE}{SE}
\DeclareMathOperator{\SD}{SD}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\proj}{proj}
\DeclareMathOperator{\colspace}{colspace}
\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator{\corr}{corr}
\DeclareMathOperator{\supp}{supp}

\newcommand{\bigperp}{%
  \mathop{\mathpalette\bigp@rp\relax}%
  \displaylimits
}

\newcommand{\bigp@rp}[2]{%
  \vcenter{
    \m@th\hbox{\scalebox{\ifx#1\displaystyle2.1\else1.5\fi}{$#1\perp$}}
  }%
}

\newcommand{\bignparallel}{%
  \mathop{\mathpalette\bignp@rp\relax}%
  \displaylimits
}

\newcommand{\bignp@rp}[2]{%
  \vcenter{
    \m@th\hbox{\scalebox{\ifx#1\displaystyle2.1\else1.5\fi}{$#1\nparallel$}}
  }%
}

\renewenvironment{cases}{%
 \begin{dcases}%
}{%
 \end{dcases}%\kern-\nulldelimiterspace%
}

\newcommand{\notimplies}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}

\newcommand{\notiff}{%
  \mathrel{{\ooalign{\hidewidth$\not\phantom{"}$\hidewidth\cr$\iff$}}}}

\AtBeginDocument{
  \def\labelitemii{\(\Diamond\)}
  \def\labelitemiii{\(\Box\)}
}

\makeatother

  \addto\captionsenglish{\renewcommand{\assumptionname}{Assumption}}
  \addto\captionsenglish{\renewcommand{\claimname}{Claim}}
  \addto\captionsenglish{\renewcommand{\corollaryname}{Corollary}}
  \addto\captionsenglish{\renewcommand{\definitionname}{Definition}}
  \addto\captionsenglish{\renewcommand{\examplename}{Example}}
  \addto\captionsenglish{\renewcommand{\exercisename}{Exercise}}
  \addto\captionsenglish{\renewcommand{\notationname}{Notation}}
  \addto\captionsenglish{\renewcommand{\propositionname}{Proposition}}
  \addto\captionsenglish{\renewcommand{\remarkname}{Remark}}
  \addto\captionsenglish{\renewcommand{\theoremname}{Theorem}}
  \addto\captionsrussian{\renewcommand{\assumptionname}{Предположение}}
  \addto\captionsrussian{\renewcommand{\claimname}{Утверждение}}
  \addto\captionsrussian{\renewcommand{\corollaryname}{Следствие}}
  \addto\captionsrussian{\renewcommand{\definitionname}{Определение}}
  \addto\captionsrussian{\renewcommand{\examplename}{Пример}}
  \addto\captionsrussian{\renewcommand{\exercisename}{Упражнение}}
  \addto\captionsrussian{\renewcommand{\notationname}{Обозначение}}
  \addto\captionsrussian{\renewcommand{\propositionname}{Предложение}}
  \addto\captionsrussian{\renewcommand{\remarkname}{Замечание}}
  \addto\captionsrussian{\renewcommand{\theoremname}{Теорема}}
  \providecommand{\assumptionname}{Предположение}
  \providecommand{\claimname}{Утверждение}
  \providecommand{\corollaryname}{Следствие}
  \providecommand{\definitionname}{Определение}
  \providecommand{\examplename}{Пример}
  \providecommand{\exercisename}{Упражнение}
  \providecommand{\notationname}{Обозначение}
  \providecommand{\propositionname}{Предложение}
  \providecommand{\remarkname}{Замечание}
  \providecommand{\theoremname}{Теорема}
 \addto\captionsenglish{\renewcommand{\solutionname}{Solution}}
 \addto\captionsrussian{\renewcommand{\solutionname}{Решение}}
 \providecommand{\solutionname}{Решение}

\begin{document}
\global\long\def\N{\mathrm{N}}
\global\long\def\P{\mathsf{P}}
\global\long\def\E{\mathsf{E}}
\global\long\def\D{\mathsf{D}}
\global\long\def\O{\Omega}
\global\long\def\F{\mathcal{F}}
\global\long\def\K{\mathsf{K}}
%\global\long\def\Ascr{\mathscr{A}}
\global\long\def\Ascr{A}
\global\long\def\Pcal{\mathcal{P}}
\global\long\def\th{\theta}
\global\long\def\toas{\xrightarrow{\textrm{a.s.}}}
\global\long\def\toP{\xrightarrow{\P}}
\global\long\def\tod{\xrightarrow{\mathrm{d}}}
\global\long\def\iid{\mathrm{i.i.d.}}
\global\long\def\T{\mathsf{T}}
\global\long\def\L{\mathsf{L}}
\global\long\def\dd#1#2{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
\global\long\def\a{\alpha}
\global\long\def\b{\beta}
\global\long\def\t{\mathrm{t}}
\global\long\def\RR{\mathbb{R}}
\global\long\def\d{\,\mathrm{d}}
\global\long\def\U{\mathrm{U}}
\global\long\def\thb{\boldsymbol{\theta}}
\global\long\def\I{\mathrm{I}}
\global\long\def\II{\mathrm{II}}
\global\long\def\ein{\mathbf{1}}
\global\long\def\pv{p\text{-value}}
\global\long\def\MLE{\mathrm{MLE}}
\global\long\def\indep{\perp\!\!\!\perp}
\global\long\def\xib{\boldsymbol{\xi}}
\global\long\def\Pscr{\mathscr{P}}
\global\long\def\m{\mathsf{m}}
\global\long\def\FWER{\mathrm{FWER}}
\global\long\def\weak{\mathrm{weak}}
\global\long\def\H{\mathbf{H}}
\global\long\def\strong{\mathrm{strong}}
\global\long\def\X{\mathbf{X}}
\global\long\def\bb{\mathbf{b}}
\global\long\def\y{\mathbf{y}}
\global\long\def\eb{\boldsymbol{\epsilon}}
\global\long\def\Db{\boldsymbol{\Delta}}
\global\long\def\M{\mathbf{M}}
\global\long\def\eb{\boldsymbol{\epsilon}}
\global\long\def\S{\mathbf{S}}
\global\long\def\R{\mathbf{R}}
\global\long\def\A{\mathbf{A}}
\global\long\def\B{\mathbf{B}}
\global\long\def\OLS{\mathrm{OLS}}
\global\long\def\mb{\boldsymbol{\mu}}
\global\long\def\Sb{\boldsymbol{\Sigma}}
\global\long\def\Ib{\mathbf{I}}
\global\long\def\SST{\mathrm{SST}}
\global\long\def\SSR{\mathrm{SSR}}
\global\long\def\SSE{\mathrm{SSE}}
\global\long\def\x{\mathbf{x}}
\global\long\def\V{\mathbf{V}}
\global\long\def\etab{\boldsymbol{\eta}}
\global\long\def\Ell{\mathscr{L}}
\global\long\def\p{\mathbf{p}}
\global\long\def\Pb{\mathbf{P}}
\global\long\def\z{\mathbf{z}}
\global\long\def\C{\mathbf{C}}
\global\long\def\W{\mathbf{W}}


 % FIXME: use wallpaper package
\begin{titlepage}
	\AddToShipoutPicture* {
		\put(0, -350) {
			\parbox[b][\paperheight]{\paperwidth} {
				\vfill
				\centering
				\includegraphics[scale = 0.4]{fig/statmod_logo}
				\vfill
			}
		}
	}
\end{titlepage}


\title{Статистика}


\subtitle{Конспект практических занятий}


\author{Мат-Мех, ПМИ, СМ--СМ, конспект семестров 5--8\\
Лекции Голяндиной Н.Э.}


\date{Первоначальный набор текста сделан Дмитрием Зотиковым\\ (NEG: UNDER CONSTRUCTION 01.11.2020)}

\maketitle
\tableofcontents{}


\part{Оценки характеристик и параметров распределения}


\chapter{Выборка и эмпирическая случайная величина}

\index{3. Что такое повторная независимая выборка (два определения)}

Пусть $\xi:\big(\Omega,\F,\P\big)\to\big(V,\mathfrak{A}\big)$ --- случайная величина с распределением $\Pcal$ (пишем $\xi\sim\Pcal$).
\begin{defn*}
\emph{Повторной независимой} \emph{выборкой} объема $n$ \emph{(до
эксперимента)} называется набор
\[
\mathbf{x}=\left(x_{1},\ldots,x_{n}\right),\quad x_{i}\sim\Pcal\ \forall i\in1:n,\ x_{1}\indep\dots\indep x_{n}
\]
 независимых в совокупности одинаково распределенных случайных величин
с распределением $\Pcal$. (Знак $\indep$ иногда используется для обозначения независимости случайных величин.)%
\begin{comment}
Повторная: одна и та же с.в.; независимая: независимые проявления.
<<Признак>>

Пример: рост.
\begin{itemize}
\item зависимая: внутри семьи
\item неповторая: рост черепах и крокодилов (<<неоднородность>>); разные
результаты в разные инстансы эксперимента.\end{itemize}
\end{comment}

\end{defn*}

\begin{defn*}
\emph{Повторной независимой выборкой} объема $n$\emph{ (после эксперимента)
}называется набор реализаций, т.е. конкретных значений $\xi$, случайных
величин $x_{i}$:
\[
\mathbf{x}=\left(x_{1},\ldots,x_{n}\right),\quad x_{i}\in V \forall i\in1:n.
\]

\end{defn*}

\begin{rem*}
Подходящее определение выбирается по контексту.
\end{rem*}

\begin{defn*}
\emph{Эмпирической случайной величиной} $\hat{\xi}_{n}$ называется
случайная величина с дискретным распределением
\[
\hat{\xi}_{n}\sim\hat{\Pcal}_{n}:\begin{pmatrix}x_{1} & \dots & x_{n}\\
1/n & \dots & 1/n
\end{pmatrix}.
\]
\end{defn*}


Если $\xi$ имеет дискретное распределение, то выборку можно \emph{сгруппировать};
тогда распределение случайной величины $\hat{\xi}_{n}$ запишется как
\[
\hat{\Pcal}_{n}:\begin{pmatrix}x_{1}^{*} & \dots & x_{m}^{*}\\
\omega_{1} & \dots & \omega_{m}
\end{pmatrix}\quad\omega_{i}=\frac{\nu_{i}}{n},
\]
 где $x_{i}^{*}$ --- уникальные значения из выборки $\mathbf{x}$,
а $\nu_{i}$ --- число $x_{i}^{*}$ в $\mathbf{x}$ (т.н. <<абсолютная
частота>>; тогда $\omega_{i}$ --- <<относительная частота>>).
В противном случае, можно разбить интервал всевозможных значений выборки
на $m$ подынтервалов: $\left\{ [e_{0},e_{1}),\ldots,[e_{m-1},e_{m})\right\} $
и считать число наблюдений $\nu_{i}=\nu_{i}[e_{i-1},e_{i})$, попавших
в интервал.
\begin{cor*}
По ЗБЧ (теореме Бернулли),
\[
\omega_{i}\toP p_{i}=\P(e_{i-1}\leq\xi<e_{i}),
\]
 т.е. относительная частота является хорошей оценкой вероятности на
больших объемах выборки.
\end{cor*}

Выше используется сходимость по вероятности (P от слова probability). Обозначение $\zeta_n \toP \zeta$ означает, что
$\forall \varepsilon > 0,\; \lim\limits_{n \to \infty} \P(|\zeta_n - \zeta| > \varepsilon) = 0$.

\chapter{Виды признаков}

Виды признаков случайной величины $\xi:\big(\Omega,\F,\P\big)\to\big(V,\mathfrak{A}\big)$
характеризуются тем, что из себя представляет множество $V$ и что
можно делать с его элементами.
\begin{description}
\item [{Количественные~признаки:}] $V\subset\mathbb{R}$, заданы операции с вещественными числами. %
\begin{comment}
(хотя бы идейно)
\end{comment}



По типу операций:
\begin{itemize}
\item Аддитивные: заданы, т.е. имеют смысл в контексте данного признака,
операции $+,-$. Разница между значениями характеризуется разностью значений.
\item Мультипликативные: заданы операции $\times,/$; признак принимает не
отрицательные значения. Разница между значениями измеряется в процентах (определяется делением).
\end{itemize}

По типу данных:
\begin{itemize}
\item Непрерывные
\item Дискретные
\end{itemize}
\item [{Порядковые~признаки}] $V$ --- упорядоченное множество, определены
отношения $>,=, <$.
\item [{Качественные~признаки}] на $V$ заданы отношения $=,\neq$

\begin{example*}
Цвет глаз, имена, пол.
\end{example*}
\end{description}
\begin{comment}
@xio: FIXME: ящики с усами, гистограмма, график фр und so weiter
\end{comment}

















\chapter{Характеристики распределений и метод подстановки}
\begin{defn*}
\emph{Статистика} --- измеримая функция от выборки.
\end{defn*}

\begin{defn*}
\emph{Характеристика} распределения --- функционал от распределения:
\[
T:\left\{ \Pcal\right\} \to D;
\]
Чаще всего, $D=\RR$.
%\begin{defn*}
%Выделяют \emph{генеральные} характеристики $T(\Pcal)=:\th$ и \emph{выборочные}
%характеристики $T(\hat{\Pcal}_{n})$.
\end{defn*}

\begin{defn*}
\emph{Оценка $\hat\th = \hat\th(x_1,\ldots,x_n)$ характеристики $\th$} --- функция от выборки,
не зависящая от этой характеристики.\end{defn*}
%\begin{cor*}
%Выражения для вычисления генеральных и выборочных характеристик отличаются
%только используемыми мерами ($\Pcal$ и $\hat{\Pcal}_{n}$ соответственно).\end{cor*}
\begin{defn*}
Пусть $\hat{\Pcal}_{n}$ --- распределение эмпирической случайной
величины. Тогда \emph{эмпирическая функция распределения} есть
\[
\widehat{\cdf}_{\xi}(x)=\cdf_{\hat{\xi}_{n}}(x)=\hat{\Pcal}_{n}((-\infty,x))=\int_{-\infty}^{x}\d\hat{\Pcal}_{n}=
\sum_{{i}:x_{i}\leq x}\frac{1}{n}=\frac{\left|\left\{ x_{i}\in\mathbf{x}:x_{i}<x\right\} \right|}{n}.
\]

\end{defn*}

Здесь используется обозначение $\cdf$, сокращение от cumulative distribution function, т.е. от названия функции распределения. Мы здесь оставим это обозначение, однако часто на занятиях будем обозначать функцию распределения просто $F(x)$ или $F_\xi(x)$, чтобы подчеркнуть, какой случайной величины это функция распределения.

\begin{claim*}
Пусть $\widehat{\cdf}_{\xi}$ --- эмпирическая функция распределения,
$\cdf_{\xi}$ --- функция распределения $\xi$. Тогда, по теореме
Гливенко-Кантелли,
\[
\sup_{x}\left|\widehat{\cdf}_{\xi}(x)-\cdf_{\xi}(x)\right|\toas 0
\]
(сходимость a.s. означает сходимость almost surely, почти наверно, почти всегда; она вводится для случайных величин, заданных на вероятностном пространстве и означает, что сходимость имеет место для почти всех элементарных событий кроме, может быть, событий меры ноль).

Может возникнуть вопрос, почему слева от стрелки случайная величина. Дело в том, что в этом утверждении, как и в любом теоретическом утверждении в математической статистике, выборка понимается как <<до эксперимента>>, т.е. все $x_i$ --- случайные величины, поэтому эмпирическая функция распределения в точке $x$, равная числу $x_i$, меньших $x$, тоже является случайной.

Более того, если $\cdf_{\xi}$ непрерывна, скорость сходимости имеет порядок
$1/\sqrt{n}$ по теореме Колмогорова:
\[
\sqrt{n}\sup_{x\in\RR}\left|\widehat{\cdf}_{\xi}(x)-\cdf_{\xi}(x)\right|\tod\Pcal_{\mathrm{K.S.}},
\]
где $\Pcal_{\mathrm{K.S.}}$ --- распределение Колмогорова-Смирнова.\end{claim*}

Выше используется другой тип сходимости, более слабый, по распределению (d от слова distribution). Эта сходимость означает, что функция распределения случайной величины слева сходится в функции распределения, указанного справа, во всех точках ее непрерывности.

\begin{rem*}
Поскольку $\widehat{\cdf}_{\xi}(x)=\omega_{x}$, где $\omega_{x}$
--- относительная частота попадания наблюдений в интервал в $\left(-\infty,x\right)$,
а $\cdf_{\xi}(x)=\P\left(\xi\in\left(-\infty,x\right)\right)$ ---
вероятность того же события, то можно применить теорему Бернулли (ЗБЧ):
\[
\widehat{\cdf}_{\xi}(x)\toP\cdf_{\xi}(x).
\]
\end{rem*}
\begin{cor*}
Значит, при достаточно больших $n$, в качестве интересующей характеристики
$\th=f(\xi)$ распределения $\Pcal_\xi$ можем брать ее оценку $\hat{\th}=\hat{\th}_{n}=f(\hat\xi_n)$
--- аналогичную характеристику $\hat{\Pcal}_{n}$. Этот метод называется методом подстановки.
\end{cor*}

\chapter{Характеристики распределений и их оценки}
\begin{defn*}
Генеральные и соответствующие им выборочные характеристики \emph{$k$-го
момента} и \emph{$k$-го центрального момента}:
\begin{align*}
\m_{k} & =\int_{\RR}x^{k}\d\Pcal & \hat{\m}_{k} & =\int_{\RR}x^{k}\d\hat{\Pcal}_{n}=\frac{1}{n}\sum_{i=1}^{n}x_{i}^{k}\\
\m_{k}^{(0)} & =\int_{\RR}\left(x-\m_{1}\right)^{k}\d\Pcal & \hat{\m}_{k}^{(0)} & =\int_{\RR}(x-\hat{\m}_{1})^{k}\d\hat{\Pcal}_{n}=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\hat{\m}_{1})^{k}.
\end{align*}

\end{defn*}

\section{Характеристики положения}

В качестве характеристики положения для количественных признаков выделяется 1-й момент --- математическое
ожидание и его оценка \emph{выборочное} \emph{среднее}:
\[
\m_{1}=\E\xi,\qquad\hat{\m}_{1}=:\bar{x}=\widehat{\E\xi}=\E\hat{\xi}_{n}.
\]

\begin{rem*}
В случае мультипликативных признаков можно посчитать среднее геометрическое;
часто логарифмируют и считают среднее арифметическое.\end{rem*}
\begin{defn*}
Пусть $p\in\left[0,1\right]$ и $\cdf=\cdf_{\Pcal}$. \emph{$p$-квантилью}
(квантилью уровня $p$) называется
\[
\qnt_{\Pcal}(p)=:z_{p}=\sup\left\{ z:\cdf(z)\leq p\right\} .
\]
\emph{Квартиль} есть квантиль уровня, кратного $1/4$; дециль ---
$1/10$; перцентиль --- $1/100$.
Эти характеристики определены для порядковых признаков (и, следовательно, для количественных тоже).\end{defn*}
\begin{rem*}
$\sup$ берется для учета случая не непрерывных функций распределения.
\end{rem*}

\begin{defn*}
\emph{Медиана} есть $0.5$-квантиль:
\[
\med\xi=z_{1/2}.
\]

\end{defn*}

\begin{defn*}
Мода ($\mathrm{mode}\,\xi$) есть точка локального максимума плотности или состояние с максимальной вероятностью для качественных признаков.
\end{defn*}
По методу подстановки можем получить аналогичные выборочные характеристики.
\begin{defn*}
\emph{Выборочная $p$-квантиль} есть такая точка $\hat{z}_{p}$, что
она больше по значению $\left|\mathbf{x}\right|\cdot p=np$ точек
из выборки:
\[
\hat{z}_{p}=\sup\left\{ z:\widehat{\cdf}_{\xi}(z)\leq p\right\} =x_{(\left\lfloor np\right\rfloor +1)}.
\]

\end{defn*}

\begin{defn*}
\emph{Выборочная медиана} упорядоченной выборки $\mathbf{x}=\left(x_{(1)},\ldots,x_{(n)}\right)$
есть\emph{
\[
\hat{z}_{1/2}=\widehat{\med}=\begin{cases}
x_{(k+1)} & n=2k+1\\
\frac{x_{(k)}+x_{(k+1)}}{2} & n=2k
\end{cases}
\]
}
\end{defn*}

\begin{defn*}
\emph{Выборочная мода} ($\widehat{\mathrm{mode}}$) есть значение
из выборки, которое чаще всего встречается.
\end{defn*}

\section{Характеристики разброса}

В качестве характеристики разброса выделяется 2-й центральный момент
--- дисперсия и выборочная дисперсия:
\[
\m_{2}^{(0)}=\D\xi\qquad\hat{\m}_{2}^{(0)}=:s^{2}=\widehat{\D\xi}=\D\hat{\xi}_{n}=\begin{cases}
\E\left(\hat{\xi}_{n}-\E\hat{\xi}_{n}\right)^{2}=\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}\\
\E\hat{\xi}_{n}^{2}-\left(\E\hat{\xi}_{n}\right)^{2}=\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\right)-\bar{x}^{2}.
\end{cases}
\]

\begin{rem*}
Если среднее $\E\xi=\mu$ известно, то дополнительно вводится
\[
s_{\mu}^{2}:=\begin{cases}
\frac{1}{n}\sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}\\
\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}\right)-\mu^{2}.
\end{cases}
\]
\begin{comment}
TODO: R считает $\tilde{s}^{2}$.
\end{comment}
\end{rem*}
\begin{example*}[Оценка дисперсии оценки мат. ожидания]
\phantom{a}

Вопрос: $\E\xi = ?$  Ответ $\widehat\E\xi = \bar{x}$.

Вопрос: насколько точна оценка? Ответ: $\D\bar{x} = \frac{\D\xi}{n}$.

Вопрос: а как по выборке оценить эту точность оценки?  Ответ: $\widehat{\D\bar{x}}=\frac{s^{2}}{n}$.

Получили оценку дисперсии оценки математического ожидания.

%Может интересовать точность построенной оценки. Вычислим дисперсию
%теоретически, после чего оценим точность по выборке:
%\[
%\D\bar{x}=\D\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}\right)=\frac{1}{n^{2}}\sum_{i=1}^{n}\D x_{i}=\frac{1}{n^{2}}\sum_{i=1}^{n}\D\xi=\frac{\D\xi}{n},
%\]
% откуда
%\[
%\widehat{\D\bar{x}}=\frac{s^{2}}{n}.
%\]

\end{example*}

%\begin{example*}[Дисперсия оценки дисперсии]
%См. по ссылке\footnote{\url{http://mathworld.wolfram.com/SampleVarianceDistribution.html}}.
%\end{example*}

\paragraph{Мера разброса для качественных признаков.}
\begin{defn*}[Энтропия]
%\emph{}Количество информации, необходимое для выявления объекта из
%$m$-элементного множества вычисляется по \emph{формуле Хартли}:
%\[
%H=\log_{2}m
%\]
% (множество это следует итеративно разбивать пополам, откуда и оценка).
Для дискретного распределения
\[
\Pcal_{\xi}:\begin{pmatrix}x_{1} & \dots & x_{m}\\
p_{1} & \dots & p_{m}
\end{pmatrix}.
\]
\emph{энтропией} (мера `размазанности' распределения) вычисляется как :
\[
H(\xi)=\sum_{i=1}^{m}p_{i}\log_{2}\frac{1}{p_{i}}.
\]
\end{defn*}
\begin{rem*}
Минимальная мера разброса, если .... В случае равномерного дискретного распределения энтропия максимальна.
\begin{comment}
Примеры, условная энтропия: Алексеева--44.
\end{comment}

\end{rem*}

\paragraph{SE и SD}
\begin{defn*}
\emph{Выборочное стандартное отклонение} есть
\[
\SD:=\sqrt{\widehat{\D\xi}}=s.
\]
Это показатель разброса случайной величины; показатель того, насколько
элементы выборки отличаются от выборочного среднего по значению.
\end{defn*}
$\SD$ позволяет оценивать стандартное отклонение распределения $\xi$.

Пусть $\hat{\th}_{n}$ --- статистика. Она имеет какое-то своё распределение,
стандартное отклонение которого можно также оценить.
\begin{defn*}
\emph{Стандартная ошибка} оценки есть
\[
\SE(\hat{\th}):=\sqrt{\widehat{\D\hat{\th}}}.
\]
Это показатель разброса оценки случайной величины.\end{defn*}
\begin{rem*}
В частном случае $\th=\E\xi$, $\hat{\th}=\bar{x}$ получаем
\emph{выборочную стандартную ошибку среднего}
\[
\SE:=\SE(\bar{x})=\sqrt{\widehat{\D\bar{x}}}=\sqrt{\widehat{\frac{\D\xi}{n}}}=\frac{s}{\sqrt{n}}.
\]
 Это, в свою очередь, показатель того, насколько выборочное среднее
отличается от истинного.
\end{rem*}
Пусть $c_{\gamma}=\qnt_{\N(0,1)}\gamma.$  $\N(\mu,\sigma^2)$ обозначает нормальное распределение с математическим ожиданием $\mu$ и дисперсией $\sigma^2$.

\begin{example*}[С мостом и машинами]
При возведении моста требуется, чтобы под ним могли проехать, условно,
95\% машин. Чтобы эту высоту вычислить, достаточно собрать выборку
высоты кузова проезжающих машин. Тогда нахождение искомой величины
можно наглядно представить как выбор такой квантили гистограммы выборки,
что суммирование соответствующих вероятностей даст $\gamma=0.95$. В предположении,
что выборка из нормального распределения, с более устойчивой оценкой
квантили, интервал будет иметь вид
\[
\left(\bar{\x}\pm\SD\cdot c_{\gamma}\right).
\]
$\SE$ как показатель разброса выборочного среднего использовать по смыслу нельзя.
\end{example*}

\begin{example*}[С паромом]
Число машин, которое способен перевезти паром, есть $\text{Грузоподъемность}/\E\xi,$
где $\xi$ --- вес машины. Поскольку оценка $\bar{x}$ всегда
считается с погрешностью относительно истинного значения, интервал
допустимого числа машин будет иметь вид
\[
\frac{\text{Грузоподъемность}}{\bar{x}\pm\SE\cdot c_{\gamma}}.
\]

\end{example*}

Подробности, включая определение $c_\gamma$ см. в разделе посвященном доверительным интервалам (Глава~\ref{ch:confidence}).


\section{Характеристики формы распределения}

Для удобства, обозначим $\sigma^2 = \m_{2}^{(0)}= \D\xi$.

\begin{defn*}
\emph{Коэффициент асимметрии Пирсона} (<<скошенности>>\footnote{<<Skewness>>.})
\[
\gamma_{3}=\mathsf{A}\xi=\frac{\m_{3}^{(0)}}{\sigma^{3}}.
\]
\begin{comment}
\begin{rem*}
Поделили на $(\D\xi)^{3/2}$ для независимости от разброса --- при
домножении числителя на $n$, вылезает 3-я степень.\end{rem*}
\end{comment}
\end{defn*}
\begin{rem*}
Не зависит от линейных преобразований.
\end{rem*}
\begin{rem*}
Старое определение скошенности было $\frac{\E\xi-\med\xi}{\sigma}$.
\end{rem*}

\begin{rem*}
Типичный случай соответствует тому, что при положительном коэффициенте асимметрии `хвост вправо'.
\end{rem*}

\begin{defn*}
\emph{Коэффициент эксцесса} (<<крутизны>>, <<kurtosis>>):
\[
\gamma_{4}=\K\xi=\frac{\m_{4}^{(0)}}{\sigma^{4}}-3.
\]
\end{defn*}
\begin{rem*}
Величина $\m_{4}^{(0)}/\sigma^{4}=3$ соответствует стандартному нормальному
распределению. Так что можно сравнивать выборку и $\gamma_{4}$ для $\N(0,1)$.
\end{rem*}

\begin{rem*}
Положительный коэффициент эксцесса соответствует медленному убыванию на концах отрезка. Причём, так как распределение стандартизуется, имеется в виду убывание на хвостах, которое медленнее по порядку (!), чем убывание на хвостах у нормального распределения. Например, сравните $e^{-x^2}$, $e^{-x^2/10}$ и $e^{-10x}$.
Часто говорят об островершинности при положительном эксцессе, но это просто вторая сторона скорости убывания на хвостах. Медленное убывание на хвостах означает на практике, что далекие от среднего значения встречаются необычно часто.
\end{rem*}


%\begin{rem*}
%При замене $z:=(\xi-\E\xi)/\sigma$ величину $\m_{4}^{(0)}/\sigma^{4}=\E(z^{4})$
%можно интерпретировать как ожидание четвертой степени центрированных
%и нормированных данных. Точки выборки, лежащие внутри $\E\xi\pm\sigma$
%из-за малости по модулю не будут увеличивать значение коэффициента,
%в то время как аутлаеры будут или <<тяжелые хвосты>> плотности распределения
%будут. Поэтому $\gamma_{4}$ принимает большие значения на распределениях
%с <<тяжелыми хвостами>> или выборках с некоторым количеством аутлаеров.
%\end{rem*}
%
%\begin{rem*}
%Справедлива оценка
%\[
%\gamma_{3}^{2}+4\leq\gamma_{4}+3\leq\infty,
%\]
% где минимум достигается $\Ber(1/2)$.
%\end{rem*}

\section{Характеристики зависимости}
\begin{defn*}
Пусть $(\xi_{1},\xi_{2})^\mathrm{T} \sim\Pcal$ и $(x_{1},y_{1})^\mathrm{T},\ldots,(x_{n},y_{n})^\mathrm{T}\sim\Pcal$ --- выборка из этого распределения.
Тогда можно записать две другие важные характеристики: \emph{ковариацию}
и \emph{коэффициент корреляции}:
\begin{align*}
\cov(\xi_{1},\xi_{2}) &
=\E(\xi_1-\E\xi_1)(\xi_2-\E\xi_2)=\E\xi_1\xi_2 - \E\xi_1\E\xi_2
%=\iint_{\RR^{2}}(u-\m_{1}(\xi_1))(v-\m_{1}(\xi_2))\Pcal(du\times dv)
& \widehat{\cov}(\mathbf{x},\mathbf{y}) & =\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})\\
\cor(\xi_{1},\xi_{2}) & =\frac{\cov(\xi_{1},\xi_{2})}{\sigma_{\xi_{1}}\sigma_{\xi_{2}}} & \widehat{\cor}(\mathbf{x},\mathbf{y}) & =\frac{\widehat{\cov}(\mathbf{x},\mathbf{y})}{s(\mathbf{x})s(\mathbf{y})}.
\end{align*}
\end{defn*}
%\begin{rem*}[Важное]
%$\xi_{1}\nparallel\xi_{2}\implies\cov(\xi_{1},\xi_{2})$, но $\cov(\xi_{1},\xi_{2})\notimplies\xi_{1}\nparallel\xi_{2}$.
%Необходимость и достаточность выполняется только в случае нормального
%распределения. %
\begin{comment}
@xio: иллюстрация?
\end{comment}
%
%\end{rem*}

%\begin{rem*}[Проблема моментов]
%Для заданной последовательности моментов $\m_{1},\m_{2},\ldots$
%не обязано существовать подходящее распределение. Помимо требований
%$\m_{2k}\geq0$ и взаимосвязи между соседними моментами по неравенству
%Гёльдера, существенно, что ряд Тейлора по $\m_{\ell}$, в который,
%как известно, раскладывается характеристическая функция, должен сходиться
%равномерно.
%\end{rem*}

\chapter{Точечная оценка параметров распределения}

\section{Метод подстановки}
Метод подстановки заключается в подстановке вместо неизвестного теоретического распределения известного эмпирического распределения. Например, вас интересует некоторая характеристика $f(\xi)$, а вы в качестве оценки предлагаете $\widehat{f(\xi)} = f(\xi_n)$, где $\xi_n=\hat\xi_n$ --- эмпирическая случайная величина.

\section{Метод моментов}

Пусть $\Pcal(\boldsymbol{\theta}),\ \boldsymbol{\theta}=\left(\theta_{1},\ldots,\theta_{r}\right)^{\T}$
--- параметрическая модель. Найдем оценки для параметров $\hat{\theta}_{i},\ i\in\overline{1:r}$,
для чего составим и решим систему уравнений:
\[
\left\{ \begin{aligned}\E g_{1}(\xi) & =\phi_{1}(\theta_{1},\ldots,\theta_{r})\\
 & \vdots\\
\E g_{r}(\xi) & =\phi_{r}(\theta_{1},\ldots,\theta_{r})
\end{aligned}
\right.\implies\left\{ \begin{aligned}\theta_{1} & =f_{1}(\E g_{1}(\xi),\ldots,\E g_{r}(\xi))\\
 & \vdots\\
\theta_{r} & =f_{r}(\E g_{1}(\xi),\ldots,\E g_{r}(\xi)).
\end{aligned}
\right.
\]
Примем
\[
\theta_{i}^{*}=f_{i}(\hat{\E}g_{1}(\xi),\ldots,\hat{\E}g_{r}(\xi)).
\]
 Часто, $g_{i}(\xi)=\xi^{i}$. Или, еще чаще, $g_{1}(\xi)=\xi$ и $g_{i}(\xi)=(\xi-\E\xi)^{i}$, $i>1$, так как для таких моментов обычно известны формулы.


\begin{rem*}
Случается, что решение находится вне пространства параметров. На практике,
если пространство параметров компактное, можно взять точку, ближайшую
к полученной оценке. Однако это свидетельствует о том, что модель
плохо соответствует данным.\end{rem*}
\begin{example}[$r=1$]
$\xi\sim\U(0,\th)$.
\begin{itemize}
\item Оценка по 1-му моменту: $g(\xi)=\xi$ и
\[
\E\xi=\int_{0}^{\th}\frac{1}{\th}x\d x=\frac{1}{\th}\left.\frac{x^{2}}{2}\right\vert _{0}^{\th}=\frac{\th}{2}\implies\th=2\E\xi,\ \th^{*}=2\bar{x}.
\]

\item Оценка по $k$-му моменту: $g(\xi)=\xi^{k}$ и
\[
\E\xi^{k}=\frac{1}{\th}\int_{0}^{\th}x^{k}\d x=\frac{1}{\th}\left.\frac{x^{k+1}}{k+1}\right\vert _{0}^{\th}=\frac{\th^{k}}{k+1}\implies\th^{*}=\sqrt[k]{(k+1)\frac{1}{n}\sum_{i=1}^{n}x_{i}^{k}}.
\]

\end{itemize}
\end{example}

\begin{example}[$r=1$]
Пусть $\xi\sim\Exp(\lambda)$. Тогда $\E\xi=\lambda$ и $\bar{x}=\lambda$.
\end{example}

%\begin{example}[$r=2$]
%Пусть $\Pcal_{\xi}(\theta_{1},\theta_{2})=\Bin(m,p)$. Тогда $g_{1}(\xi)=\xi$,
%$g_{2}(\xi)=(\xi-\E\xi)^{2}$ и
%\[
%\left\{ \begin{aligned}\E\xi & =mp\\
%\D\xi & =mp(1-p)
%\end{aligned}
%\right.\quad\left\{ \begin{aligned}m & =\frac{\E\xi}{p}\\
%\D\xi & =\E\xi-\E\xi p
%\end{aligned}
%\right.\quad\left\{ \begin{aligned}p & =\frac{\E\xi-\D\xi}{\E\xi}\\
%m & =\frac{\left(\E\xi\right)^{2}}{\E\xi-\D\xi}
%\end{aligned}
%\right.\implies\left\{ \begin{aligned}\hat{p} & =\frac{\bar{x}-s^{2}}{\bar{x}}\\
%\hat{m} & =\frac{\bar{x}^{2}}{\bar{x}-s^{2}}.
%\end{aligned}
%\right.
%\]
%
%\end{example}

\section{Метод оценки максимального правдоподобия}

Пусть $\Pcal_{\xi}(\boldsymbol{\theta}),\ \boldsymbol{\theta}=\left(\theta_{1},\ldots,\theta_{r}\right)^{\T}$
--- параметрическая модель.
\begin{defn*}
Пусть
\[
\P(\mathbf{y}\mid\boldsymbol{\theta})=\begin{cases}
\P_{\boldsymbol{\theta}}(x_{1}=y_{1},\ldots,x_{n}=y_{n}) & \Pcal_{\xi}(\boldsymbol{\theta})\text{ дискретно};\\
p_{\boldsymbol{\theta}}(\mathbf{y}) & \Pcal_{\xi}(\boldsymbol{\theta})\text{ абсолютно непрерывно.}
\end{cases}
\]
Тогда \emph{функция правдоподобия} определяется как значение распределения выборки (плотности в непрерывном случае и вероятности значений в дискретном) с подстановкой выборки вместо аргумента:
\[
\L(\boldsymbol{\theta}\mid\mathbf{x})=\P(\mathbf{x}\mid\boldsymbol{\theta}).
\]
\end{defn*}
\begin{example}
Пусть $\xi\sim\N(\mu,\sigma^{2})$. По независимости $x_{i}$, $p_{\boldsymbol{\theta}}(\mathbf{x})$
распадается в произведение:
\[
\L(\boldsymbol{\theta}\mid\mathbf{x})=p_{\boldsymbol{\theta}}(\mathbf{x})=\prod_{i=1}^{n}p_{\boldsymbol{\theta}}(x_{i})=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{ -\frac{(x_{i}-\mu)^{2}}{2\sigma^{2}}\right\} =\frac{1}{\left(2\pi\right)^{n/2}\sigma^{n}}\exp\left\{ -\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu)^{2}\right\} .
\]

\end{example}

\begin{example}
$\xi\sim\Pois(\lambda)$,
\[
\P(\xi=k)=\frac{\lambda^{k}}{k!}e^{-\lambda}\implies\L(\boldsymbol{\theta}\mid\mathbf{x})=\prod_{i=1}^{n}\frac{1}{x_{i}!}\lambda^{x_{i}}e^{-\lambda}=\frac{1}{\prod_{i=1}^{n}x_{i}!}\lambda^{n\bar{x}}e^{-n\lambda}.
\]
\end{example}
\begin{claim*}
Пусть $\mathbf{x}$ --- выборка. В качестве оценки максимального правдоподобия\footnote{\selectlanguage{english}%
Maximum likelihood estimate\foreignlanguage{russian}{ (MLE).}\selectlanguage{russian}%
} $\hat{\boldsymbol{\theta}}_{\mathrm{MLE}}$ следует взять
\[
\hat{\boldsymbol{\theta}}_{\mathrm{MLE}}=\argmax_{\boldsymbol{\theta}}\ln\L(\boldsymbol{\theta}\mid\mathbf{x}).
\]
\end{claim*}
\begin{example*}
\label{exa:mle-pois}$\xi\sim\Pois(\lambda)$.
\[
\ln\L(\lambda\mid\mathbf{x})=-\sum_{i=1}^{n}\ln(x_{i}!)-n\lambda+n\bar{x}\ln\lambda\implies
\frac{\partial\ln\L(\lambda\mid\mathbf{x})}{\partial\lambda}=-n+\frac{n\bar{x}}{\lambda}
\]
 откуда
\[
\frac{\partial\ln\L(\lambda\mid\mathbf{x})}{\partial\lambda}=0\iff-n+\frac{n\bar{x}}{\lambda}=0,\ n\bar{x}-n\lambda=0,\ \lambda=\bar{x}.
\]

\end{example*}

\begin{claim*}
В условиях регулярности%
\begin{comment}
\footnote{Условия регулярности таковы:
\begin{itemize}
\item $\left\{ x:p_{\th}(x)>0\right\} $ не зависит от $\th$,
\item $\int_{V^{n}}\hat{\th}\L(\th\mid\mathbf{x})\d\mathbf{x}$ и $\int_{V^{n}}\L(\th\mid\mathbf{x})\d\mathbf{x}$
можно дифференцировать по $\th$ под знаком интеграла,
\item $I_{n}(\th)>0$.\end{itemize}
}
\end{comment}
:
\begin{enumerate}
\item Существует один глобальный максимум, так что
\[
\left.\frac{\partial\ln\L(\lambda\mid\mathbf{x})}{\partial\lambda}\right\vert _{\boldsymbol{\theta}=\hat{\boldsymbol{\theta}}_{\mathrm{MLE}}}=0.
\]

\item $\hat{\boldsymbol{\theta}}_{\mathrm{MLE}}$ обладает всеми свойствами (про определение этих свойст написано в следующих разделах):

\begin{enumerate}
\item Состоятельность;
\item Асимптотическая несмещенность;
\item Асимптотическая нормальность;
\item Асимптотическая эффективность\emph{.}%
\begin{comment}
\emph{FIXME: асимптотическа}
\end{comment}

\end{enumerate}
\end{enumerate}
\end{claim*}

\chapter{Свойства оценок}

Полезное свойство для любой случайной величины $\zeta$:
\begin{equation}
\label{eq:decomp}
\E(\zeta - a)^2 = \D\zeta + (\E\zeta - a)^2.
\end{equation}
Доказывается легко: $\E(\zeta - a)^2 = \E((\zeta - \E\zeta)+(\E\zeta-a))^2 = ...$.

\section{Несмещенность}
\begin{defn*}
\emph{Смещение}\footnote{\selectlanguage{english}%
Bias\foreignlanguage{russian}{.}\selectlanguage{russian}%
} есть
\[
\bias\hat{\theta}_{n}:=\E\hat{\th}_{n}-\th\quad\forall\theta\in\Theta.
\]

\end{defn*}

\begin{defn*}
\emph{Среднеквадратичная ошибка}\footnote{\selectlanguage{english}%
Mean squared error (MSE).\selectlanguage{russian}%
} есть
\[
\MSE\hat{\theta}_{n}:=\E(\hat{\th}_{n}-\th)^{2}.
\]
\end{defn*}
\begin{rem*}
В \eqref{eq:decomp} берем $\zeta = \hat{\th}_{n}$, $a=\th$ и получим
\begin{equation}
\label{eq:MSE}
\underbrace{\E(\hat{\th}_{n}-\th)^{2}}_{\mathrm{MSE}}=\D\hat{\th}_{n}+\underbrace{(\E(\hat{\th}_{n}-\th))^{2}}_{\mathrm{bias}^{2}}.
\end{equation}
\end{rem*}
\begin{defn*}
Оценка называется \emph{несмещенной}, если $\bias\hat{\theta}_{n}=0$,
т.е.
\[
\E\hat{\th}_{n}=\th.
\]
\end{defn*}
\begin{prop*}
$\bar{x}$ --- несмещенная оценка $\E\xi$.\end{prop*}
\begin{proof}
Пусть $\th=\E\xi,\ \hat{\th}_{n}=\hat\E{\xi}_{n}=\bar{x}$.
Тогда
\[
\E\bar{x}=\E\frac{1}{n}\sum_{i=1}^{n}x_{i}=\frac{1}{n}\sum_{i=1}^{n}\E x_{i}=\frac{1}{n}\sum_{i=1}^{n}\E\xi=\E\xi\implies\E\hat{\th}_{n}=\E\th,\ \bias\hat{\theta}_{n}=0.
\]

\end{proof}
\begin{comment}
TODO: В принципе, $\hat{\m}_{k}$ несмещенная
\end{comment}

\begin{prop*}
\label{cl:S2-asympt-consistent}$s^{2}$ является только \emph{асимптотически}
несмещенной оценкой $\D\xi$.\end{prop*}
\begin{proof}
%Это доказательство `в лоб'. Хорошее доказательство на основе разложения $\E(\zeta-a)^2$ смотрите в конспекте.
В \eqref{eq:decomp} берем $\zeta = \hat{\xi}_{n}$, $a=\E\xi$ и выразим дисперсию; получим для $s^{2}=\hat\D\xi_n=\frac{1}{n}\sum_{i=1}^n (x_i-\bar{x})^2$:
$$s^{2}  =  \frac{1}{n}\sum_{i=1}^n (x_i-\E\xi)^2 - (\bar{x}-\E\xi)^2$$.

Берем мат.ож. и получаем  (так как $\E\bar{x} = \E\xi$):
\begin{eqnarray*}
\E s^{2} & = & \frac{1}{n}\sum_{i=1}^n \E(x_i-\E\xi)^2 - \E(\bar{x}-\E\xi)^2 %\E\widehat{\D\xi}=\E\widehat{\D\eta}=\E\left(\widehat{\E\eta^{2}}-\left(\widehat{\E\eta}\right)^{2}\right)=\E\left(\frac{1}{n}\sum_{i=1}^{n}y_{i}^{2}-\left(\frac{1}{n}\sum_{i=1}^{n}y_{i}\right)^{2}\right)\\
% & = & \E\left(\frac{1}{n}\sum_{i=1}^{n}y_{i}^{2}-\frac{1}{n^{2}}\sum_{i,j=1}^{n}y_{i}y_{j}\right)=\E\left(\frac{1}{n}\sum_{i=1}^{n}y_{i}^{2}-\frac{1}{n^{2}}\sum_{i,j=1}^{n}y_{i}^{2}\right)=\frac{1}{n}\sum_{i=1}^{n}\E y_{i}^{2}-\frac{1}{n^{2}}\sum_{i=1}^{n}\E y_{i}^{2}\\
% & = & \frac{1}{n}\sum_{i=1}^{n}\E\left(x_{i}-\E\xi\right)^{2}-\frac{1}{n^{2}}\sum_{i=1}^{n}\E\left(x_{i}-\E\xi\right)^{2}=
=\frac{1}{n}\sum_{i=1}^{n}\D x_{i}-\D \bar{x}=\D\xi-\frac{1}{n}\D\xi\\
 & = & \frac{n-1}{n}\D\xi\xrightarrow[n\to\infty]{}\D\xi.
\end{eqnarray*}

\end{proof}

\begin{defn*}
\emph{Исправленная} дисперсия:
\[
\tilde{s}^{2}:=\frac{n}{n-1}s^{2}.
\]
\end{defn*}

Очевидно, исправленная дисперсия --- несмещенная оценка дисперсии.

%\begin{prop*}
%$\widehat{\cdf}_{\xi}$ --- несмещенная оценка $\cdf_{\xi}$.%
%\begin{comment}
%TODO: Чернова
%\end{comment}
%\end{prop*}
%\begin{proof}
%FIXME\end{proof}
%\begin{rem*}
%Как правило, оценки по методу моментов только асимптотически несмещенные.
%Действительно, несмещенность означала бы выполнение для всех $\th\in\Theta$ равенства
%\[
%\E\th^{*}=\E\phi^{-1}(\hat{\E}g(\xi))=\th=\phi^{-1}(\E\hat{\E}g(\xi)).
%\]
%Но часто $\phi^{-1}$ --- выпуклая, так что имеем, на самом деле,
%неравенство Йенсена.
%\end{rem*}
%
%\begin{prop*}
%В условиях регулярности, ОМП асимптотически несмещенная.\end{prop*}
%\begin{proof}
%FIXME
%\end{proof}

\section{Состоятельность}
\begin{defn*}
Оценка называется \emph{состоятельной в среднеквадратичном смысле},
если
\[
\MSE\hat{\theta}_{n}\xrightarrow[n\to\infty]{}0.
\]

\end{defn*}

Как следует из равенства \eqref{eq:MSE}, для асимптотически несмещенных оценок состоятельность в средне-квадратическом следует из сходимости дисперсии оценки к нулю.

\begin{defn*}
Оценка называется \emph{состоятельной}, если%
\begin{comment}
\footnote{Consistency.}
\end{comment}
{}
\[
\hat{\th}_{n}\toP\theta.
\]
\end{defn*}
\begin{prop*}
\label{prop:consistent_est}Если оценка асимптотически несмещенная и состоятельная
в среднеквадратичном смысле, то она состоятельная.\end{prop*}
\begin{proof}
В самом деле, по неравенству Чебышёва,
\[
\P(|\hat{\th}_{n}-\th|>\epsilon)=\P(|\hat{\th}_{n}-\E\hat{\th}_{n}|>\epsilon)\leq\frac{\D\hat{\th}_{n}}{\epsilon^{2}}=\frac{\MSE\hat{\theta}_{n}}{\epsilon^{2}}\xrightarrow[n\to\infty]{}0.
\]
\end{proof}
\begin{prop*}
\label{prop:m_k-consistent}$\hat{\m}_{k}$ является состоятельной
оценкой $\m_{k}$.\end{prop*}
\begin{proof}
Докажем для $\hat{\m}_{1}$. По определению выборки до эксперимента,
$x_{i}\sim\Pcal$. Тогда, по теореме Хинчина о ЗБЧ,
\[
\bar{x}=\frac{\sum_{i=1}^{n}x_{i}}{n}\toP\m_{1}(\Pcal).
\]
 Для $k$-го момента доказывается аналогично заменой $y_{i}:=x_{i}^{k}$.\end{proof}
\begin{rem*}
Состоятельность выполняется и для центральных моментов $\m_{k}^{(0)}$, так как они выражаются через нецентральные, а свойство состоятельности сохраняется для линейной комбинации.
% с доказательство не пройдет, потому что $x_{i}$
%и $\bar{x}$ не будут независимыми.
\end{rem*}
%\begin{prop*}
%\label{prop:m_k_0-consistent}$\hat{\m}_{k}^{(0)}$ является состоятельной
%оценкой $\m_{k}^{(0)}.$\end{prop*}
%\begin{claim*}
%Пусть $\xi_{n}\toP c$ и $f\in C(U_{\epsilon}(c))$. Тогда $f(\xi_{n})\toP f(c).$\end{claim*}
%\begin{proof}[Доказательство предложения]
%Докажем для $s^{2}$. Пусть $f:(x,y)\mapsto x-y^{2}$. Устроим последовательность
%$\left(\hat{\m}_{2},\hat{\m}_{1}\right)\toP(\m_{2},\m_{1})$. Тогда
%\[
%f(\hat{\m}_{2},\hat{\m}_{1})=\hat{\m}_{2}-\hat{\m}_{1}^{2}=\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}-\bar{x}^{2}=s^{2}\toP f(\m_{2},\m_{1})=\D\xi.
%\]
%%
%
%Для $\m_{k}^{(0)}$ доказывается аналогично.\end{proof}

В частности,
%\begin{prop*}
$\bar{x}$ --- состоятельная оценка $\E\xi$ и
%\end{prop*}
%\begin{proof}
%Либо по (\ref{prop:m_k-consistent}) для $k=1$, либо из того факта,
%что $\mathrm{bias}\,\bar{x}=0,$ значит
%\[
%\mathrm{MSE}\,\bar{x}=\D\bar{x}=\frac{\D\xi}{n}\xrightarrow[n\to\infty]{}0,
%\]
% и по (\ref{prop:consistent_est}) получаем утверждение.\end{proof}
%\begin{prop*}
$s^{2}$ --- состоятельная оценка $\D\xi$.
%\end{prop*}
%\begin{proof}
%По (\ref{prop:m_k_0-consistent}) с $k=2$.\end{proof}
%\begin{prop*}
%$\widehat{\cdf}_{\xi}$ --- состоятельная оценка $\cdf$ в каждой
%точке.\end{prop*}
%\begin{proof}
%Введем случайную величину
%\[
%y_{i}:=\ein_{\left\{ x_{i}<x\right\} }=\begin{cases}
%1 & x_{i}<x\\
%0 & x_{i}\geq x.
%\end{cases}
%\]
% $y_{i}$ независимы ($\cdf_{y_{i}}(x)\cdf_{y_{j}}(y)=\cdf_{y_{i},y_{j}}(x,y)$)
%и одинаково распределены; кроме того, их математическое ожидание конечно:
%\[
%\E y_{i}=1\cdot\P(x_{i}<x)+0\cdot\P(x_{i}\geq x)=\P(x_{i}<x)=\cdf_{\xi}(x)<\infty.
%\]
%Тогда по теореме Хинчина о ЗБЧ,
%\[
%\widehat{\cdf}_{\xi}(x)=\frac{\left|\left\{ x_{i}\in\bar{x}:x_{i}<x\right\} \right|}{n}=\frac{\sum_{i=1}^{n}y_{i}}{n}\toP\E y_{i}=\E\ein_{\left\{ x_{i}<x\right\} }=\P(x_{i}<x)=\cdf(x).
%\]
%\end{proof}
%\begin{claim*}
%Пусть $\exists!p_{0}:\cdf(x)=p_{0}$ и $\cdf(x)$ монотонно возрастает
%в окрестности $p_{0}$. Тогда $\bar{z}_{p_{0}}\toP z_{p_{0}}$, т.е.
%является состоятельной оценкой.\end{claim*}
%\begin{prop*}
%Оценки, полученные по методу моментов, являются состоятельными.\end{prop*}
%\begin{proof}
%Поскольку $f_{i}$ --- непрерывные функции и при непрерывных преобразованиях
%сходимость не портится:
%\[
%\th^{*}=\phi^{-1}(\hat{\E}g(\xi))\toP\phi^{-1}(\E g(\xi))=\phi^{-1}(\phi(\th))=\th.
%\]
%\end{proof}
%\begin{prop*}
%Оценки $\hat{\thb}_{\mathrm{MLE}}$, полученные по методу максимального
%правдоподобия, являются состоятельными.\end{prop*}
%\begin{proof}
%Пусть $\thb_{0}$ --- истинный параметр $\Pcal(\thb)$. По УЗБЧ,
%\[
%\frac{1}{n}\ln\L(\thb\mid\mathbf{x})=\frac{1}{n}\sum_{i=1}^{n}\ln p_{\thb}(x_{i})\toP\E\ln p_{\thb}(x_{i})=\int_{\RR}\ln\left(p_{\thb}(x)\right)p_{\thb_{0}}(x)\d x.
%\]
% Навесим на обе стороны $\argmax$ в условии, что это непрерывное
%преобразование:
%\[
%\hat{\boldsymbol{\theta}}_{\mathrm{MLE}}\gets\argmax_{\thb}\frac{1}{n}\ln\L(\thb\mid\mathbf{x})\toP\argmax_{\thb}\int_{\RR}\ln\left(p_{\thb}(x)\right)p_{\thb_{0}}(x)\d x\to\thb^{*}.
%\]
%Тогда в предположении непрерывности $p_{\thb}$ по $\thb$, $\hat{\boldsymbol{\theta}}_{\mathrm{MLE}}\toP\thb^{*}$.
%Покажем, что $\thb^{*}=\thb_{0}$. Поделим на $p_{\thb_{0}}$ ---
%константу по $\thb$:
%\[
%\frac{1}{n}\sum_{i=1}^{n}\ln\frac{p_{\thb}}{p_{\thb_{0}}}(x_{i})\toP\int_{\RR}\ln\left(\frac{p_{\thb}(x)}{p_{\thb_{0}}(x)}\right)p_{\thb_{0}}(x)\d x=\E\ln\frac{p_{\thb}}{p_{\thb_{0}}}\leq\ln\E\frac{p_{\thb}}{p_{\thb_{0}}}=\ln\int_{\RR}\frac{p_{\thb}}{p_{\thb_{0}}}(x)p_{\thb_{0}}(x)\d x=\ln1=0
%\]
% по неравенству Єнсена $\E g(\xi)\leq g(\E\xi),$ для выпуклой вверх
%$g(x)=\log(x)$. Таким образом,
%\[
%\int_{\RR}\ln\left(\frac{p_{\thb}(x)}{p_{\thb_{0}}(x)}\right)p_{\thb_{0}}(x)\d x=0\iff\ln\left(\frac{p_{\thb}(x)}{p_{\thb_{0}}(x)}\right)=0\iff\frac{p_{\thb}(x)}{p_{\thb_{0}}(x)}=1\iff p_{\thb}(x)=p_{\thb_{0}}(x)
%\]
% для почти всех $x$. В предположении свойства \emph{идентифицируемости}
%задачи ($\th_{1}\neq\th_{2}\implies\Pcal_{\th_{1}}\neq\Pcal_{\th_{2}}$),
%получаем $\thb=\thb_{0}$.
%\end{proof}

\section{Асимптотическая нормальность}
\begin{defn*}
Оценка $\hat{\th}_{n}$ называется \emph{асимптотически нормальной}
оценкой параметра $\th$ с коэффициентом $\sigma^{2}(\th)$ если
\[
\sqrt{n}\left(\hat{\th}_{n}-\th\right)\tod\N(0,\sigma^{2}(\th)).
\]
\begin{comment}
Оценка называется \emph{асимптотически нормальной}, если
\[
\frac{\hat{\th}_{n}-\E\hat{\th}_{n}}{\sqrt{\D\hat{\th}_{n}}}\tod\N(0,1).
\]
\end{comment}
\begin{comment}
\[
\exists c:\D\hat{\th}_{n}=c^{2}/n,\ \E\hat{\th}_{n}=\th\implies\frac{\sqrt{n}(\hat{\th}_{n}-\th)}{c}\xrightarrow[n\to\infty]{\mathrm{d}}\N(0,1)
\]
\end{comment}

\end{defn*}
\begin{comment}
Если $\xi\sim\N(\mu,\sigma^{2})$, то $\bar{x}$ --- просто
нормальная оценка (как линейная комбинация нормальных случайных величин).
\end{comment}

\begin{example*}
$\bar{x}$ --- асимптотически нормальная оценка, если $\D\xi<\infty,\ \D\xi\neq0$:
\[
\sqrt{n}\left(\bar{x}-\E\xi\right)\tod\N(0,\D\xi).
\]
\end{example*}
\begin{proof}
По ЦПТ,
\[
\sqrt{n}\left(\bar{x}-\E\xi\right)=\frac{\sum_{i=1}^{n}x_{i}-n\E\xi}{\sqrt{n}}\tod\N(0,\D\xi).
\]
\end{proof}

Мы обсуждали, что \emph{асимптотическую нормальность можно определять и в слабом смысле} --- как сходимость по распределению к нормальному распределению $N(0,1)$ стандартизированной случайной величины: $(\hat\th_n-\E\hat\th_n)/\sqrt{\D\hat\th_n}\tod\N(0,1)$.

%\begin{example*}
%$\hat{\m}_{k}$ --- асимптотически нормальная оценка.\end{example*}
%\begin{proof}
%FIXME\end{proof}
%\begin{example*}
%$s^{2}$ и $\tilde{s}^{2}$ --- асимптотически нормальные оценки,
%если $0\neq\D(\xi-\E\xi)^{2}<\infty$:
%\[
%\sqrt{n}\left(s^{2}-\D\xi\right)\tod\N(0,\D(\xi-\E\xi)^{2}).
%\]
%\end{example*}
%\begin{proof}
%Пусть $\eta=\xi-\E\xi$ и $y_{i}=x_{i}-\E\xi$. Тогда
%\begin{eqnarray*}
%s^{2} & = & \frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\E\xi-(\bar{x}-\E\xi))^{2}=\hat{\E}\eta^{2}-\bar{y}^{2}\\
%\sqrt{n}(s^{2}-\D\xi) & = & \sqrt{n}(\hat{\E}\eta^{2}-\bar{y}^{2}-\D\xi)=\sqrt{n}(\hat{\E}\eta^{2}-\underbrace{\D\xi}_{=\D\eta=\E\eta^{2}})-\sqrt{n}\bar{y}^{2}\\
% & = & \underbrace{\frac{\sum_{i=1}^{n}y_{i}^{2}-n\E\eta^{2}}{\sqrt{n}}}_{\tod\N(0,\D\eta^{2})}-\underbrace{\bar{y}}_{\tod0}\underbrace{\sqrt{n}\bar{y}}_{\tod\N(0,\D\xi)}\tod\N(0,\D(\xi-\E\xi)^{2}).
%\end{eqnarray*}
%
%\end{proof}
%
%\begin{example*}
%$\widehat{\cdf}_{\xi}$ --- асимптотически нормальная оценка:
%\[
%\sqrt{n}\left(\widehat{\cdf}_{\xi}(x)-\cdf_{\xi}(x)\right)\tod\N(0,\hat{\sigma}^{2}),\quad\text{где}\ \hat{\sigma}^{2}=\cdf(x)(1-\cdf(x)).
%\]
%\end{example*}
%\begin{proof}
%FIXME %
%\begin{comment}
%TODO: Чернова--15
%\end{comment}
%\end{proof}
%\begin{example*}
%При определенных условиях, $\widehat{\med}\xi$ является асимптотически
%нормальной оценкой $\med\xi$.\end{example*}
%\begin{proof}
%FIXME%
%\begin{comment}
%Чернова--20
%\end{comment}
%\end{proof}
%\begin{prop*}
%Асимптотически нормальная оценка состоятельна.\end{prop*}
%\begin{proof}
%Действительно,
%\[
%\hat{\th}_{n}-\th=\frac{1}{\sqrt{n}}\sqrt{n}(\hat{\th}_{n}-\th)\tod0\cdot\eta=0,\quad\eta\sim\N(0,\sigma^{2}(\th)).
%\]
% Слабая сходимость же к константе влечет слабую сходимость по вероятности,
%откуда $\hat{\th}_{n}\toP\th$.\end{proof}
%\begin{cor*}
%Асимптотически нормальные оценки сходятся к оцениваемому параметру
%со скоростью $1/\sqrt{n}$.\end{cor*}
%\begin{rem*}
%Если оценка не асимптотически нормальная, она \emph{может} сходится
%к параметру быстрее $1/\sqrt{n}$.\end{rem*}
%\begin{defn*}
%Пусть $\hat{\th}^{(1)},\hat{\th}^{(2)}$ --- асимптотически нормальные
%оценки с соответствующими коэффициентами $\left(\sigma^{(1)}\right)^{2}(\th)$
%и $\left(\sigma^{(2)}\right)^{2}(\th)$. Говорят, что\emph{ $\hat{\th}^{(1)}$
%лучше $\hat{\th}^{(2)}$ в смысле асимптотического подхода}, если
%\[
%\left(\sigma^{(1)}\right)^{2}(\th)\leq\left(\sigma^{(2)}\right)^{2}(\th),\quad\forall\th\in\Theta
%\]
% (и хотя бы при одном $\th$ это неравенство строгое).
%\end{defn*}

\section{Эффективность}

\begin{defn*}
Говорят, что оценка $\hat{\th}^{(1)}$ \emph{лучше $\hat{\th}^{(2)}$
в среднеквадратичном смысле}, если
\[
\MSE\hat{\th}^{(1)}\leq\MSE\hat{\th}^{(2)}.
\]
\end{defn*}
%\begin{rem*}
%Оценка $\hat{\th}^{*}$ является лучшей в среднеквадратичном смысле
%в классе всевозможных оценок только если она совпадает с самим оцениваемым
%параметром, $\hat{\th}^{*}=\th$. Значит, в невырожденном с точки
%зрения статистики случае наилучшей среднеквадратичной оценки не существует.
%\end{rem*}

\begin{rem*}
Для несмещенных оценок определение эквивалентно, конечно,
\[
\D\hat{\th}^{(1)}\leq\D\hat{\th}^{(2)}.
\]
\begin{comment}
Пусть $\xi\sim\N(\mu,\sigma^{2})$. $\D x_{i}=\sigma^{2}>\D\bar{x}=\sigma^{2}/n$,
поэтому $\bar{x}$ является более эффективной оценкой $\E\xi$,
чем случайный элемент из выборки.
\end{comment}
\end{rem*}
%\begin{defn*}
%Поскольку в классе всех оценок наилучшей не существует, класс этот
%разбивается  на $\mathcal{K}_{b}=\left\{ \hat{\th}\mid\E\hat{\th}=\th+\bias\th=\th+b\right\} $
%--- классы всех оценок с одинаковым смещением. Оценка $\hat{\th}^{*}\in\mathcal{K}_{b}$
%называется эффективной, если её среднеквадратичная ошибка меньше всех
%других оценок в этом классе:
%\[
%\E(\hat{\th}^{*}-\th)^{2}\leq\E(\hat{\th}-\th)^{2},\quad\forall\hat{\th}\in\mathcal{K}_{b},\ \th\in\Theta.
%\]
%
%\end{defn*}
%
%\begin{defn*}
%Эффективная оценка в классе $\mathcal{K}_{0}$ называется просто \emph{эффективной}.\end{defn*}
%\begin{prop*}
%Эффективная оценка в классе $\mathcal{K}_{b}$ единственна.\end{prop*}
%\begin{proof}
%FIXME%
%\begin{comment}
%Чернова--37
%\end{comment}
%\end{proof}
%\begin{example*}
%Для $\xi\sim\U(0,\th)$, оценка $\hat{\th}_{n}^{(1)}=\max(\mathbf{x})$
%более эффективна, чем $\hat{\th}_{n}^{(2)}=2\bar{x}$.\end{example*}
%\begin{proof}
%FIXME%
%\begin{comment}
%Чернова--38
%\end{comment}
%\end{proof}
\begin{defn*}
В классе несмещенных оценок оценка называется эффективной (в средне-квадратическом), если ее дисперсия минимальна. В классе асимптотически несмещенных оценок оценка $\hat{\th}=\hat{\th}_n$ называется асимптотически эффективной, если для любой другой оценки $\hat{\th}^*$ выполнено\\ $\lim_{n\rightarrow\infty}\D\hat{\th}_n/\D\hat{\th}_n^*\leq 1$.
\end{defn*}




\subsection{Эффективность и неравенство Рао-Крамера}

Пусть $\Pcal_{\xi}(\boldsymbol{\theta}),\ \boldsymbol{\theta}=\left(\theta_{1},\ldots,\theta_{r}\right)^{\T}$
--- параметрическая модель. Пусть $r=1$.
\begin{defn*}
\emph{Информанта $n$-го порядка}:
\[
S_{n}(\mathbf{x},\theta)=\frac{\mathrm{d}^{n}\ln\L(\theta\mid\mathbf{x})}{\mathrm{d}\theta^{n}}.
\]

\end{defn*}

\begin{defn*}
\emph{Информационное количество Фишера}: %
\begin{comment}
@xio: Количество информации в выборке о параметре?
\end{comment}
{}
\[
I_{n}(\theta):=-\E S_{2}(\mathbf{x},\theta).
\]
\end{defn*}
\begin{claim*}
\[
I_{n}(\theta)=\E S_{1}^{2}(\mathbf{x},\theta).
\]
\end{claim*}
\begin{example*}
\label{exa:In-pois}$\xi\sim\Pois(\lambda)$.
\[
S_{1}(\mathbf{x},\theta)=-n+\frac{n\bar{x}}{\lambda},\quad S_{2}(\mathbf{x},\theta)=-\frac{n\bar{x}}{\lambda^{2}}\implies I_{n}(\lambda)=\E\frac{n\bar{x}}{\lambda^{2}}=\frac{n}{\lambda^{2}}\E\bar{x}=\frac{n}{\lambda}.
\]
\end{example*}
\begin{rem*}
\[
\ln\L(\theta\mid\mathbf{x})=\sum_{i=1}^{n}\ln p_{\theta}(x_{i})\implies S_{2}=\frac{\mathrm{d}^{2}\ln\L(\theta\mid\mathbf{x})}{\mathrm{d}\theta^{2}}=\sum_{i=1}^{n}(\ln p_{\theta}(x_{i}))'',
\]
откуда, для повторной независимой выборки,
\[
I_{n}(\theta)=-\sum_{i=1}^{n}\E(\ln p_{\theta}(x_{i}))''=n\cdot i(\theta),\quad\text{где }i(\theta)=-\E(\ln p_{\theta}(\xi))''.
\]
\begin{comment}
 где $i(\theta)$ --- малая информанта Фишера.
\end{comment}
\end{rem*}
\begin{defn*}
$C\subset\RR$ есть \emph{носитель} параметрического семейства распределений
$\Pcal(\th)$, если
\[
\xi\sim\Pcal(\th)\implies\P(\xi\in C)=1,\ \forall\th\in\Theta.
\]

\end{defn*}

\begin{defn*}
Условие регулярности: имеют отношение к независимости носителя распределения от параметра, а также к существованию и ограниченности производных функции лог-правдоподобия по параметру до определённого порядка дифференцирования.
%\begin{itemize}
%\item Существует $C$ --- носитель распределения $\Pcal(\th)$ такой, что
%$\forall y\ \sqrt{p_{\th}(y)}$ непрерывно дифференцируема по $\th$
%всюду в области $\Theta$.
%\item Существует $I_{n}(\th)>0$, непрерывное по $\th$ всюду в области
%$\Theta$.
%\end{itemize}
\end{defn*}
\begin{example*}
$\Exp(\lambda)$ --- регулярное семейство; $\U(0,\th)$ --- не является
регулярным.\end{example*}
%\begin{proof}
%FIXME %
%\begin{comment}
%Чернова--48
%\end{comment}
%\end{proof}
\begin{claim*}
Для несмещенных оценок в условиях регулярности справедливо неравенство\emph{
Рао--Крамера}:
\[
\D\hat{\theta}_{n}\geq\frac{1}{I_{n}(\theta)}.
\]


Для смещенных оценок,
\[
\D\hat{\th}_{n}\geq\frac{(1+\bias'(\th))^{2}}{I_{n}(\th)}.
\]
\begin{comment}
Чернова--50
\end{comment}

\end{claim*}

\begin{cor*}
\emph{\label{def:eff-est}}Несмещенная оценка является эффективной,
если:
\[
\D\hat{\theta}_{n}=\frac{1}{I_{n}(\theta)}.
\]

\end{cor*}

\begin{cor*}
\emph{\label{def:eff-est}}Асимптотически несмещенная оценка является асимптотически эффективной,
если:
\[
\D\hat{\theta}_{n}\cdot{I_{n}(\theta)} \rightarrow 1 \text{\ as\ } n\rightarrow \infty.
\]

\end{cor*}


\begin{comment}
Для несмещенных оценок неравенство Рао--Крамера указывает точную нижнюю
границу дисперсий оценок.
\end{comment}

\begin{xca*}[Хорошее]
Показать, что $\bar{x}$ является эффективной оценкой $\mu$
в модели $\xi\sim\N(\mu,\sigma^{2})$.\end{xca*}
%\begin{cor*}
%Пусть $\hat{\theta}_{n}$ --- асимптотически несмещенная оценка. Тогда
%$\hat{\theta}_{n}$ --- \emph{асимптотически эффективная}, если
%\[
%\D\hat{\theta}_{n}\cdot I_{n}\xrightarrow[n\to\infty]{}1.
%\]
%\begin{comment}
%\begin{rem*}
%$\D\hat{\theta}_{n}=\frac{1}{n\cdot i(\theta)}$, значит, быстрее,
%чем $1/n$ не умеем.\end{rem*}
%\end{comment}
%\end{cor*}
\begin{example*}
Пусть $\xi\sim\N(\mu,\sigma^{2})$. Можно посчитать, что $s^{2}$
является только асимптотически эффективной оценкой $\sigma^{2}$;
$\tilde{s}^{2}$ --- просто эффективной.
\end{example*}

\begin{example*}
Пусть $\xi\sim\Pois(\lambda)$. Поскольку
\begin{eqnarray*}
\D\hat{\lambda}_{n} & = & \D\bar{x}=\E\xi/n=\lambda/n\\
I_{n}(\lambda) & = & n/\lambda,
\end{eqnarray*}
то $\hat{\lambda}_{n}$ --- эффективная оценка (по
свойствам $\hat{\th}_{\MLE}$, гарантировано, что она асимптотически эффективная).
\end{example*}

\section{Устойчивость оценок}
Так как в реальных данных часто бывают те или иные ошибки, часто жертвуют точностью для увеличения
устойчивости (робастности) к выбросам. Устойчивые аналоги оценок часто строятся на основе рангов (номеров по порядку в упорядоченной выборке). Приведем пример.
\begin{example*}[Сравнение оценок мат. ожидания симметричного распределения]
Пусть $\Pcal$ \emph{симметрично} \emph{--- }в этом случае $\widehat{\med\xi}=\bar{x}$
и имеет смысл сравнить две этих характеристики. %
\begin{comment}
Найдем среднеквадратичную ошибку $\widehat{\med\xi}$ и $\bar{x}$.
Поскольку обе этих оценки несмещенные, $\MSE=\D$ и
\end{comment}
{}
\begin{eqnarray*}
\D\bar{x} & = & \frac{\D\xi}{n}\\
\D\widehat{\med\xi} & \sim & \frac{1}{4n\pdf_{\N(\mu,\sigma^{2})}^{2}(\med\xi)}\quad\text{при }n\to\infty.
\end{eqnarray*}
 Так, если $\xi\sim\N(\mu,\sigma^{2})$, то
\[
\pdf_{\N(\mu,\sigma^{2})}^{2}(\med\xi)=\frac{1}{2\pi\sigma^{2}}\exp\left\{ -\frac{(\med\xi-\mu)^{2}}{\sigma^{2}}\right\} =\frac{1}{2\pi\sigma^{2}},
\]
откуда
\[
\D\widehat{\med\xi}=\frac{\pi}{2}\frac{\sigma^{2}}{n}>\frac{\sigma^{2}}{n}=\D\bar{x},
\]
значит $\bar{x}$ эффективнее $\widehat{\med\xi}$.
\begin{rem*}
В то же время, $\widehat{\med\xi}$ более устойчива к аутлаерам, чем
$\bar{x}$, и этим лучше.
\end{rem*}
\end{example*}
\begin{comment}
Достаточность
\end{comment}

%\end{document}

%\chapter{Построение эффективных оценок}
%
%Эффективную оценку можно построить как функцию от полной и достаточной
%статистики.
%\begin{defn*}
%Пусть $x_{i}\sim\Pcal_{\th},\ \th\in\Theta$. Статистика $T(\mathbf{x})$
%называется \emph{достаточной} для параметра $\th$, если при любом
%$t$ и $B\in\mathfrak{B}(\RR^{n})$ распределение $\P(\mathbf{x}\in B\mid T=t)$
%не зависит от $\th$.
%\end{defn*}
%Таким образом, известное и фиксированное значение достаточной статистики
%дает всю информацию о параметре (и выборка тогда не нужна).
%\begin{claim*}[Факторизация Неймана--Фишера]
%$T$ достаточна, тогда и только тогда, когда
%\[
%\L(\th\mid\mathbf{x})\overset{\ae}{=}h(\mathbf{x})\Psi(T,\th).
%\]
%\end{claim*}
%\begin{example*}
%Пусть $\xi\sim\Pois(\lambda)$, тогда
%\[
%\L(\lambda\mid\mathbf{x})=\prod_{i=1}^{n}e^{-\lambda}\frac{\lambda^{x_{i}}}{x_{i}!}=\underbrace{\frac{1}{\prod_{i=1}^{n}x_{i}!}}_{h(\mathbf{x})}\underbrace{e^{-n\lambda}\lambda^{n\bar{x}}}_{\Psi(n\bar{x},\lambda)}\implies T=n\bar{x}.
%\]
%
%\end{example*}
%
%\begin{example*}
%Пусть $\xi\sim\U(0,\th)$, тогда%
%\begin{comment}
%FIXME: почему так странно
%\end{comment}
%{}
%\[
%\L(\lambda\mid\mathbf{x})=\prod_{i=1}^{n}\frac{1}{\th}\ein_{\left[0,\th\right]}(x_{i})=\underbrace{\frac{1}{\th^{n}}\ein(x_{(n)}<\th)}_{\Psi(x_{(n)},\th)}\underbrace{\ein(x_{(1)}\geq0)}_{h(\mathbf{x})}\implies T=x_{(n)}.
%\]
%
%\end{example*}
%
%\begin{example*}
%Пусть $\xi\sim\N(\mu,\sigma^{2})$, тогда проверяется, что для $\thb=\left(\mu,\sigma^{2}\right)^{\T}$
%\[
%\L(\thb\mid\mathbf{x})=\Psi\left(\left(n\overline{\mathbf{x}^{2}},n\bar{x}\right),\thb\right).
%\]
%\end{example*}
%\begin{defn*}
%Статистика $T$ называется \emph{полной}, если для борелевской $g$
%\[
%\E g(T)=0\implies g(T)=0,\quad\forall\th\in\Theta.
%\]
%\end{defn*}
%\begin{cor*}
%Оценка, являющаяся функцией от $S$, единственна в классе оценок с
%таким же смещением.\end{cor*}
%\begin{proof}
%Пусть их две: $\hat{\th}_{n}^{(1)}(T)$ и $\hat{\th}_{n}^{(2)}(T)$.
%Тогда $\E\left(\hat{\th}_{n}^{(1)}(T)-\hat{\th}_{n}^{(2)}(T)\right)=\E g(T)=0$,
%значит $g(T)=0=\hat{\th}_{n}^{(1)}(T)-\hat{\th}_{n}^{(2)}(T)$.\end{proof}
%\begin{thm*}[Рао--Блэкуэлл--Колмогоров]
%Если $T$ --- полная и достаточная, то оценка $\hat{\th}(T)$, являющаяся
%функцией от $T$, эффективна в классе оценок с таким же смещением.\end{thm*}
%\begin{proof}
%Для любой $\hat{\th}_{n}\in K_{b}$,
%\begin{eqnarray*}
%\MSE\hat{\th}_{n} & = & \E(\hat{\th}_{n}-\th)^{2}=\E(\hat{\th}_{n}-\hat{\th}_{n}(T)+\hat{\th}_{n}(T)-\th)^{2}=\E(\hat{\th}_{n}-\hat{\th}_{n}(T))^{2}+\E(\hat{\th}_{n}(T)-\th)^{2}\\
% & \geq & \E(\hat{\th}_{n}(T)-\th)^{2}=\MSE\hat{\th}_{n}(T).
%\end{eqnarray*}
%Использовано равенство
%\begin{eqnarray*}
%\E(\hat{\th}_{n}-\hat{\th}_{n}(T))(\hat{\th}_{n}(T)-\th) & = & \E\left(\E\left\{ (\hat{\th}_{n}-\hat{\th}_{n}(T))(\hat{\th}_{n}(T)-\th)\mid T\right\} \right)\\
% & = & \E\left((\hat{\th}_{n}(T)-\th)\E\left\{ (\hat{\th}_{n}-\hat{\th}_{n}(T))\mid T\right\} \right)=0,
%\end{eqnarray*}
% потому что $\E\left\{ \hat{\th}_{n}\mid T\right\} =\hat{\th}_{n}(T)$
%по полноте $T$, так что и $\E\left\{ (\hat{\th}_{n}-\hat{\th}_{n}(T))\mid T\right\} =0$.
%\end{proof}
%\begin{comment}
%FIXME: очень хочется пример
%\end{comment}



\part{Некоторые распределения, связанные с нормальным}

\index{1. Распределения и их свойства: нормальное, Стьюдента, chi^{2}
,
экспоненциальное, Пуассона, Бернулли, бионмиальное,
отрицательно-биномиальное, геометрическое, логнормальное@1. Распределения и их свойства: нормальное, Стьюдента, $\chi^{2}$,
экспоненциальное, Пуассона, Бернулли, бионмиальное, отрицательно-биномиальное,
геометрическое, логнормальное}

\chapter{Распределение $N(a,\sigma^2)$}

Свойства хорошо известны. В частности, плотность имеет вид
\[
p(x) = \frac{1}{\sigma\sqrt{2\pi}}\; e^{ -\frac{(x-a)^2}{2\sigma^2} },
\]
математическое ожидание равно $a$, дисперсия $\sigma^2$, асимметрия и эксцесс равны 0.

Рассмотрим вопрос про измерение расстояния в сигмах.
Будет говорить, что точка далеко от мат.ожидания, если это и более далекие значения маловероятны.

Формально, пусть $\xi\sim N(a,\sigma^2)$. Рассмотрим $\P(|\xi - a|> k \sigma)$. Эта вероятность не зависит от $\sigma$ и равна $2(1- \Phi(k))$, где $\Phi(x)$ --- функция стандартного нормального распределения $\N(0,1)$.

Значения $\P(|\xi - a|> k\sigma)$:\\
\begin{tabular}{l|r}
\hline
$k$& Вероятность\\
\hline
1&	0.317\\
1.64&	0.101\\
1.96&	0.050\\
2	&0.046\\
3	&2.70E-03\\
6	&1.97E-09\\
\hline
\end{tabular}

Отсюда правило двух сигма (вероятность быть на расстоянии от мат.ож. больше двух cигм примерно равна 0.05), правило трех сигм, правило шести сигм.


\chapter{Распределение $\chi^{2}(m)$}
\begin{defn*}[Распределение $\chi^{2}(m)$]
$\eta$ имеет распределение $\chi^{2}$ с $m$ степенями свободы $(\eta\sim\chi^{2}(m))$, если
\[
\eta=\sum_{i=1}^{m}\zeta_{i}^{2},\quad\zeta_{i}\sim\N(0,1),\ \zeta_{i}\text{ независимы}.
\]

\end{defn*}

\paragraph{Свойства\protect\footnote{Вычисление $\protect\D\eta$: \protect\url{https://www.statlect.com/probability-distributions/chi-square-distribution}}
$\chi^{2}(m)$}

\begin{eqnarray*}
\E\eta & = & \sum_{i=1}^{m}\E\zeta_{i}^{2}=m\\
\D\eta & = & 2m
\end{eqnarray*}

\begin{claim*}
Пусть $\eta_{m}\sim\chi^{2}(m)$. Тогда, по ЦПТ,
\[
\frac{\eta_{m}-\E\eta_{m}}{\sqrt{\D\eta_{m}}}=\frac{\eta_{m}-m}{\sqrt{2m}}\tod\N(0,1).
\]
\end{claim*}
\begin{example*}
$m=50,\ \eta_{m}=80$. Тогда
\[
\frac{80-50}{10}=3
\]
 и
\[
\cdf_{\chi^{2}(50)}(80)=0.9955\approx\Phi(3)=0.9986.
\]
 %
\begin{comment}
Картиночки с разным $m$
\end{comment}
\end{example*}
\begin{prop*}
$\chi^{2}(m)/m\xrightarrow[m\to\infty]{}1.$\end{prop*}
\begin{proof}
По ЗБЧ.
\end{proof}

\chapter{Распределение Стьюдента $\protect\t(m)$}
\begin{defn*}[Распределение $\t(m)$]
$\xi$ имеет распределение Стьюдента с $m$ степенями свободы ($\xi\sim\t(m)$), если
\[
\frac{\zeta}{\sqrt{\eta/m}},\quad\zeta\sim\N(0,1),\ \eta\sim\chi^{2}(m).
\]

\end{defn*}

\paragraph{Свойства $\protect\t(m)$}
\begin{itemize}
\item При $m=1$ это распределение Коши, у него не существует математического ожидания.
\item При $m>1$, $\E\xi=0$ по симметричности.
\item При $m>2$, $\D\xi=m/(m-2)$.
\item При $m>3$, $\mathsf{A}\xi=0$ по симметричности.
\item При $m>4$, $\K\xi=6/(m-4)$.\end{itemize}
\begin{prop*}
Распределение Стьюдента сходится к стандартному нормальному:
\[
\t\Rightarrow\N(0,1).
\]
\end{prop*}
\begin{proof}[Соображения по поводу]
$\D\xi\to1,\ \K\xi\to0.$
\end{proof}

\chapter{Распределение Фишера}
\begin{defn*}
Распределение Фишера имеет вид
\[
\mathrm{F}(m,k)=\frac{\chi^{2}(m)/m}{\chi^{2}(k)/k}.
\]
\end{defn*}
\begin{rem*}
$\mathrm{F}(1,k)\sim\t^{2}(k)$; $\mathrm{F}(m,\infty)=\chi^{2}(m)/m$,
потому что $\chi^{2}(k)/k\xrightarrow[k\to\infty]{}1$.
\end{rem*}

\chapter{Квадратичные формы от нормально распределенных случайных величин}
%(Это на след. семестр, сейчас можно не вникать.)\\
Пусть $\xib=\left(\xi_{1},\ldots,\xi_{p}\right)^{\T}\sim\N(\mathbf{0},\sigma^{2}\mathbf{I}_{p})$,
$\mathbf{B}$ --- симметричная, неотрицательно определенная матрица.
Найдем распределение $\xib^{\T}\mathbf{B}\xib$.
\begin{claim*}
Пусть $\xib\sim\N(\mathbf{0},\sigma^{2}\mathbf{I}_{p})$, $\mathbf{B},\mathbf{C}$
--- симметричные матрицы размерности $p\times p$. Тогда $\xib^{\T}\mathbf{B}\xib\indep\xib^{\T}\mathbf{C}\xib\iff\mathbf{B}\mathbf{C}=\mathbf{0}.$\end{claim*}
\begin{example*}[Независимость $\bar{x}^{2}$ и $s^{2}$]
\label{exa:x2-s2-indep}Запишем
\begin{eqnarray*}
\bar{x}^{2} & = & \frac{1}{n^{2}}\left(\sum_{i=1}^{n}x_{i}\right)^{2}=\frac{1}{n^{2}}\sum_{i=1}^{n}\sum_{j=1}^{n}x_{i}x_{j}=\frac{1}{n}\mathbf{x}\underbrace{\begin{pmatrix}1/n & \dots & 1/n\\
\vdots & \ddots & \vdots\\
1/n & \dots & 1/n
\end{pmatrix}}_{\mathbf{B}}\mathbf{x}^{\T}\\
s^{2} & = & \frac{1}{n}\sum_{i=1}^{n}x_{i}^{2}-\frac{1}{n}\mathbf{x}\mathbf{B}\mathbf{x}^{\T}=\frac{1}{n}\left(\mathbf{x}\mathbf{I}_{n}\mathbf{x}^{\T}-\mathbf{x}\mathbf{B}\mathbf{x}^{\T}\right)=\frac{1}{n}\mathbf{x}\underbrace{\begin{pmatrix}1-1/n & \dots & -1/n\\
\vdots & \ddots & \vdots\\
-1/n & \dots & 1-1/n
\end{pmatrix}}_{\mathbf{C}=\mathbf{I}_{n}-\mathbf{B}}\mathbf{x}^{\T}.
\end{eqnarray*}
 Таким образом, $n\bar{x}^{2}=\mathbf{x}\mathbf{B}\mathbf{x}^{\T}$
и $ns^{2}=\mathbf{x}\mathbf{C}\mathbf{x}^{\T}$. Но
\[
\mathbf{B}\mathbf{C}=\mathbf{B}(\mathbf{I}_{n}-\mathbf{B})=\mathbf{B}-\mathbf{B}^{2}=\mathbf{0},
\]
 так как
\[
\mathbf{B}^{2}=\begin{pmatrix}1/n & \dots & 1/n\\
\vdots & \ddots & \vdots\\
1/n & \dots & 1/n
\end{pmatrix}^{2}=\begin{pmatrix}n\cdot1/n^2 & \dots & n\cdot1/n^2\\
\vdots & \ddots & \vdots\\
n\cdot1/n^2 & \dots & n\cdot1/n^2
\end{pmatrix}=\mathbf{B}.
\]

(Вообще, справедливость $\mathbf{B}\mathbf{C}=0$ не удивительна в случае, когда $\mathbf{B}$ --- матрица проектора на линейное подпространство, а $\mathbf{C} = \mathbf{I}_{n}-\mathbf{B}$ --- матрица проектора на ортогональное дополнение. В данном примере, это проектор на подпространство, натянутое на вектор из единиц.)

 Значит, $\bar{x}^{2}\indep s^{2}.$
\end{example*}
Видно, что $\sigma^{-2}\xib^{\T}\mathbf{I}_{p}\xib\sim\chi^{2}(p)$.
На самом деле, справедливо
\begin{claim*}
Пусть $\xib\sim\N(\mathbf{0},\sigma^{2}\mathbf{I}_{p})$, $\mathbf{B}$
--- симметричная, неотрицательно неопределенная матрица размерности
$p\times p$ и $\rk\mathbf{B}=r$. Тогда
\[
\sigma^{-2}\xib^{\T}\mathbf{B}\xib\sim\chi^{2}(r)\iff\mathbf{B}^{2}=\mathbf{B}.
\]
\end{claim*}
\begin{example*}
\label{exa:s2-chi2}Покажем, что
\[
ns^{2}/\sigma^{2}\sim\chi^{2}(p-1).
\]
Воспользуемся представлением из предыдущего примера: $ps^{2}=\mathbf{x}^{\T}\mathbf{C}\mathbf{x}$.
Но $\rk\mathbf{C}=\rk(\mathbf{I}_{p}-\mathbf{B})=p-1$; $\mathbf{B}^{2}=\mathbf{B}$,
значит $p\sigma^{-2}s^{2}\sim\chi^{2}(p-1)$.\end{example*}
\begin{claim*}[Cochran]
Пусть $\xib\sim\N(\mathbf{0},\mathbf{I}_{p})$, $\xib^{\T}\xib=\sum_{i}Q_{i}$,
где $Q_{i}$ --- квадратичная форма, заданная $\mathbf{B}_{i}$, $\rk\mathbf{B}_{i}=r_{i}$.
Тогда следующие утверждения эквивалентны:
\begin{enumerate}
\item $\sum r_{i}=p$
\item $Q_{i}\sim\chi^{2}(r_{i})$
\item $Q_{i}\indep Q_{j},\quad\forall i\neq j$, т.е. $\mathbf{B}_{i}\mathbf{B}_{j}=\mathbf{0}$.
\end{enumerate}
\end{claim*}

\chapter{Распределение важных статистик}
Пусть $\xi \sim \N(a, \sigma^2)$.
%\section{Связанные с матем. ож.}
%\begin{prop*}
%Пусть $\D\xi=\sigma^{2}<\infty$; тогда
%\[
%t=\sqrt{n}\frac{(\bar{x}-\E\xi)}{\sigma}\xrightarrow[n\to\infty]{}\N(0,1)
%\]
% \end{prop*}
%\begin{proof}
%По ЦПТ.\end{proof}
\begin{prop*}
$t=\sqrt{n}\frac{(\bar{x}-\E\xi)}{\sigma}$ имеет стандартное нормальное распределение.
\end{prop*}
\begin{proof}
\[
t=\frac{\bar{x}-a}{\sqrt{\D\bar{x}}}=\sqrt{n}\,\dfrac{\bar{x}-a}{\sigma}\sim\N(0,1).
\]

\end{proof}

Определим $s^2_a = \sum_{i=1}^n (x_i-a)^2/n$.

\begin{prop*}
$ns^2_a/\sigma^2 \sim \chi^2(n)$.
\end{prop*}
\begin{proof}
\[
\chi^{2}=\frac{ns_{{a}}^{2}}{\sigma ^{2}}=\dfrac{n\cdot1/n\cdot\sum_{i=1}^{n}\left(x_{i}-{a}\right)^{2}}{\sigma ^{2}}=\sum_{i=1}^{n}\left(\frac{x_{i}-{a}}{\sigma }\right)^{2}\sim\chi^{2}(n).
\]

\end{proof}

\begin{prop*}
$ns^2/\sigma^2 = (n-1)\tilde{s}^2/\sigma^2 \sim \chi^2(n-1)$.
\end{prop*}

\begin{proof}
См. раздел~\ref{exa:s2-chi2}).
\end{proof}

\begin{proof}[Альтернативное доказательство]
По определению запишем
\[
\underbrace{\D\hat{\xi}_{n}}_{s^{2}}=\D(\hat{\xi}_{n}-{a})=\underbrace{\E(\hat{\xi}_{n}-{a})^{2}}_{s_{{a}}^{2}}-\underbrace{(\E(\hat{\xi}_{n}-{a}))^{2}}_{\left(\bar{x}-{a}\right)^{2}}.
\]
Домножив обе части на $n/\sigma ^{2}$, получим
\[
\frac{ns^{2}}{\sigma ^{2}}=\frac{ns_{{a}}^{2}}{\sigma ^{2}}-\frac{n(\bar{x}-{a})^{2}}{\sigma ^{2}}=\underbrace{\frac{ns_{{a}}^{2}}{\sigma ^{2}}}_{\sim\chi^{2}(n)}-\underbrace{\left(\frac{\sqrt{n}\left(\bar{x}-{a}\right)}{\sigma }\right)^{2}}_{\sim\chi^{2}(1)}\implies\frac{ns^{2}}{\sigma ^{2}}\sim\chi^{2}(n-1).
\]
\end{proof}
\begin{rem*}
Для строгого доказательства, нужно использовать независимость $\bar{x}^{2}$
и $s^{2}$ (см. раздел~\ref{exa:x2-s2-indep}).
\end{rem*}

\begin{prop*}
Следующая статистика имеет распределение Стьюдента:
\[
t=\sqrt{n-1}\,\frac{\bar{x}-a}{s}=\frac{\sqrt{n-1}(\bar{x}-a)}{\sqrt{n-1}/\sqrt{n}\cdot\tilde{s}}=\sqrt{n}\,\frac{\bar{x}-a}{\tilde{s}}\xrightarrow[n\to\infty]{}\N(0,1).
\]

\end{prop*}

\begin{prop*}
$t=\sqrt{n-1}\,\frac{\bar{x}-a}{s}=\sqrt{n}\,\frac{\bar{x}-a}{\tilde{s}}
\sim\t(n-1)$.
\end{prop*}
\begin{proof}
\[
t=\frac{\sqrt{n-1}\,(\bar{x}-a)}{s}=\frac{\sqrt{n-1}\left(\dfrac{\bar{x}-a}{\sigma}\right)}{s/\sigma}=\frac{\left(\dfrac{\bar{x}-a}{\sigma}\right)}{\sqrt{\dfrac{s^{2}/\sigma^{2}}{n-1}}}=\frac{\dfrac{\sqrt{n}(\bar{x}-a)}{\sigma}}{\sqrt{\dfrac{ns^{2}/\sigma^{2}}{n-1}}}=\frac{\zeta}{\sqrt{\eta/(n-1)}}\sim\t(n-1),
\]
 поскольку
\[
\zeta=\dfrac{\sqrt{n}(\bar{x}-a)}{\sigma}\sim\N(0,1),\quad\eta=\frac{ns^{2}}{\sigma^{2}}\sim\chi^{2}(n-1).
\]
и они независимы (также пока без доказательства --- используется техника квадратичных форм или можно доказать через разложение дисперсии).
\end{proof}
\begin{comment}
\[
\sum_{i=1}^{n}(x_{i}-\E\xi)^{2}=\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}+n(\bar{x}-\E\xi)^{2}
\]
 --- разложение кв. ф.
\end{comment}



\part{Проверка гипотез и доверительные интервалы}

Этот раздел иногда называется <<Confirmatory Data Analysis>> в противовес
<<Exploratory Data Analysis>>, не включающему в себя понятие \emph{гипотезы}.

\begin{comment}
Параметрические методы опираются на

априорное знание параметров распределения (очень часто --- норм)

$n\geq30$

interval \& continuous data

Иначе --- не-параметрические
\end{comment}



\chapter{Построение критерия}
%Тут должно быть про определение критерия и построение его через статистику критерия.

\section{Общие сведения}

Пусть $H_0$ --- это гипотеза, т.е. некоторое предположение о случайной величине $\xi$, которое мы хотим проверить (модель --- это предположение, которое считается верным без проверки).

Проблема: случайно может произойти что угодно, т.е. безошибочных решений практически не бывает. Приходится задавать максимальный уровень вероятности ошибки, на который можно согласиться при принятии решения.

Задаем уровень значимости (significance level) $0<\alpha<1$.

Тогда критерий --- это разбиение множества $V^n$ всевозможных значений выборки $\x$ на две области, критическую $\mathcal{A}^{(\text{крит})}_\alpha$ и доверительную $\mathcal{A}^{(\text{дов})}_\alpha$ так, что
$\alpha_I = \P_{H_0}(\x\in \mathcal{A}^{(\text{крит})}_\alpha) = \alpha$ (здесь $\P_{H_0}$ --- вероятность (мера), соответствующая предположению, что $H_0$ верна).

При проверки гипотезы (уже после эксперимента), если выборка попала в критическую область $\mathcal{A}^{(\text{крит})}_\alpha$, то нулевая гипотеза отвергается, а если в доверительную $\mathcal{A}^{(\text{дов})}_\alpha$, то не отвергается (важно, что именно нет основания отвергнуть, но и принять нельзя).

Обычно разбиение строят с помощью статистики критерия $t=t(\x)$. В этом случае на доверительную и критическую область нужно делать область значений статистики критерия, а это подмножество вещественных чисел.
Поэтому критическая область ${\Ascr}^{(\text{крит})}_\alpha$ выбирается так, чтобы
$\alpha_I=\P_{H_0} (t\in {\Ascr}^{(\text{крит})}_\alpha) = \alpha$.

Допустимо строить разбиение так, чтобы выполнялось $\alpha_I \leq \alpha$ (тогда критерий называется консервативным).

Часто удается построить только асимптотический критерий, когда $\alpha_I \rightarrow \alpha$ при $n\rightarrow \infty$. В этом случае критерий можно применять при достаточно (для критерия) большом объеме выборки, где допустимый объем выборки зависит от скорости сходимости.

Ниже более подробно.

\section{Схема построение критерия на основе статистики критерия}
%\paragraph{Схема построения критерия с помощью статистики критерия}
\begin{enumerate}
\item
Строим статистику критерия $t$ так, что:
\begin{itemize}
\item Cтатистика критерия $t$ должна измерять то, насколько выборка соответствует гипотезе.
В этом случае мы получаем значение статистики критерия для <<идеального соответствия>>.

Например, если гипотеза про математическое ожидание, то $t=\bar{x}-\E\xi$ подходит под это требование.
Если гипотеза про дисперсию, то соответствие правильнее измерять отношением и поэтому подошло бы $t=s^2/\D\xi$.

\begin{example*}
Пусть $H_{0}:\E\xi=a_{0}$; тогда $t=\bar{\mathbf{x}}-a_{0}$ и <<идеальное
значение>> $t=0$.
\end{example*}
\item Распределение $t$ при верной $H_{0}$ должно быть известно хотя бы
асимптотически. Из-за этого часто преобразовывают вариант меры несоответствия, приведенный выше.
Для $H_0:, \E\xi = a_0$ в модели $\xi\sim N(a, \sigma^2)$ с известной дисперсией $\sigma^2$ удобно использовать статистику критерия $t=\sqrt{n}(\bar{\mathbf{x}}-a_{0})/\sigma \sim N(0,1)$.
\begin{example*}
Еще примеры см. в разделе \ref{sec:ttest}.
\end{example*}
\end{itemize}

\item
Строим разбиение области значений статистики критерия $t$ так, что: %
\begin{itemize}
\item $\P(t\in \Ascr_\alpha^{\text{крит}} )= \alpha$.

\item Если альтернативная гипотеза $H_{1}$ (см. про нее в след.разделе) не конкретизирована, то $\Ascr_\a^{\text{(крит)}}$ следует выбрать
так, чтобы она располагалась как можно дальше от идеального значения.

\begin{example*}
В случае $t\sim\N(0,1)$ при идеальном значении $0$, разумно определить $\Ascr_\a^{\text{(крит)}}$
<<на хвостах>> графика $\pdf_{\N(0,1)}$ симметрично по обе стороны
от 0 так, что для $\Ascr_\a^{\text{(крит)}}=(-\infty,-t_{\a})\cup(t_{\a},\infty)$
\[
\a/2=\int_{-\infty}^{-t_{\a}}\pdf_{\N(0,1)}(y)\d y=\int_{t_{\a}}^{+\infty}\pdf_{\N(0,1)}(y)\d y.
\]
 Иными словами,
\[
\a/2=1-\cdf_{\N(0,1)}(t_{1})\implies t_{1}=\cdf_{\N(0,1)}^{-1}(1-\a/2)
\]
и аналогично для $t_{0}$.
\end{example*}
\item Если $H_{1}$ известна, то $\Ascr_\a^{\text{(крит)}}$ выбирается так,
чтобы максимизировать мощность критерия против альтернативы $H_{1}$, определения см. ниже.
\end{itemize}
\end{enumerate}

\section{Ошибки первого и второго рода}
\begin{defn*}[Ошибки I-го и II-го родов]
Пусть мы проверяем нулевую гипотезу $H_{0}$. Зафиксируем альтернативную гипотезу $H_{1}$ --- такое отклонение
от $H_{0}$, что его обнаружение важно для нас. Тогда
\begin{itemize}
\item ошибка \emph{I-го рода} есть отвержение $H_{0}$, при верной $H_{0}$; соответствующая
вероятность есть
\[
\a_{\mathrm{I}}:=\P_{H_{0}}(\mathbf{x}\in\Ascr_{\text{крит}}^{(\a)}).
\]


\begin{rem*}
Для точного критерия вероятность $\a_{\mathrm{I}}$ совпадает с уровнем
значимости $\a$.
\end{rem*}
\item ошибка \emph{II-го рода} есть не отвержение $H_{0}$ при верной $H_{1}$;
соответствующая вероятность есть
\[
\a_{\mathrm{II}}:=\P_{H_{1}}(\mathbf{x}\in\Ascr_{\text{дов}}^{(\a)}).
\]

\end{itemize}
\end{defn*}

\begin{defn*}
\emph{Мощность} критерия против альтернативы есть
\[
\b:=1-\a_{\mathrm{II}}=1-\P_{H_{1}}(\mathbf{x}\in\Ascr_{\text{дов}}^{(\a)})=\P_{H_{1}}(\mathbf{x}\in\Ascr_{\text{крит}}^{(\a)}).
\]


Иными словами, это способность критерия отличать $H_{1}$ от $H_{0}$.
\end{defn*}

\begin{defn*}
Критерий называется \emph{состоятельным}, если $\b\to1$.\end{defn*}
\begin{rem*}
Утверждать об \emph{отвержении} гипотезы можно с вероятностью ошибки
$\a$ (достаточно малой); утверждать о \emph{принятии} гипотезы можно
с вероятностью ошибки $\a_{\mathrm{II}}$ --- не контролируемой и
могущей быть довольно большой. Поэтому гипотезу $H_0$ можно только
отвергать или не отвергать, так как мы контролируем ошибку неправильного решения (отвергнуть).
Принимать гипотезу нельзя, так как мы не контролируем, вообще говоря, ошибку неправильного решения в случае принятия гипотезы.
\end{rem*}

\begin{rem*}
В связи с введенным понятием мощности можно описать две проблемы:
\begin{itemize}
\item
проблема маленьких объемов выборки состоит в том, что в этом случае мощность маленькая и критерий не заметит отличие от $H_1$ от $H_0$, т.е. с большой вероятностью не отвергнет $H_0$, хотя будет верна $H_1$;
\item
как ни странно, но есть также проблема слишком больших объемов выборки, когда мощность слишком большая, т.е. критерий может отвергнуть $H_0$ с большой вероятностью, даже если она <<почти>> верна (например, из-за ошибок округления).
\end{itemize}
\end{rem*}

Заметим, что вероятность ошибки первого рода $\alpha_I$ фиксирована (как минимум, она ограничена сверху выбранным значением $\alpha$, в то время как $\a_{\mathrm{II}} = \a_{\mathrm{II}}(\alpha, n, H_1)$. Ниже продемонстрируем эту зависимость на примере.

\begin{example*}
\label{exa:h0-h1}Пусть $\xi\sim\N(a,\sigma^{2})$, $\sigma^{2}$
известна --- это модель. Гипотезы имеют вид $H_{0}:a=a_{0},\ H_{1}:a=a_{1}$. Тогда
\[
t=\dfrac{\sqrt{n}\left(\bar{\mathbf{x}}-a_{0}\right)}{\sigma}\sim\N(0,1)\text{ при верной }H_{0}.
\]
 В то же время, поскольку при верной $H_{1}$, $\E\bar{\mathbf{x}}=1/n\cdot\sum_{i=1}^{n}\xi_{i}=n/n\cdot a_{1}$
то
\[
\E t=\frac{\sqrt{n}\left(a_{1}-a_{0}\right)}{\sigma}\implies t\sim\N\left(\frac{\sqrt{n}\left(a_{1}-a_{0}\right)}{\sigma},1\right)\text{ при верной }H_{1}.
\]
(дисперсия, конечно, не меняется при сдвиге).

\begin{figure}[H]
\centering{}\includegraphics[width=0.5\textwidth]{fig/h0_h1}\caption{Плотности распределения $t$ (неоптимальное разбиение)}
\end{figure}


Чтобы минимизировать $\a_{\mathrm{II}}$, логично определить $\Ascr_{\text{крит}}$
только на одном хвосте --- с той стороны, где находится альтернатива.

\begin{figure}[H]
\centering{}\includegraphics[width=0.5\textwidth]{fig/h0_h1-opt}\caption{Плотности распределения $t$ (оптимальное разбиение)}
\end{figure}

Таким образом, мы увидели, что если известна альтернатива, то можно выбирать критерий
(= разбиение на доверительную и критическую области; статистика критерия — лишь удобной
средство для этого), более мощный против именно этой альтернативы. И вообще, разные критерии для одной и той же основной гипотезы сравниваются по мощности против интересующей нас
альтернативы.

Помимо этого, по рисунку видно, что ошибка второго рода $\a_{\mathrm{II}}$ уменьшается при (1) увеличении
ошибки первого рода (уровня значимости $\a$) (поэтому мы ее выбираем максимальной из всех
допустимых); (2) увеличении объема выборки (если $\a_{\mathrm{II}}$ стремится к нулю при увеличении объема
выборки, то критерий называется состоятельным после данной альтернативы) и (3) увеличении
разницы между $H_0$ и $H_1$ в том же смысле, в каком статистика критерия измеряет разницу между выборкой и нулевой гипотезой. Напомним, что мощность критерия против некоторой альтернативы (чего-то
отличного от основной гипотезы) говорит нам о том, насколько хорошо критерий обнаруживает
это отличие.

\end{example*}
%\begin{rem*}
%Помимо этого, по рисунку видно, что минимизировать $\a_{\mathrm{II}}$
%(согласившись на б\'oльшую ошибку первого рода) можно сдвинув вправо
%центр второй плотности, увеличив $n$.
%
%Аналогично, чем $a_{1}$ дальше от $a_{0}$, тем $\a_{\mathrm{II}}$
%меньше. Стоит отметить, что $H_{1}$ не выбирается, но берется из
%смысла задачи.
%\end{rem*}
%
%\begin{example*}[С гранатами]
%\begin{comment}
%@xio: FIXME
%\end{comment}
%\end{example*}

\section{Понятие вероятностного уровня $p$-value.}
\begin{defn*}
\emph{$p$-value} есть такой значение, что при
значениях уровня значимости $\a$, больших $p$-value, $H_{0}$ отвергается (по причине
попадания $t$ в $\Ascr_\a^{\text{крит}}$), а при меньших --- не
отвергается.
\end{defn*}

$p$-value - не вероятность, это пороговое значение. Неформально его можно интерпретировать как меру согласованности $H_{0}$ и выборки. Например, при больших значениях $p$-value практически при всех разумных уровнях значимости гипотеза не отвергается. При близких к нулю значениях $p$-value, наоборот, гипотеза будет отвергаться.

$p$-value --- максимальное значение уровня значимости, при котором гипотеза не отвергается (значение статистики критерия попадает в доверит. область). Или, что эквивалентно, минимальное значение уровня значимости, при котором гипотеза отвергается.

%Все статистические пакеты выдают значение $p$-value в качестве результата проверки гипотезы, так как они не знают, какая вероятность ошибки является допустимой при принятии решения в реальной задаче, где происходит проверка гипотезы.

Заметим, что для вычисления значения $p$-value даже для одной и той же статистики критерия нет единой формулы.

\paragraph{Неформальный комментарий, как находить $p$-value.}

Нужно сначала 1) нарисовать плотность статистики критерия, если верна $H_0$, и на ней сделать разметку --- где там критическая область, где доверительная для некоторого уровня значимости $\a$ ($\a$ равняется вероятности попасть в критическую область, т.е. равно площади под соотв.частью графика плотности). Разбиение на доверительную и критическую область обычно делают так, чтобы в критическую область попадали значения, наиболее далекие от того значения статистики критерия, которое соответствует идеальному соотношению выборки и гипотезы.
2) нарисовать еще раз эту плотность и нанести значение статистики критерия.

Глядя на обе каринки, мысленно или рисуя, меняйте $\a$ и двигайте границу между доверительной и критической областями.
Как упоминали выше, $p$-value --- максимальное значение уровня значимости, при котором гипотеза не отвергается (значение статистики критерия попадает в доверит. область). Или, что эквивалентно, минимальное значение уровня значимости, при котором гипотеза отвергается.
Замечу, что на основе этого определения сразу понятно, как, получив значение $p$-value, определить, при каких уровнях значимости (можно написать неравенство) гипотеза отвергается, а при каких --- нет. Именно в таком виде и надо выдавать ответ.

На основе этих манипуляций должно стать понятно, что нужно для вычисления $p$-value --- считать функцию распределения $F(t)$ или считать $1 - F(t)$, умножать потом на два или нет.
После того как поймете, можно уже находить $p$-value с помощью R или Python. Например, в R есть pnorm --- функция распределения нормального распределения, pt --- распределения Стъюдента, pf --- распределение Фишера.

На всякий случай, замечание про уровень значимости $\a$: уровень значимости задается заранее, еще до проверки гипотезы. Его смысл --- максимальная вероятность ошибки (отвергнуть H0 неправильно), на которую согласен тот, кто будет отвечать за последствия ошибки.
Но проблема в том, что те, кто делают обработку данных (в том числе, компьютеры) --- это не те, которые отвечают за последствия, т.е. не те, кто устанавливают уровень значимости. Поэтому надо дать ответ в общем виде --- в таком диапазоне уровней значимости гипотеза отвергается, а в таком - нет.
Чтобы это сделать, все статистические пакеты выдают $p$-value. На его основе уже можно сформулировать общий ответ: при таких-то уровнях значимости гипотеза отвергается, при таких-то не отвергается.
Иногда, когда некого спросить об уровне значимости, используются некоторые стандартные значения типа 0.05.

%\begin{example*}[Средняя температура в холодильнике]
%Хотят купить холодильник, такой, чтобы температура не опускалась
%ниже 0 --- иначе продукты померзнут. Пусть $\xi\sim\N(a,\sigma^{2}),\ \sigma^{2}=4$.
%Известно количество измерений $n=25$ и $\bar{\mathbf{x}}=0.7$. Выдвинута
%$H_{0}:\E\xi=0$ --- если гипотеза не опровергнется, то холодильник
%не купят.
%
%Зададим уровень значимости $\a_{1}=0.2$ (хранится петрушка) Найдем
%\[
%t_{1}=\cdf_{\N(0,1)}^{-1}(1-\a_{1}/2)=\cdf_{\N(0,1)}^{-1}(0.9)\approx1.28.
%\]
% В то же время, по \ref{sub:E_xi_norm},
%\[
%\N(0,1)\sim t=\dfrac{\sqrt{n}\left(\bar{\mathbf{x}}-a_{0}\right)}{\sigma}=\frac{5(0.7-0)}{2}=1.75.
%\]
% Таким образом, $t\in\Ascr_{\text{крит}}$, $H_{0}$ отвергнется
%и холодильник, быть может, купят.
%
%Пусть $\a_{2}=0.01$ (хранится дорогая красная икра). Тогда $t_{1}\approx2.57$,
%$t=1.75\in\Ascr_{\text{дов}}$ и $H_{0}$ не отвергается; значит,
%холодильник не купят.
%
%Пороговым значением $\a$ ($p$-value) будет
%\[
%2\cdot(1-\cdf_{\N(0,1)}(1.75))\approx0.08.
%\]
%\begin{comment}
%@xio: $p$-value $\to$ 1 это хорошо или плохо?
%\end{comment}
%
%\end{example*}
%
%\begin{example*}[С мышой]
%В одном из рукавов T-образного лабиринта лежит морковка. К развилке
%по лабиринту бежит мышь и 7 раз из 10 поворачивает в направлении морковки.
%На основании этих данных хотим сделать вывод, что мышь чует морковь
%на расстоянии, после чего написать научную статью.
%\begin{itemize}
%\item $\xi\sim\Ber(p)$. Выдвинем гипотезу, что мышь \emph{не} чует морковку,
%$H_{0}:p=p_{0}=0.5$. По ЦПТ,
%\[
%\frac{\sum_{i=1}^{n}\xi_{i}-\sum_{i=1}^{n}\E\xi_{i}}{\sqrt{\sum_{i=1}^{n}\D\xi_{i}}}=\frac{n\bar{\mathbf{x}}-n\E\xi}{\sqrt{n}\sqrt{\D\xi}}=\frac{\sqrt{n}\left(\bar{\mathbf{x}}-p_{0}\right)}{\sqrt{p_{0}(1-p_{0})}}\tod\N(0,1).
%\]
% Пусть это будет статистикой критерия с идеальным значением 0. Тогда
%\[
%t=\frac{\sqrt{n}\left(\bar{\mathbf{x}}-p_{0}\right)}{\sqrt{p_{0}(1-p_{0})}}=\frac{\sqrt{10}\cdot0.2}{0.5}\approx1.2649\implies p\text{-value}=2\cdot(1-\cdf_{\N(0,1)}(1.2649))\approx0.2.
%\]
% Значит, с уровнем значимости $0.2$ гипотеза не отвергается. Хочется
%иметь, конечно, один из стандартных уровней значимости, например $0.1$.
%\item Увеличим мощность критерия, введя альтернативную гипотезу, что мышь
%чует морковку (в предположении, что все мыши любят морковь и к ней
%бегут), $H_{1}:p_{1}>p_{0}$. По \ref{exa:h0-h1}, можем устроить
%односторонний критерий, так что $p$-value теперь 0.1. Однако пользуемся
%асимптотическим критерием при $n=10$.
%\item Воспользуемся точным односторонним критерием со статистикой
%\[
%t:=n\bar{\mathbf{x}}=\sum_{i=1}^{n}x_{i}\sim\Bin(n,p_{0})
%\]
% и идеальным значением $np_{0}$. Тогда $t=10\cdot0.7=7$. При уровне
%значимости $\a=0.1$ успешно попадаем в критическую область, вследствие
%чего $H_{0}$ отвергается, и можем публиковаться.
%
%
%\begin{figure}[H]
%\centering{}\includegraphics[width=0.5\textwidth]{fig/mouse-bin-cdf}
%\end{figure}
%
%
%\end{itemize}
%\end{example*}

%\section{Понятие гипотезы и критерия}
%
%Пусть $x_{1},\ldots,x_{n}\sim\Pcal$, $\Pscr$ --- множество всех
%распределений. О $\Pcal$ возможно делать утверждения вида $\Pcal\in\Pscr'\subset\Pscr$.
%Стоит задача выбрать такое утверждение, что оно некоторым наилучшим
%образом соответствует выборке.
%\begin{defn*}
%\emph{Модель} --- это предположение о выделенном классе $\Pscr_{M}\subset\Pscr$,
%которому принадлежит $\Pcal$ (допустим, $\Pscr_{M}=\left\{ \N({a},\sigma_{0})\right\} $,
%где $\sigma_{0}$ --- фиксированное значение). Иными словами, это
%утверждение о $\Pcal$, которое считается верным и не проверяется.
%\end{defn*}
%
%\begin{defn*}
%\emph{Гипотеза} --- утверждение о $\Pcal$, требующее проверки. Гипотеза
%называется \emph{простой}, если она соответствует только одному распределению
%в рамках рассматриваемой модели:
%\[
%H:\Pcal=\Pcal_{0}\in\Pscr_{M}
%\]
% (например, $\Pcal_{0}=\N({a}_{0},\sigma_{0})$) или \emph{сложной},
%если целому множеству:
%\[
%H:\Pcal\in\Pscr'\subset\Pscr_{M}
%\]
% (например, $\Pscr'=\left\{ \N({a},\sigma_{0}):{a}>0\right\} $).
%\end{defn*}
%Очень часто возникает (и далее рассматривается) случай выдвижения
%только лишь двух гипотез: $H_{0}:\Pcal\in\Pscr_{0}\subset\Pscr$ ---
%\emph{нулевой}, основной и $H_{1}:\Pcal\in\Pscr_{1}\subset\Pscr$
%--- \emph{альтернативной}. $H_{1}$ учитывает отклонения от $H_{0}$,
%обнаружение которых желательно. Возможны варианты:
%\begin{itemize}
%\item простая $H_{0}$ и простая $H_{1}$;
%\item простая $H_{0}$ и сложная $H_{1}$;
%\item сложная $H_{0}$ и сложная $H_{1}$.\end{itemize}
%\begin{defn*}
%\emph{Критерий} есть отображение
%\[
%\varphi:\mathbf{x}\mapsto\left\{ H_{0},H_{1}\right\} .
%\]
%
%\end{defn*}
%Критерий <<решает>>, противоречат или не противоречат выдвинутой
%гипотезе выборочные данные.
%\begin{defn*}
%Говорят, что гипотеза \emph{отвергается}, если $\varphi(\mathbf{x})=H_{1}$
%и \emph{не отвергается} иначе.
%\end{defn*}
%Так как в заданной постановке любой критерий принимает не более двух
%значений, то $\dom\varphi$ разбивается на два дизъюнктных множества
%$\Ascr_{\text{крит}}$ и $\Ascr_{\text{дов}}$, называемых
%\emph{критической} и \emph{доверительной} областями, таких, что
%\[
%\varphi(\mathbf{x})=\begin{cases}
%H_{0} & \mathbf{x}\in\Ascr_{\text{дов}}\\
%H_{1} & \mathbf{x}\in\Ascr_{\text{крит}.}
%\end{cases}
%\]
%
%
%Поскольку выборка конечного объема позволяет делать только вероятностные
%заключения, со статистическим критерием ассоциированы ошибки $i$-ых
%родов.
%\begin{defn*}
%Говорят, что произошла \emph{ошибка $i$-го рода} критерия $\varphi$,
%если критерий отверг верную гипотезу $H_{i-1}$. Соответствующие вероятности
%обозначаются
%\[
%\a_{i}(\varphi)=\P_{H_{i-1}}(\varphi(\mathbf{x})\neq H_{i-1}).
%\]
%
%
%Поскольку в рассмотрение введены только $H_{0}$ и $H_{1}$, возможны
%ошибки \emph{I-го рода} --- принятие случайного различия за систематическое
%и \emph{II-го рода} --- принятие наблюдаемого различия за случайный
%эффект с соответствующими вероятностями
%\begin{eqnarray*}
%\a_{\I} & = & \P_{H_{0}}(\varphi(\mathbf{x})\neq H_{0})=\P_{H_{0}}(\mathbf{x}\in\Ascr_{\text{крит}})\\
%\a_{\II} & = & \P_{H_{1}}(\varphi(\mathbf{x})\neq H_{1})=\P_{H_{1}}(\mathbf{x}\in\Ascr_{\text{дов}}).
%\end{eqnarray*}
%\end{defn*}
%\begin{rem*}
%Если $H_{i}$ --- сложная гипотеза, то $\a_{i+1}(\varphi)$ будет
%зависеть от того, на каком именно распределении $\Pcal$, отвечающем
%$H_{i}$, вычисляется эта вероятность.
%\end{rem*}
%
%\begin{defn*}
%\emph{Мощность} критерия против альтернативы это вероятность справедливо
%отвергнуть $H_{0}$:
%\[
%\b=1-\a_{\mathrm{II}}=1-\P_{H_{1}}(\varphi(\mathbf{x})=H_{0})=\P_{H_{1}}(\varphi(\mathbf{x})=H_{1}).
%\]
% Иными словами, это способность критерия отличать $H_{0}$ от $H_{1}$.
%\end{defn*}
%
%\section{Построение оптимальных критериев}
%
%Рассмотрим пример.
%\begin{example*}
%\label{exa:phi-b}Пусть $\xi\sim\N({a},1)$ и $\mathbf{x}=\left\{ x\right\} $.
%Выдвинем $H_{0}:{a}=0$ против простой альтернативы $H_{1}:{a}=1$
%Рассмотрим критерий
%\[
%\varphi_{b}(\mathbf{x})=\begin{cases}
%H_{0} & x<b\\
%H_{1} & x\geq b.
%\end{cases}
%\]
%(FIXME: Рисунок) Ясно, что из всего множества критериев $\left\{ \varphi_{b}\right\} $,
%не все одинаково хорошо описывают нормальную выборку: так, тождественный
%$\varphi_{\infty}(\mathbf{x})\equiv H_{0}$ будет вести себя хуже
%любого $\varphi_{b},\ b<\infty$.
%\end{example*}
%Выбирать оптимальный критерий можно тремя способами: минимаксным,
%Байесовским подходами и выбором наиболее мощного критерия.
%\begin{defn*}
%$\varphi^{(1)}$ не хуже $\varphi^{(1)}$ в \emph{минимаксном} смысле,
%если
%\[
%\max(\a_{\I}(\varphi^{(1)}),\a_{\II}(\varphi^{(1)}))\leq\max(\a_{\I}(\varphi^{(2)}),\a_{\II}(\varphi^{(2)})).
%\]
%Если $\phi^{*}$ не хуже всех остальных в этом смысле, то он называется
%\emph{минимаксным}.\end{defn*}
%\begin{example*}
%$\varphi_{1/2}$ минимаксный.\end{example*}
%\begin{defn*}
%Пусть известны $r=\P(H_{0})$, $s=1-r=\P(H_{1})$ (или задана линейная
%функция потерь, равная $r$ в случае ошибки 1-го рода и $s$ --- второго).
%Тогда $\varphi^{(1)}$ не хуже $\varphi^{(1)}$ в \emph{байесовском}
%смысле, если
%\[
%r\a_{\I}(\varphi^{(1)})+s\a_{\II}(\varphi^{(1)})\leq r\a_{\I}(\varphi^{(2)})+s\a_{\II}(\varphi^{(2)}).
%\]
%Если $\phi^{*}$ не хуже всех остальных в этом смысле, то он называется
%\emph{байесовским}.\end{defn*}
%\begin{rem*}
%Короче говоря, по правилу полной вероятности это вероятность ошибки
%критерия:
%\[
%\P(H_{0})\P(\underbrace{\varphi(\mathbf{x})=H_{1}}_{\mathrm{Err}}\mid H_{0})+\P(H_{1})\P(\underbrace{\varphi(\mathbf{x})=H_{0}}_{\mathrm{Err}}\mid H_{1})=\P(\mathrm{Err}).
%\]
%\end{rem*}
%\begin{example*}
%$\varphi_{1/2}$ байесовский с $s=r$.\end{example*}
%\begin{defn*}
%Пусть до эксперимента зафиксирован \emph{уровень значимости}\footnote{Неформально, $\a$ обратно пропорциональна <<строгости>> критерия,
%выбираемой экспериментатором.} критерия $\a\in\left[0,1\right]$. Критерий
%\[
%\varphi^{*}\in K_{\a}=\left\{ \varphi(\mathbf{x})\mid\a_{\I}(\varphi)\leq\a\right\}
%\]
% называется \emph{наиболее мощным} критерием, если
%\[
%\a_{\II}(\varphi^{*})\leq\a_{\II}(\varphi),\quad\forall\phi\in K_{\a}.
%\]
%\end{defn*}
%\begin{rem*}
%Стандартные уровни значимости: $\a=0.05$ или $\a=0.01$.
%\end{rem*}
%Все три подхода могут быть сведены к универсальному критерию --- критерию
%отношения правдоподобия.
%
%В примере \ref{exa:phi-b} интуитивно ясно, что следует выбрать ту
%гипотезу, значение плотности которой в точке $x$ больше другой. В
%случае, если $n>1$, справедливо взять произведение плотностей ---
%т.е. функций правдоподобия и рассматривать их отношение
%\[
%L(\mathbf{x})=\frac{\L_{2}(\thb\mid\mathbf{x})}{\L_{1}(\thb\mid\mathbf{x})}.
%\]
%Выбор гипотезы затем делать по тому, больше или меньше $L$ единицы.
%Однако, чтобы учесть произвольный уровень ошибки, следует сравнивать
%не с 1, а с константой $c$.
%\begin{defn*}
%Пусть $\Pcal_{0},\Pcal_{1}$ либо одновременно дискретны, либо непрерывны.
%Пусть также $\neg\exists c_{0}:\P_{H_{0}}(L(\mathbf{x})=c_{0})=0$
%(в противном случае критерий не сможет различить гипотезы на множестве
%не-нулевой меры) --- т.е., $\P_{H_{0}}(L(\mathbf{x})\geq c)$ непрерывна
%по $c>0$. Тогда \emph{критерием отношения правдоподобия} называется
%\[
%\varphi_{c}(\mathbf{x})=\begin{cases}
%H_{0} & L(\mathbf{x})<c\\
%H_{1} & L(\mathbf{x})\geq c.
%\end{cases}
%\]
%
%\end{defn*}
%
%\subsection{Явный вид оптимальных критериев}
%\begin{claim*}
%В предположениях из определения, критерий отношения правдоподобия
%является
%\begin{enumerate}
%\item минимаксным при $c:\a_{\I}(\varphi_{c})=\a_{\II}(\varphi_{c})$;
%\item байесовским при заданных $r,s:c=r/s$;
%\item (лемма Неймана-Пирсона) наибольшей мощности при заданном $\a:\a_{\I}(\varphi_{c})=\a.$
%\end{enumerate}
%\end{claim*}
%\begin{example*}
%Пусть $\xi\sim\N({a},1)$. $H_{0}:{a}={a}_{0}$, $H_{1}:{a}={a}_{1}>{a}_{0}$.
%Критическая область задается неравенством
%\[
%L(\mathbf{x})=\frac{\L_{2}(\thb\mid\mathbf{x})}{\L_{1}(\thb\mid\mathbf{x})}=\exp\left(\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}\left(x_{i}-{a}_{0}\right)^{2}-\left(x_{i}-{a}_{1}\right)^{2}\right)\geq c,
%\]
%упрощая которое получают
%\[
%\bar{x}\geq\frac{\ln c}{({a}_{1}-{a}_{0})n}+\frac{{a}_{1}-{a}_{0}}{2}.
%\]
%
%\begin{itemize}
%\item Пусть критерий байесовский с $r=1/4$, $s=3/4$; тогда $c=1/3$.
%\item Пусть критерий наибольшей мощности c заданным $\a$, критическая область
%задается $\bar{x}\geq c_{0}.$ Тогда
%\[
%\a_{\I}=\P_{H_{0}}(\bar{x}\geq c_{0})=\P_{H_{0}}(\sqrt{n}(\bar{x}-{a}_{0})\geq\sqrt{n}\left(c_{0}-{a}_{0}\right))=1-\cdf_{\N(0,1)}\left(\sqrt{n}\left(c_{0}-{a}_{0}\right)\right)=\a
%\]
%если
%\[
%c_{0}=\frac{z_{1-\a}}{\sqrt{n}}+{a}_{0}.
%\]
%
%\item Пусть критерий минимаксный. Тогда
%\[
%\a_{\II}=\cdf_{\N(0,1)}\left(\sqrt{n}(c_{0}-{a}_{1})\right)=1-\cdf_{\N(0,1)}(\sqrt{n}(c_{0}-{a}_{0}))=\cdf_{\N(0,1)}(\sqrt{n}({a}_{0}-c_{0}))
%\]
% если
%\[
%c_{0}-{a}_{1}={a}_{0}-c_{0},\quad c_{0}=\frac{{a}_{0}+{a}_{1}}{2}.
%\]
%
%\end{itemize}
%\end{example*}
%\begin{defn*}
%Критерий называется \emph{состоятельным }против альтернативы $H_{1}$,
%если $\forall\Pcal_{1}\in\Pscr_{1}$
%\[
%\b(\varphi,\Pcal_{1})=1-\P_{\Pcal_{1}}(\varphi(\mathbf{x})=H_{0})\xrightarrow[n\to\infty]{}1.
%\]
%\end{defn*}
%\begin{example*}
%Критерий наибольшей мощности
%\[
%\varphi(\mathbf{x})=\begin{cases}
%H_{0} & \bar{x}<\frac{z_{1-\a}}{\sqrt{n}}+{a}_{0}\\
%H_{1} & \text{иначе}
%\end{cases}
%\]
% является состоятельным, потому что
%\[
%\a_{\II}(\varphi)=\P_{H_{1}}\left(\bar{x}-\frac{z_{1-\a}}{\sqrt{n}}<{a}_{0}\right),
%\]
% но, при верной $H_{1}$,
%\[
%\xi_{n}=\bar{x}-\frac{z_{1-\a}}{\sqrt{n}}\toP{a}_{1},
%\]
% из чего следует слабая сходимость --- сходимость $\cdf_{\xi_{n}}(x)\to\cdf_{{a}_{1}}(x)=\P({a}_{1}<x)$
%по всех точках; учитывая $\cdf_{{a}_{1}}({a}_{0})=0$,
%\[
%\a_{\II}(\varphi)=F_{\xi_{n}}({a}_{0})\to F_{{a}_{1}}({a}_{0})=0.
%\]
%
%\end{example*}
%
%\begin{defn*}
%Если
%\begin{labeling}{00.00.0000}
%\item [{$\a_{\I}=\a$}] то критерий называется \emph{точным},
%\item [{$\a_{\I}\xrightarrow[n\to\infty]{}\a$}] \emph{асимптотическим},
%\item [{$\a_{\I}>\a$}] \emph{радикальным} (т.е. отвергает гипотезу чаще,
%чем точный),
%\item [{$\a_{\I}<\a$}] \emph{консервативным} (если гипотеза отвергнута,
%то уж наверняка).
%\end{labeling}
%\end{defn*}
%
%
%
%\subsection{О постановке $H_{0}$}
%
%\label{rem:form-h0}Задача может допускать две постановки; в этом
%случае, поскольку $\a_{\I}$ ($=\a$ для правильно построенного критерия)
%контролируется экспериментатором, проверяется отрицание эффекта, который
%хотят подтвердить: к примеру, что новое лекарство \emph{не} лучше
%старого; если $H_{0}$ отвергнется, это будет означать, что новое
%лекарство-таки лучше старого с вероятностью не ниже, чем $1-\a$.
%
%\begin{comment}
%Некто заявляет о способности не глядя угадывать масти предлагаемых
%карт. Выдвигаем гипотезу $H_{0}$: человек \emph{не} экстрасенс, так
%что угадывает с вероятностью $p=1/4$. Предлагаем отгадать масть 25
%карт; число угаданных карт есть $x$. Наибольшей <<строгости>> будет
%соответствовать $x=25$, таким образом
%\[
%\P_{H_{0}}(x\in\Ascr_{\text{крит}})=\P_{H_{0}}(x=25)=\left(\frac{1}{4}\right)^{25}\approx10^{-15}=\a.
%\]
% Если же для нас достаточно 24 угаданных карт из 25, то $\a$ будет
%побольше: $\P_{H_{0}}(x\geq24)=\P_{H_{0}}(x=24)+\P_{H_{0}}(x=25)$
%и т.д. Зафиксированная до эксперимента $\a$ определяет количество
%карт $c$ такое, что при угадывании большего количества $H_{0}$ будет
%считаться опровергнутой:
%\[
%\a_{\I}=\P_{H_{0}}(x\geq c)\leq\a.
%\]
% Среди всех таких $c$ следует выбрать наименьшее, чтобы минимизировать
%ошибку второго рода.
%\end{comment}
%\begin{comment}
%@xio: TODO: дописать, почему.
%\end{comment}
%\begin{comment}
%@xio: нельзя ли сказать, что в этом случае использовалась простая
%статистика --- количество угаданных карт?
%\end{comment}
%
%\begin{example*}[С гранатами]
%\begin{comment}
%@xio: FIXME
%\end{comment}
%FIXME
%\end{example*}
%
%\subsection{Об отвержении гипотезы}
%
%Утверждать об \emph{отвержении} гипотезы можно с вероятностью ошибки
%$\a$ (достаточно малой и произвольно задаваемой экспериментатором);
%утверждать о \emph{принятии} гипотезы можно с вероятностью ошибки
%$\a_{\mathrm{II}}$ --- не контролируемой и потенциально довольно
%большой. Иными словами, попадание в доверительную область может означать
%как то, что $H_{0}$ верна, так и то, что верна $H_{1}$, но для распознания
%этого не хватило мощности. Поэтому безопасно гипотезу можно только
%отвергать или не отвергать. Можно и принять, если известна мощность
%критерия против всех возможных альтернатив, экспериментатора устраивающая.
%
%При высокой вероятности ошибки II-го рода возможна ситуация не отвержении
%заведомо ложной гипотезы. Это, в свою очередь, может произойти из-за
%маленького объема выборки (критерий не находит разницу, см. \ref{exa:h0-h1}).
%Чем больше объем выборки, тем мощность больше, но возможна ситуация,
%когда критерий чувствителен настолько, что находит разницу там, где
%не должен --- например, при генерации <<идеальным>> датчиком случайных
%чисел, начиная с какого-то объема заведомо истинная гипотеза может
%быть отвергнута из-за ошибок в точности представления чисел с плавающей
%точкой.
%\begin{defn*}
%\emph{Критерий} называется \emph{одно- (двух-) сторонним} по тому,
%где находится альтернатива.
%\end{defn*}
%
%\begin{defn*}
%\emph{Критическая область} называется \emph{одно- (дву-) сторонней
%}по тому, где формально располагается $\Ascr_{\text{крит}}$.
%\end{defn*}

%\section{Построение критерия при помощи статистики критерия}
%\begin{defn*}
%\emph{Статистика критерия} есть отображение
%\[
%T:\mathbf{x}\mapsto y\in\RR
%\]
% такое, что при верной $H_{0}$, $T\tod\mathcal{Q}$, где $\mathcal{Q}$
%--- полностью известное непрерывное распределение, а при верной $H_{1}$
%известно поведение $T$.
%\end{defn*}
%Поскольку распределение $T$ при верной $H_{0}$ известно, она должна
%вести себя как любая другая случайная величина из $\mathcal{Q}$ ---
%попадание в некоторые области менее вероятно, чем в другие. Поэтому
%разумно разбить $\supp T$ по уровню значимости $\a$ на $\Ascr_{\text{крит}}\sqcup\Ascr_{\text{дов}}$
%так, что попадание в $\Ascr_{\text{крит}}$ \emph{при верной $H_{0}$}
%происходит с заранее зафиксированной (малой) вероятностью $\a$. Значит,
%если $T(\mathbf{x})\in\Ascr_{\text{крит}}$, то с некоторой же
%вероятностью можно заявлять об отвержении $H_{0}$. Таким образом,
%$T$ измеряет то, насколько выборка соответствует гипотезе.
%
%
%\section{Разбиение на доверительную и критические области}
%
%Разберем на примере построение разбиения. Пусть $\xi\sim\N({a},\sigma^{2})$,
%$H_{0}:{a}={a}_{0}$ и фиксирован $\a$.
%Используется статистика
%\[
%t=z=\sqrt{n}\frac{\bar{x}-a_{0}}{\sigma}\sim\N(0,1).
%\]
%В зависимости от $H_{1}$, возможны варианты.
%
%
%\subsection{Простая альтернатива}
%
%\label{exa:h0-h1}Пусть $H_{1}:{a}={a}_{1}$, причем ${a}_{1}>{a}_{0}$.
%Тогда, поскольку при верной $H_{1}$, $\E\bar{x}=1/n\cdot\sum_{i=1}^{n}\xi_{i}=n/n\cdot{a}_{1}$,
%то
%\[
%\E T=\frac{\sqrt{n}\left({a}_{1}-{a}_{0}\right)}{\sigma}\implies T\sim\N\left(\frac{\sqrt{n}\left({a}_{1}-{a}_{0}\right)}{\sigma},1\right)\text{ при верной }H_{1}.
%\]
%(дисперсия, конечно, не меняется при сдвиге).
%
%\begin{figure}[H]
%\centering{}\includegraphics[width=0.5\textwidth]{fig/h0_h1}\caption{Плотности распределения $z$ (неоптимальное разбиение)}
%\end{figure}
%
%
%Чтобы минимизировать $\a_{\mathrm{II}}$, логично определить $\Ascr_{\text{крит}}$
%только на одном хвосте --- с той стороны, где находится альтернатива.
%Помимо этого, по рисунку видно, что минимизировать $\a_{\mathrm{II}}$
%(согласившись на б\'oльшую ошибку первого рода) можно сдвинув вправо
%центр второй плотности, увеличив $n$. Аналогично, чем ${a}_{1}$
%дальше от ${a}_{0}$, тем $\a_{\mathrm{II}}$ меньше.
%
%\begin{figure}[H]
%\centering{}\includegraphics[width=0.5\textwidth]{fig/h0_h1-opt}\caption{Плотности распределения $z$ (оптимальное разбиение)}
%\end{figure}
%
%
%Таким образом, $\Ascr_{\text{крит}}=(C,+\infty)$, $C=z_{1-\a}$.
%
%Разумеется, если ${a}_{_{1}}<{a}_{0}$, то $\Ascr_{\text{крит}}=(-\infty,C),$
%$C=z_{\a}$.
%
%
%\subsection{Односторонний критерий (сложная альтернатива)}
%
%В общем случае, пусть $H_{1}:{a}={a}_{1}\ \forall{a}_{1}>{a}_{0}$;
%тогда по ЗБЧ $\bar{x}-{a}_{0}\to{a}_{1}-{a}_{0}>0$ и $T\to+\infty$.
%Следовательно, чтобы максимизировать величину $\b=\P_{H_{1}}(\mathbf{x}\in\Ascr_{\text{крит}}^{(\a)}),$
%следует разместить $\Ascr$ на правом хвосте плотности: $\Ascr_{\text{крит}}=\left(z_{1-\a},\infty\right)$.
%Для $H_{1}:{a}={a}_{1}\ \forall{a}_{1}<{a}_{0}$ аналогично $\Ascr_{\text{крит}}=\left(-\infty,z_{\a}\right)$.
%
%
%\subsection{Двусторонний критерий}
%
%\begin{comment}
%В случае $T\sim\N(0,1)$, разумно определить $\Ascr_{\text{крит}}$
%<<на хвостах>> графика $\pdf_{\N(0,1)}$ симметрично по обе стороны
%от 0 так, что для $\Ascr_{\text{крит}}=(-\infty,T_{0})\cup(T_{1},\infty)$
%\[
%\a/2=\int_{-\infty}^{T_{0}}\pdf_{\N(0,1)}(y)\d y=\int_{T_{1}}^{+\infty}\pdf_{\N(0,1)}(y)\d y.
%\]
% Иными словами,
%\[
%\a/2=1-\cdf_{\N(0,1)}(T_{1})\implies T_{1}=\cdf_{\N(0,1)}^{-1}(1-\a/2)
%\]
%и аналогично для $T_{0}$.
%\end{comment}
%Пусть $H_{1}:\E\xi={a}_{1}\neq{a}_{0}$; тогда по ЗБЧ $\bar{x}-{a}_{0}\to{a}_{1}-{a}_{0}$
%и $\left|T\right|\xrightarrow[n\to\infty]{}\infty$, откуда $\Ascr_{\text{крит}}=\RR\setminus\left(z_{\a/2},z_{1-\a/2}\right)=\RR\setminus\left(-C,C\right)$,
%где $C=-z_{\a/2}$.
%
%В общем виде, с использованием статистики, критерий может быть определен
%как
%\[
%\varphi(\mathbf{x})=\begin{cases}
%H_{0} & \left|T(\mathbf{x})\right|<C\\
%H_{1} & \left|T(\mathbf{x})\right|\geq C,
%\end{cases}
%\]
%где \emph{критическое значение} $C$ определяется из уравнения $\a=\P(\left|T\right|\geq C)$.
%
%Иногда вместо сравнения значения $T$ с критическим вычисляют \emph{реально
%достигнутый уровень значимости критерия} (<<\emph{$p$-value}>>).
%\begin{defn*}
%\emph{$p$-value }значения статистики $T$ на выборке $\mathbf{x}$
%есть вероятность, взяв выборку из распределения $H_{0}$, получить
%по ней большее отклонение $\left|T(\mathbf{x})\right|$ эмпирического
%от истинного распределения, чем получено по проверяемой выборке:
%\[
%\pv=\a^{*}=\P_{H_{0}}(\left|T\right|\geq\left|T(\mathbf{x})\right|).
%\]
%\begin{comment}
%@xio: TODO: картина с bellcurve, $t_{\mathrm{obs}}$ пойнтом и закрашенной
%вероятностью справа от нее.
%\end{comment}
%
%\end{defn*}
%Значит, критерий может быть задан и как
%\[
%\varphi(\mathbf{x})=\begin{cases}
%H_{0} & \a^{*}>\a\\
%H_{1} & \a^{*}\leq\a.
%\end{cases}
%\]
%
%\begin{rem*}
%$p$-value обратно пропорционален <<существенности>> результата.%
%\begin{comment}
%Иными словами, это мера согласованности $H_{0}$ и выборки.
%\end{comment}
%
%\end{rem*}
%
%\begin{rem*}
%Пусть $\a^{*}=0.05$. Это значит, что в среднем всего лишь 5\% <<контрольных>>
%выборок, удовлетворяющих основной гипотезе, будут обладать большим
%отклонением $\left|T(\mathbf{x})\right|$ по сравнению с тестируемой
%выборкой --- последняя ведет себя не хуже, чем 5\% <<правильных>>
%выборок.
%\end{rem*}
%
%\section{Схема построения критерия с помощью статистики }
%\begin{enumerate}
%\item Фиксируют предположение относительно данных.
%\item Выдвигают $H_{0}$ и $H_{1}$.
%
%\begin{itemize}
%\item $H_{0}$ формулируется согласно замечанию \ref{rem:form-h0}.
%\item $H_{1}$ ставится по смыслу задачи (см. далее).
%\end{itemize}
%\item Выбирают подходящий критерий и статистику $T$.
%\item Фиксируют уровень значимости $\a$.
%\item Строят разбиение $\im T$ с помощью квантилей распределения $T$ (при
%верной $H_{0}$) так, чтобы $\a_{\I}=\a$; положение квантилей выбирают
%из известного поведения статистики при верной $H_{1}$.
%\item Считают значение статистики и принимают решение об отвержении $H_{0}$
%одним из способов:
%\[
%\varphi(\mathbf{x})=\begin{cases}
%H_{0} & \left|T(\mathbf{x})\right|<C\\
%H_{1} & \left|T(\mathbf{x})\right|\geq C,
%\end{cases}\qquad\varphi(\mathbf{x})=\begin{cases}
%H_{0} & \a^{*}>\a\\
%H_{1} & \a^{*}\leq\a.
%\end{cases}
%\]
%\end{enumerate}
%\begin{example*}[Средняя температура в холодильнике]
%Хотят купить холодильник, такой, чтобы температура держалась в окрестности
%0. Известно количество измерений $n=25$ и $\bar{x}=0.7$.
%\begin{enumerate}
%\item Пусть $\xi\sim\N({a},4)$.
%\item Выдвинута $H_{0}:\E\xi={a}_{0}=0$ --- если гипотеза опровергнется,
%то холодильник не купят; $H_{1}:\E\xi={a}_{1}\neq{a}_{0}$.
%\item Поскольку модель нормальная и известная $\sigma^{2}$, выберем статистику
%\ref{sub:E_xi_norm} (<<z-test>>):
%\[
%z=\dfrac{\sqrt{n}\left(\bar{x}-{a}_{0}\right)}{\sigma}\sim\N(0,1)\text{ при верной }H_{0}.
%\]
% Идеальное значение статистики --- 0.
%\item Зафиксируем два уровня значимости: $\a^{(1)}=0.2$ (храним петрушку)
%и $\a^{(2)}=0.01$ (храним дорогую красную икру).
%\item Построим разбиение. Поскольку ${a}_{1}\neq{a}_{0}$, то $\Ascr_{\text{крит}}=\RR\setminus(z_{\a/2},z_{1-\a/2})$.
%Для введенных уровней значимости это означает
%
%\begin{enumerate}
%\item $\Ascr_{\text{крит}}^{(\a^{(1)})}\approx\RR\setminus\left(-1.28,1.28\right)$.
%\item $\Ascr_{\text{крит}}^{(\a^{(2)})}\approx\RR\setminus\left(-2.576,2.576\right)$.
%\end{enumerate}
%\item Посчитаем
%\[
%z(\mathbf{x})=\dfrac{\sqrt{n}\left(\bar{x}-{a}_{0}\right)}{\sigma}=\frac{5(0.7-0)}{2}=1.75.
%\]
%Дальнейшее принятие решения возможно на основании критического значения
%или $p$-value.
%
%\begin{itemize}
%\item По вычислению критического значения:
%
%\begin{itemize}
%\item $z\in\Ascr_{\text{крит}}^{(\a^{(1)})}$, $H_{0}$ отвергается,
%холодильник не покупают.
%\item $z\in\Ascr_{\text{дов}}^{(\a^{(2)})}$, $H_{0}$ не отвергается,
%холодильник, быть может, покупают.
%\end{itemize}
%\item Можно посчитать $p$-value:
%\[
%2\cdot(1-\cdf_{\N(0,1)}(1.75))\approx0.08.
%\]
% Поэтому при уровне значимости $\a^{(1)}=0.2>0.08$ $H_{0}$ отвергается,
%а при $\a^{(2)}=0.01<0.08$ не отвергается.
%\end{itemize}
%\end{enumerate}
%\end{example*}
%
%\begin{example*}[С мышой]
%В одном из рукавов T-образного лабиринта лежит морковка. К развилке
%по лабиринту бежит мышь и 7 раз из 10 поворачивает в направлении морковки.
%На основании этих данных хотим сделать вывод, что мышь чует морковь
%на расстоянии, после чего написать научную статью.
%\begin{itemize}
%\item $\xi\sim\Ber(p)$. Выдвинем гипотезу, что мышь \emph{не} чует морковку,
%$H_{0}:p=p_{0}=0.5$. Поскольку $\E\xi=p$, воспользуемся критерием
%для проверки гипотезы о значении среднего с идеальным значением 0;
%учитывая $\D\xi=p(1-p)$,
%\begin{eqnarray*}
%T & = & \sqrt{n}\frac{\bar{x}-p_{0}}{\sqrt{p_{0}(1-p_{0})}}\tod\N(0,1).\\
% & = & \frac{\sqrt{10}\cdot0.2}{0.5}\approx1.2649\implies p\text{-value}=2\cdot(1-\cdf_{\N(0,1)}(1.2649))\approx0.2.
%\end{eqnarray*}
% Значит, с уровнем значимости $0.2$ гипотеза не отвергается. Хочется
%иметь, конечно, один из стандартных уровней значимости, например $0.1$.
%\item Увеличим мощность критерия, введя альтернативную гипотезу, что мышь
%чует морковку (в предположении, что все мыши любят морковь и к ней
%бегут), $H_{1}:p_{1}>p_{0}$. По \ref{exa:h0-h1}, можем устроить
%односторонний критерий, так что $p$-value теперь 0.1. Однако пользуемся
%асимптотическим критерием при $n=10$.
%\item Воспользуемся точным односторонним критерием со статистикой
%\[
%T:=n\bar{x}=\sum_{i=1}^{n}x_{i}\sim\Bin(n,p_{0})
%\]
% и идеальным значением $np_{0}$. Тогда $T=10\cdot0.7=7$. При уровне
%значимости $\a=0.1$ успешно попадаем в критическую область, вследствие
%чего $H_{0}$ отвергается, и можем публиковаться.
%
%
%\begin{figure}[H]
%\centering{}\includegraphics[width=0.5\textwidth]{fig/mouse-bin-cdf}
%\end{figure}
%
%
%\end{itemize}
%\end{example*}
%\begin{rem*}
%Исторически существовало два подхода к проверке гипотез: Фишера (<<significance
%test>>) и Неймана-Пирсона (<<hypothesis testing>>).
%\begin{description}
%\item [{Фишер}] Выдвигается $H_{0}$. Подсчитывается и сообщается точное
%$p$-value. Если результат <<незначительный>>, не делается никаких
%выводов об отвержении $H_{0}$, но делается возможным дополнительный
%сбор данных.
%\item [{Нейман-Пирсон}] Выдвигаются $H_{1},H_{2}$, фиксируются $\a_{\I},\a_{\II}$
%и $n$. На этом основании определяются $\Ascr_{\text{крит}}$
%для каждой гипотезы. Если данные попали в $\Ascr_{\text{крит}}$
%$H_{1}$ --- предпочитается $H_{2}$, иначе $H_{2}$.
%\end{description}
%Современная теория проверки гипотез есть смесь двух этих подходов,
%не всегда консистентная. Вводные курсы по статистике формулируют теорию,
%похожую на significance testing Фишера; при повышенных требованиях
%к математической строгости, пользуются теорией Неймана-Пирсона.
%\end{rem*}
%
%\begin{rem*}[О графике $p$-values]
%Поскольку
%\[
%\a_{\mathrm{I}}\gets\P_{H_{0}}(T\in\Ascr_{\text{крит}})=\P_{H_{0}}(p\text{-value}<\a),
%\]
%то $p$-value по распределению стремятся к $\U(0,1)$ при верной $H_{0}$.
%Это соображение позволяет визуально проверить истинность гипотезы:
%достаточно несколько (много) раз произвести эксперимент, для каждой
%выборки $\bar{x}^{(i)}$ посчитать свой $p$-value, построить
%график и убедиться, что получилась прямая. %
%\begin{comment}
%@xio: проверить
%\end{comment}
%{} Для подсчета мощности $\b=\P_{H_{1}}(\mathbf{x}\in\Ascr_{\text{крит}}^{(\a)})=\P_{H_{1}}(p\text{-value}<\a)$
%считать выборку с параметрами $H_{1}$, а $T$ относительно $H_{0}$.
%\end{rem*}
\chapter{Проверка гипотезы о значении параметра (характеристики)}

\section{Проверка гипотезы о значении мат. ожидания ($t$-критерий)}
\label{sec:ttest}
$H_{0}:\E\xi={a}={a}_{0}$. Соответствие оценки математического ожидания
гипотезе удобно выражать разницей $\bar{x}-{a}_{0}$ с <<идеальным>>
значением 0. Отнормировав эту разницу, получим статистику, распределение
которой известно.


\subsection{$\protect\D\xi=\sigma^{2}<\infty$}
\begin{prop*}
Пусть $\D\xi=\sigma^{2}<\infty$; тогда используется следующая статистика ($z$-score)
\[
t=z=\sqrt{n}\frac{(\bar{x}-{a}_{0})}{\sigma}\xrightarrow[n\to\infty]{}\N(0,1)
\]
 \end{prop*}
\begin{prop*}
При условии $\xi\sim \N(a,\sigma^2)$,
\[
t=z\sim\N(0,1).
\]
\end{prop*}
\begin{proof}
\[
z=\frac{\bar{x}-{a}_{0}}{\sqrt{\D\bar{x}}}=\sqrt{n}\,\dfrac{\bar{x}-{a}_{0}}{\sigma}\sim\N(0,1).
\]

\end{proof}

%\paragraph{Разбиение}
%\begin{description}
%\item [{$H_{1}:\E\xi\neq{a}_{0}$}] $\Ascr_{\text{крит}}=\RR\setminus\left(z_{\a/2},z_{1-\a/2}\right)$
%\item [{$H_{1}:\E\xi>{a}_{0}$}] $\Ascr_{\text{крит}}=(z_{1-\a},\infty)$
%\item [{$H_{1}:\E\xi<{a}_{0}$}] $\Ascr_{\text{крит}}=(-\infty,z_{\a})$
%\end{description}

\subsection{$\protect\D\xi$ неизвестна}
\label{sect:expect_gen}
\begin{prop*}
Пусть $\D\xi$ неизвестна; тогда используется следующая статистика
\[
t=\sqrt{n-1}\,\frac{\bar{x}-{a}_{0}}{s}=\sqrt{n}\,\frac{\bar{x}-{a}_{0}}{\tilde{s}}\xrightarrow[n\to\infty]{}\N(0,1).
\]
\end{prop*}


Сходимость к нормальному распределению следует из модифицированной теоремы Леви (модифицированная ЦПТ), которая позволяет заменять дисперсию на ее состоятельную оценку с сохранением сходимости к тому же нормальному распределению.


\begin{prop*}
При условии нормальности данных,
\[
t\sim\t(n-1).
\]
\end{prop*}

\begin{comment}
\[
\sum_{i=1}^{n}(x_{i}-\E\xi)^{2}=\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}+n(\bar{x}-\E\xi)^{2}
\]
 --- разложение кв. ф.
\end{comment}



%\paragraph{Разбиение}
%\begin{description}
%\item [{$H_{1}:\E\xi\neq{a}_{0}$}] $\Ascr_{\text{крит}}=\RR\setminus\left(\qnt_{\t(n-1)}(\a/2),\qnt_{\t(n-1)}(1-\a/2)\right)$
%\item [{$H_{1}:\E\xi>{a}_{0}$}] $\Ascr_{\text{крит}}=(\qnt_{\t(n-1)}(1-\a),\infty)$
%\item [{$H_{1}:\E\xi<{a}_{0}$}] $\Ascr_{\text{крит}}=(-\infty,\qnt_{\t(n-1)}\a)$\end{description}
%\begin{rem*}
%При нормальной аппроксимации $\qnt_{\t(n-1)}$ заменить на $\N(0,1)$.
%\end{rem*}

\subsection{Проверка гипотезы о мат.ож. в модели с одним параметром}
\label{sect:expect_one}

Разница с общим случаем состоит в том, что в параметрической модели с одним параметром не нужно оценивать дисперсию. Так как все выражается через этот параметр, то имеем формулу для дисперсии через значение параметра, предполагаемое в нулевой гипотезе.

\paragraph{$z$-критерий для пропорции в модели Бернулли}
Пусть $\xi\sim\Ber(p)$. Поскольку $\E\xi=p$, можно воспользоваться
только что введенной статистикой; учитывая $\D\xi=p(1-p)$, получаем статистику критерия для гипотезы $H_0: p=p_0$:
\[
t=\sqrt{n}\frac{\bar{x}-p_{0}}{\sqrt{p_{0}(1-p_{0})}}\tod\N(0,1).
\]

\paragraph{$z$-критерий для интенсивности потока в модели Пуассона}
Пусть $\xi\sim\Pois(\lambda)$. Поскольку $\E\xi=\lambda$, можно воспользоваться
только что введенной статистикой; учитывая $\D\xi=\lambda$, получаем статистику критерия для гипотезы $H_0: \lambda=\lambda_0$:
\[
t=\sqrt{n}\frac{\bar{x}-\lambda_{0}}{\sqrt{\lambda_{0}}}\tod\N(0,1).
\]


\section{Проверка гипотезы о значении дисперсии в нормальной модели (критерий
$\chi^{2}$)}

Пусть $\xi\sim\N({a},\sigma^{2})$. $H_{0}:\D\xi=\sigma^{2}=\sigma_{0}^{2}$.
Соответствие оценки дисперсии гипотезе удобно выражать отношением
$s^{2}/\sigma_{0}^{2}$ (или $s_{{a}}/\sigma_{0}^{2}$ если ${a}$
известно) с <<идеальным>> значением 1. Домножив на $n$, получим
статистику, распределение которой известно.
%\begin{rem*}[Важное]
%Критерий работает только в нормальной модели и не становится асимптотически
%нормальным в ином случае!
%\end{rem*}

\subsection{$\protect\E\xi={a}<\infty$}
\begin{prop*}
Пусть $\E\xi={a}<\infty$; При условии нормальности данных используется
следующая статистика:
\[
\chi^{2}=n\frac{s_{{a}}^{2}}{\sigma_{0}^{2}}\sim\chi^{2}(n).
\]
\end{prop*}

%\paragraph{Разбиение}
%\begin{description}
%\item [{$H_{1}:\D\xi\neq\sigma_{0}^{2}$}] $\Ascr_{\text{крит}}=\RR_{+}\setminus\left(\qnt_{\chi^{2}(n)}(\a/2),\qnt_{\chi^{2}(n)}(1-\a/2)\right)$
%\item [{$H_{1}:\D\xi>\sigma_{0}^{2}$}] $\Ascr_{\text{крит}}=(\qnt_{\chi^{2}(n)}(1-\a),\infty)$
%\item [{$H_{1}:\D\xi<\sigma_{0}^{2}$}] $\Ascr_{\text{крит}}=(0,\qnt_{\chi^{2}(n)}\a)$
%\end{description}

\subsection{$\protect\E\xi$ неизвестно}
\begin{prop*}
Пусть $\E\xi$ неизвестно. При условии нормальности данных используется
следующая статистика:
\[
\chi^{2}=n\frac{s^{2}}{\sigma_{0}^{2}}=(n-1)\frac{\tilde{s}^{2}}{\sigma_{0}^{2}}\sim\chi^{2}(n-1).
\]
\end{prop*}

%\paragraph{Разбиение}
%\begin{description}
%\item [{$H_{1}:\D\xi\neq\sigma_{0}^{2}$}] $\Ascr_{\text{крит}}=\RR_{+}\setminus\left(\qnt_{\chi^{2}(n-1)}(\a/2),\qnt_{\chi^{2}(n-1)}(1-\a/2)\right)$
%\item [{$H_{1}:\D\xi>\sigma_{0}^{2}$}] $\Ascr_{\text{крит}}=(\qnt_{\chi^{2}(n-1)}(1-\a),\infty)$
%\item [{$H_{1}:\D\xi<\sigma_{0}^{2}$}] $\Ascr_{\text{крит}}=(0,\qnt_{\chi^{2}(n-1)}\a)$\end{description}
\begin{xca*}
$s^{2}=1.44,\bar{x}=55,n=101$. Проверить гипотезу $\sigma_{0}^{2}=1.5$
в нормальной модели.\end{xca*}
\begin{solution}
Воспользуемся статистикой
\[
\chi^{2}=\frac{ns^{2}}{\sigma_{0}^{2}}=101\cdot0.96=96.96.
\]
 <<Идеальные>> значения близки к $\E\xi_{\chi^{2}(100)}=100$, так
что определим критическую область на концах плотности:
\[
\pv/2=\cdf_{\chi^{2}(100)}(96.96)=\mathtt{pchisq(96.96,100)}\approx0.43\implies\pv\approx0.86.
\]

\begin{rem*}
Можно посчитать и по таблицам для нормального распределения. Раз
\[
\frac{\eta_{m}-\E\eta_{m}}{\sqrt{\D\eta_{m}}}\xrightarrow[m\to\infty]{\d}\N(0,1),
\]
 то
\[
\frac{96.96-100}{\sqrt{200}}\approx-0.215\implies\pv/2=\Phi(-0.215)\approx0.415.
\]

\end{rem*}
\end{solution}

\section{Асимптотический критерий для гипотезы о значении параметра на основе MLE}

Если умеем находить $\hat{\th}_{\mathrm{MLE}}$, то по асимптотической
нормальности,
\[
\frac{\hat{\th}_{\mathrm{MLE}}-\E\hat{\th}_{\mathrm{MLE}}}{\sqrt{\D\hat{\th}_{\mathrm{MLE}}}}\tod\N(0,1),
\]
 по асимптотической несмещенности,
\[
\frac{\hat{\th}_{\mathrm{MLE}}-\th}{\sqrt{\D\hat{\th}_{\mathrm{MLE}}}}\tod\N(0,1),
\]
и, учитывая асимптотическую эффективность ($\D\hat{\th}_{\mathrm{MLE}}I_{n}(\th)\xrightarrow[n\to\infty]{}1$),
запишем статистику для $H_0: \th=\th_0$:
\[
t=\left(\hat{\th}_{\mathrm{MLE}}-\th_0\right)\sqrt{I_{n}(\th_0)}\tod\N(0,1).
\]

\textbf{Задание}: построить критерий для гипотезы о значении параметра для распределений Бернулли и Пуассона.

\chapter{Доверительные интервалы}
\label{ch:confidence}

\section{Мотивация и определение}

Точечные оценки не дают информации о том, насколько (количественно) настоящее значение далеко от оценки.
%Для построенных оценок может понадобиться оценка точности. Так, даже
%состоятельная оценка может не быть в полном смысле <<точной>>: пусть
%$\th_{n}^{*}\toP\th_{0}$; тогда
%\[
%\hat{\th}_{n}'=\begin{cases}
%c & n<N\gg1\\
%\hat{\th}_{n} & \text{иначе}
%\end{cases}
%\]
% все-равно будет, конечно, состоятельной.
%
%$\D\hat{\th}_{n}$ может быть не всегда просто вычислить и использовать.
\begin{defn*}
$\left[b_{1},b_{2}\right]$ --- \emph{доверительный интервал} для
параметра $\theta$ с уровнем доверия $\gamma\in\left[0,1\right]$,
если $\forall\th$
\[
\P(\theta\in\left[b_{1},b_{2}\right])=\gamma,\quad\text{где }b_{1}=b_{1}(\mathbf{x}),b_{2}=b_{2}(\mathbf{x}),
\]
т.е. границы доверительного интервала --- это статистики (функции от выборки, случайные величины <<до эксперимента>>).
\end{defn*}

\begin{rem*}
Если выборка из дискретного распределения, то $b_{1},b_{2}$ --- тоже дискретны.
Поэтому наперед заданную точность получить может не получиться; в
таких случаях знак <<$=$>> заменяют <<$\geq$>>. Аналогично с
заменой на <<$\xrightarrow[n\to\infty]{}$>> для асимптотических доверительных интервалов, когда точные получить невозможно или трудоемко.
\end{rem*}

\section{Доверительный интервал для проверки гипотезы о значении параметра}

Зафиксируем $H_{0}:\th=\th_{0}$ и $\gamma=1-\alpha$, где $\a$ играет
роль уровня значимости. По определению доверительного интервала, $\P(\theta\in\left[a_{\gamma}(\mathbf{x}),b_{\gamma}(\mathbf{x})\right])=\gamma.$
Тогда
\[
\P(\theta\in\left[b_1(\mathbf{x}),b_2(\mathbf{x})\right])=
\gamma=1-\a \implies\a=1-\P(\theta\in\left[a_{\gamma}(\mathbf{x}),b_{\gamma}(\mathbf{x})\right])=
\P(\th\not\in\left[a_{\gamma}(\mathbf{x}),b_{\gamma}(\mathbf{x})\right]).
\]
 %и $\Ascr_{\text{крит}}=\RR\setminus\left[b_1(\mathbf{x}),b_2(\mathbf{x})\right].$
Соответственно,
\[
\begin{cases}
\text{отвергаем\ } H_0 \text{, если\ }& \th_0\not\in\left[b_1(\mathbf{x}),b_2(\mathbf{x})\right]\\
\text{не отвергаем\ } H_0 \text{, если\ }& \th_0\in\left[b_1(\mathbf{x}),b_2(\mathbf{x})\right].
\end{cases}
\]
 Вероятность ошибки первого рода равна $\a$, что соответствует определению критерия.
 Заметим, что здесь мы пользуемся общим определением критерия, а не частным случаем, когда критерий строится через статистику критерия.

\section{Доверительные интервалы для математического ожидания и дисперсии
в нормальной модели}
\begin{assumption*}
Пусть $\xi\sim\N({a},\sigma^{2})$.
\end{assumption*}

\subsection{Доверительный интервал для ${a}$}
\begin{itemize}
\item Пусть $\sigma^{2}$ известно. Свяжем ${a}$ с выборкой через статистику критерия \\ $t=\sqrt{n}\frac{(\bar{x}-{a} )}{\sigma}\sim\N(0,1)$:
\[
\gamma=\P(c_{1}<t<c_{2})=\P\left(c_{1}<\sqrt{n}\frac{(\bar{x}-{a})}{\sigma}<c_{2}\right)=\P\left({a}\in\left(\bar{x}-\frac{\sigma c_{2}}{\sqrt{n}},\bar{x}-\frac{\sigma c_{1}}{\sqrt{n}}\right)\right).
\]
 Решений уравнения $\P\left(c_{1}<\sqrt{n}(\bar{x}-{a})/\sigma<c_{2}\right)=\Phi(c_{2})-\Phi(c_{1})=\gamma$
бесконечно много. Чем $\left[c_{1},c_{2}\right]$ короче, тем лучше.
Поскольку $\Phi$ симметрична и унимодальна,
\[
\begin{aligned}c_{1}=-c_{\gamma}\\
c_{2}=c_{\gamma},
\end{aligned}
\quad\text{где }c_{\gamma}=\cdf_{\N(0,1)}^{-1}\left(\gamma+\frac{1-\gamma}{2}\right)=x_{\frac{1+\gamma}{2}}.
\]
\begin{comment}
Картиночка с нормальной плотностью и $\gamma$ в серединке
\end{comment}
{} Наконец,
\[
\P\left({a}\in\left(\bar{x}\pm\frac{\sigma}{\sqrt{n}}c_{\gamma}\right)\right)=\gamma.
\]

\item Пусть $\sigma^{2}$ неизвестно. По аналогии,
\[
\gamma=\P\left(c_{1}<\frac{\sqrt{n-1}(\bar{x}-{a} )}{s}<c_{2}\right)=\P\left({a} \in\left(\bar{x}\pm\frac{c_{\gamma}s}{\sqrt{n-1}}\right)\right),\quad c_{\gamma}=\cdf_{\t(n-1)}^{-1}\left(\frac{1+\gamma}{2}\right)
\]
 и
\[
\P\left({a} \in\left(\bar{x}\pm\frac{\tilde{s}}{\sqrt{n}}c_\gamma\right)\right)=\gamma.
\]
\end{itemize}
\begin{xca*}
Пусть $s^{2}=1.21,\bar{x}=1.9,n=36.$ Построить 95\% доверительный
интервал для $\E\xi$.\end{xca*}
\begin{solution}
\[
c_{\gamma}=\mathtt{qt(0.975,35)}\approx2.03\implies\left(1.9\pm\frac{2.03\cdot\sqrt{1.21}}{\sqrt{35}}\right)=\left(1.52;2.28\right).
\]

\end{solution}

\subsection{Доверительный интервал для $\sigma^{2}$}
\begin{itemize}
\item Пусть ${a}$ известно. Поскольку плотность $\chi^{2}$ становится
все более симметричной с ростом $n$, примем
\[
c_{1}=\cdf_{\chi^{2}(n)}^{-1}\left(\frac{1-\gamma}{2}\right),\ c_{2}=\cdf_{\chi^{2}(n)}^{-1}\left(\frac{1+\gamma}{2}\right).
\]
 Тогда
\[
\P\left(c_{1}<\frac{ns_{{a}}^{2}}{\sigma ^{2}}<c_{2}\right)=
\gamma\iff\P\left(\sigma ^{2}\in\left(\frac{ns_{{a}}^{2}}{c_2},\frac{ns_{{a}}^{2}}{c_1}\right)\right)=\gamma.
\]

\item Пусть ${a}$ неизвестно. Тогда аналогично
\[
\P\left(\sigma ^{2}\in\left(\frac{ns^{2}}{c_2},\frac{ns^{2}}{c_1}\right)\right)=\gamma,
\]
где
\[
c_{1}=\cdf_{\chi^{2}(n-1)}^{-1}\left(\frac{1-\gamma}{2}\right),\ c_{2}=\cdf_{\chi^{2}(n-1)}^{-1}\left(\frac{1+\gamma}{2}\right).
\]

\end{itemize}

%\begin{defn*}
%Случайная величина $g(x_{1},\ldots,x_{n},\th)$ называется \emph{центральной
%статистикой параметра} $\th$, если
%\begin{enumerate}
%\item Её распределение (<<центральное распределение>>) не зависит от распределения
%$\th$.
%\item $G_{n}$ (функция распределения центрального распределения) непрерывна.
%\item $\forall z_{1},z_{2}$ и $\Pcal_{\th}$-почти всюду
%\[
%z_{1}<g(x_{1},\ldots,x_{n},\th)<z_{2}
%\]
% монотонно разрешимо относительно $\th$, т.е.
%\[
%\exists f_{1},f_{n}:f_{1}(x_{1},\ldots,x_{n},\th,z_{1},z_{2})<\th<f_{2}(x_{1},\ldots,x_{n},\th,z_{1},z_{2}).
%\]
%
%\end{enumerate}
%\end{defn*}
%Рассмотрим всегда разрешимое
%\begin{eqnarray*}
%\gamma & = & G_{n}(z_{2})-G_{n}(z_{1})=\P(z_{1}<g(x_{1},\ldots,x_{n},\th)<z_{2})\\
% & = & \P(\underbrace{f_{1}(z_{1},z_{2},x_{1},\ldots,x_{n})}_{c_{1}}<\th<\underbrace{f_{2}(z_{1},z_{2},x_{1},\ldots,x_{n})}_{c_{2}}).
%\end{eqnarray*}
%



%\end{document}
\section{Асимптотический доверительный интервал для математического ожидания
в модели с конечной дисперсией}

Если модель неизвестна, но известно, что $\D\xi<\infty$, можно построить
доверительный интервал для $\E\xi={a}$, не задавая параметрическую модель.
Пусть $\left\{ x_{i}\right\} $
$\iid$, тогда
\[
t=\frac{\sqrt{n}\left(\bar{x}-{a}\right)}{\sigma}\xrightarrow[n\to\infty]{}\N(0,1).
\]
 Если заменить $\sigma$ на ее состоятельную оценку ($s$), то по модифицированной теореме Леви (будет у Владимира Викторовича в след.году) сходимость не испортится. Тогда
\[
\P\left(\E\xi\in\left(\bar{x}\pm\frac{sc_{\gamma}}{\sqrt{n}}\right)\right)\xrightarrow[n\to\infty]{}\gamma,\quad c_{\gamma}=\cdf_{\t(n-1)}^{-1}\left(\frac{1+\gamma}{2}\right).
\]

Все это так же, как было в разделе \ref{sect:expect_gen}.

Аналогично разделу \ref{sect:expect_one}, доверительные интервалы можно улучшить (сделать большее точными, т.е. вероятность попадания в них ближе к $\gamma$ при фиксированном $n$) в параметрической модели, если вместо независимой оценки дисперсии использовать оценку, полученную на основе оценок параметров. Разница в том, что там можно было использовать значение параметра, взятое из гипотезы, а в доверительных интервалах  придется подставлять оценки (или решать нелинейные неравенства, см. следующий раздел).
%Альтернативно замену $\sigma$ на $s$ можно обосновать по теореме
%Слуцкого.
%\begin{claim*}[Слуцкий]
%Если $\xi_{n}\tod\xi,\ \eta_{n}\toP c$, то $\xi_{n}+\eta_{n}\tod\xi+c$
%и $\xi_{n}\eta_{n}\tod c\xi$.
%\end{claim*}
%Используя тот факт, что $s\toP\sigma$, запишем
%\[
%\P\left(c_{1}<\frac{\sqrt{n}\left(\bar{x}-{a}\right)}{\sigma}\frac{\sigma}{s}<c_{2}\right)\xrightarrow[n\to\infty]{}\Phi(c_{2})-\Phi(c_{1}).
%\]

\section{Асимптотический доверительный интервал для параметра на основе MLE}

В точности, как было при проверке гипотез,
\[
T=\left(\hat{\th}_{\mathrm{MLE}}-\th\right)\sqrt{I_{n}(\th)}\tod\N(0,1).
\]
 Чтобы по аналогии с предыдущим выразить $\th$ в $\P(c_{1}<T<c_{2})=\P(\left|T\right|<c_{\gamma})=\gamma$,
необходимо знать зависимость $I_{n}(\th)$ от  $\th$. Для $\Pois$ и $\Ber$
разрешение неравенства относительно $\th$ эквивалентно решению неравенства для квадратичного полинома. %
\begin{comment}
@xio: FIXME: Не симметрично; но для $\hat{\lambda}$ не будет $<0$;
для $\hat{p}\in[0,1]$.
\end{comment}


В общем случае, можно вместо $\th$ в $I_{n}(\th)$ подставить $\hat{\th}_{\mathrm{MLE}}$
(при $n\to\infty$ это не должно сильно испортить дело), откуда
\begin{eqnarray}
\P\left(-c_{\gamma}<\left(\hat{\th}_{\mathrm{MLE}}-
\th\right)\sqrt{I_{n}(\hat{\th}_{\mathrm{MLE}})}<c_{\gamma}\right)\to
\gamma\iff \\
\P\left(\th\in\left(\hat{\th}_{\mathrm{MLE}}\pm\frac{c_{\gamma}}{\sqrt{I_{n}(\hat{\th}_{\mathrm{MLE}})}}\right)\right)\to\gamma,
\end{eqnarray}
 где
\[
T\tod\N(0,1)\implies c_{\gamma}=\cdf_{\N(0,1)}^{-1}\left(\frac{1+\gamma}{2}\right).
\]

\begin{example*}
$\xi\sim\Pois(\lambda)$. В разделе~\ref{exa:mle-pois} получали: $\hat{\lambda}_{\mathrm{MLE}}=\bar{x}$ и
$I_{n}(\lambda)=n/\lambda$ и $I_{n}(\hat\lambda)=n/\bar{x}$,
откуда
\[
\P\left(\lambda\in\left(\bar{x}\pm c_\gamma\frac{\sqrt{\bar{x}}}{\sqrt{n}}\right)\right) \xrightarrow[n\to\infty]{}\gamma.
\]
 %
\begin{comment}
@xio: FIXME: для $\E\xi$, $\sqrt{\bar{x}}=S\because\widehat{\D\xi}=\bar{x}=\hat{\lambda}$?
\end{comment}

\end{example*}
\begin{rem*}
Этот доверительный интервал не очень хорош, потому что может включать значения меньше 0.
\end{rem*}

\begin{example*}
$\xi\sim\Ber(p)$. $p=\E\xi$. $\hat{p}=\bar{x}$, откуда
\[
\P\left(p\in\left(\hat{p}\pm c_{\gamma}\frac{\sqrt{\hat{p}(1-\hat{p})}}{\sqrt{n}}\right)\right)\xrightarrow[n\to\infty]{}\gamma.
\]

\begin{rem*}
Этот доверительный интервал не очень хорош, потому что не обязательно принадлежит
$\left[0,1\right]$.
\end{rem*}
\end{example*}

\textbf{Задание}: Построить хорошие доверительные интервалы в моделях Бернулли и Пуассона, решив неравенства с квадратными уравнениями.

\textbf{Задание}: Построить доверительный интервал для параметра $\lambda$ экспоненциального распределения $\Exp(\lambda)$.


\section{Использование $\SE$ для проверки гипотез и построения доверительных интервалов}

Пусть оценка $\hat{\th}_{n}$ имеет (асимптотически) нормальное распределение и является асимптотически несмещенной. Тогда
 доверительный интервал уровня $\gamma$ для $\th$ (т.е. такой интервал, в котором лежит $\gamma$ всех значений величины)
задается как
\[
\hat{\th}_{n}\pm c_{\gamma}\sqrt{\D\hat{\th}_{n}},
\]
где $c_{\gamma}$ --- $(\gamma+1)/2$-квантиль стандартного нормального распределения.
К примеру, для $\N(0,1)$ и 95\%-квантили это был бы интервал$(-1.96;1.96)$, а так нужно передвинуть его на
среднее и растянуть на корень из дисперсии.

Но стандартное отклонение $\sqrt{\D\hat{\th}_{n}}$ распределения
$\hat{\th}_{n}$ можно оценить как $\SE$ (standard error, стандартная ошибка). Значит, доверительный интервал
будет иметь вид
\[
\hat{\th}_{n}\pm c_{\gamma}\SE.
\]

Аналогично, статистика критерия $H_0: \th = \th_0$ будет иметь вид
\[
t = (\hat{\th}_{n} - \th_0)/\SE(\hat{\th}_{n}),
\]
которая имеет асимптотически нормальное распределение $\N(0,1)$ с `идеальным' значение в нуле.

Заметим, что $\SE$ играет роль `сигмы' распределения оценки.
 %
\begin{comment}
Чтобы сходимость оставалась верной, даже если подставляем оценку,
нужна состоятельность.
\end{comment}


\chapter{Критерии проверки гипотезы о согласии с видом распределения}
\section{Критерий $\chi^{2}$}

По выборке возможно проверить гипотезу о виде распределения случайной
величины, реализацией которой является выборка.
Для проверки гипотезы согласия с видом произвольного \emph{дискретного}
распределения используется асимптотический \emph{критерий $\chi^{2}$
(<<chi-squared test for goodness of fit>>).}

\subsection{Распределение с известными параметрами}

Пусть
\[
H_{0}:\Pcal=\Pcal_{0},\text{ где }\Pcal_{0}:\begin{pmatrix}x_{1}^{*} & \dots & x_{k}^{*}\\
p_{1} & \dots & p_{k}
\end{pmatrix}.
\]
 Сгруппируем $\mathbf{x}$; каждому $x_{i}^{*}$ сопоставим \emph{эмпирическую}
абсолютную частоту $\nu_{i}$; тогда $np_{i}$ --- \emph{ожидаемая}
абсолютная частота.

В качестве меры расхождения между эмпирическим и генеральным распределением
рассматривается величина
\[
\sum_{i=1}^{k}c_{i}\left(\frac{\nu_{i}}{n}-p_{i}\right)^{2},\quad c_{i}=\frac{n}{p_{i}},
\]
 откуда записывается статистика критерия
\[
T=\sum_{i=1}^{k}\frac{(\nu_{i}-np_{i})^{2}}{np_{i}}
\]
 с `идеальным' значением 0 (следовательно, критическая область только справа).
\begin{claim*}
$T\tod\chi^{2}(k-1)$.
\end{claim*}

%\paragraph{Разбиение}
%
%$\Ascr_{\text{крит}}=\left(\qnt_{\chi^{2}(k-1)}(1-\a),\infty\right)$
%--- гипотеза отвергается, если расстояние между предполагаемым и наблюдаемым
%распределениями большое.

\begin{defn*}
Критерий \emph{применим}, если $\a_{\mathrm{I}}=\a$ или $\a_{\mathrm{I}}\approx \a$ с достаточной степенью точности.%
\begin{comment}
@xio: так?
\end{comment}
\begin{comment}
$n\downarrow\implies$ не применим
\end{comment}
\end{defn*}
\begin{rem*}
Поскольку критерий асимптотический, с достаточной (тому, кто дает такие рекомендации) степенью точностью
он применим в случае, если
\begin{enumerate}
\item $n\geq50$;
\item $np_{i}\geq5.$
\end{enumerate}
\end{rem*}

\begin{rem*}
Если условие $np_{i}\geq5$ не выполняется, следует объединить состояния,
например, с краев или слева направо; если в хвосте оказалось $<5$,
то следует присоединить к последнему.\end{rem*}

\begin{rem*}
Почему бы не подстраховаться и не объединить состояния так, чтобы было >10? Ответ: теряем в мощности. \\
\textbf{Задание} Привести пример, демонстрирующий потерю мощности.
\end{rem*}


\begin{example*}[С монеткой]
Пусть $n=4040$, $\#\mathrm{H}=2048,\ \#\mathrm{T}=1092$. Проверим
$H_{0}:\Pcal=\Ber(0.5)$ с $\a=0.1$. Условия критерия выполняются,
поэтому посчитаем
\[
T=\frac{(2048-2020)^{2}}{2020}+\frac{(1092-2020)^{2}}{2020}=\frac{28^{2}+28^{2}}{2020}\approx0.78,
\]
откуда
\[
p\text{-value}=1-\cdf_{\chi^{2}(1)}(0.78)\approx0.38.
\]
 $0.38>0.1$, значит $H_{0}$ не отвергается.\end{example*}
\begin{rem*}
Если нужно подстраховаться от подгонки (искусственно составленных под гипотезу выборок), то критическую область можно выбрать с двух сторон, слева и справа. Например, чтобы отверглась гипотеза о $p=0.5$ для альтернирующей (и явно не
случайной) последовательности $\mathbf{x}=(0,1,0,1,\ldots)$ имеет
$T=0$.
Однако, если мы не подозреваем данные в обмане, то так не делают.
\begin{comment}
@xio: вставить про разные виды графиков: не случайная выборка ---
выгнута вверх, не равномерная --- вниз?
\end{comment}

\end{rem*}



%\begin{xca*}
%$n=100$,
%\[
%\begin{pmatrix}\diamondsuit & \heartsuit & \clubsuit & \spadesuit\\
%20 & 30 & 10 & 40
%\end{pmatrix}.
%\]
% Проверить гипотезу, что колода полная.\end{xca*}
%\begin{solution}
%$H_{0}:\Pcal_{\xi}=\U(1/4)$. Поскольку речь идет о согласии с дискретным
%не параметризованным распределением, напрямую воспользуемся критерием
%$\chi^{2}$. Раз все $np_{i}=100\cdot1/4=25,$
%\[
%\chi^{2}=\sum_{i=1}^{k}\frac{(\nu_{i}-np_{i})^{2}}{np_{i}}=1+1+\frac{15^{2}}{25}+\frac{15^{2}}{25}=2+2\cdot9=20.
%\]
% Так как $\chi^{2}\sim\chi^{2}(k-1)=\chi^{2}(3)$ со средним 3, и
%<<идеальное>> значение 0, определим критическую область в правом
%конце плотности. Из этих соображений
%\[
%\pv=1-\cdf_{\chi^{2}(3)}(20)=1-\mathtt{pchisq(20,3)}\approx0.00017.
%\]
%
%\end{solution}

\subsection{Распределение с неизвестными параметрами}

В случае сложной гипотезы $\Pcal\in\left\{ \Pcal(\thb)\right\} _{\thb\in\Theta}$,
$\thb=\left(\th_{1},\ldots,\th_{r}\right)^{\T},$ следует найти оценку
$\hat{\thb}_{\mathrm{MLE}}$ %(или $\hat{\thb}:\hat{\thb}\to\hat{\thb}_{\mathrm{MLE}}$)
по методу максимального правдоподобия. При подстановке оценок вместо
истинных параметров критерий становится консервативным. Чтобы этого
избежать, необходимо сделать поправку на количество параметров ---
отнять $r$. Что приятно, одна и та же поправка работает для всех
распределений; в этом случае,
\[
T =  \sum_{i=1}^{k}\frac{(\nu_{i}-np_{i}(\hat\thb_{\mathrm{MLE}}))^{2}}{np_{i}}
\tod\chi^{2}(k-r-1).
\]

Важно: параметр можно считать известным, только если его значение выбрано без знания, какая получилась выборка.

\paragraph{Оценки по методу минимума хи-квадрат}
Предельное распределение статистики критерия не поменяется, если вместо оценки максимального правдоподобия подставить любую другую оценка с тем же предельным распределением. Рассмотрим оценки по минимуму хи-квадрат:
\[
\thb_{\mathrm{minChiSq}} =
\arg \min_{\thb} \sum_{i=1}^{k}\frac{(\nu_{i}-np_{i}(\thb))^{2}}{np_{i}}
\tod\chi^{2}(k-r-1).
\]

Утверждение (без доказательства) заключается в том, что в условиях регулярности оценки, полученные по методу максимального правдоподобия, эквивалентны оценкам, полученным по методу минимума хи-квадрат. Таким образом, в критерии хи-квадрат можно использовать статистику в виде
\[
T =  \min_{\thb} \sum_{i=1}^{k}\frac{(\nu_{i}-np_{i}(\thb))^{2}}{np_{i}}
\tod\chi^{2}(k-r-1).
\]


%\begin{xca}
%60 человек купило подарок сразу, 10 со второго раза, 20 с третьего,
%10 с четвертого:
%\[
%\begin{pmatrix}0 & 1 & 2 & 3\\
%60 & 10 & 20 & 10
%\end{pmatrix}.
%\]
% Проверить гипотезу о том, что это выборка из геометрического распределения.\end{xca}
%\begin{solution}
%$H_{0}:\Pcal_{\xi}=\Geom(p)$. Воспользуемся критерием $\chi^{2}$
%для параметризированного распределения $\Geom(\hat{p}_{\MLE}).$
%
%Найдем
%\[
%\hat{p}_{\MLE}=\argmax_{p}\log\L(\mathbf{x};p)\Longleftarrow\frac{\d}{\d p}\log\L(\mathbf{x};\hat{p}_{\MLE})=0.
%\]
% Так как $\pdf_{\Geom(p)}(k)=(1-p)^{k}p$,
%\begin{eqnarray*}
%\log\L(\mathbf{x};p) & = & \log\prod_{k=1}^{n}(1-p)^{k}p=\log(1-p)^{n\bar{x}}p^{n}=n\bar{x}\log(1-p)+n\log p\\
% & = & n(\bar{x}\log(1-p)+\log p)
%\end{eqnarray*}
% откуда
%\[
%\frac{\d}{\d p}\log\L(\mathbf{x};p)=n\left(-\frac{\bar{x}}{1-p}+\frac{1}{p}\right)=0\iff1-p-p\bar{x}=0\iff p=\frac{1}{1+\bar{x}}.
%\]
% Учитывая
%\[
%\bar{x}=0.1+2\cdot0.2+3\cdot0.1=0.8,
%\]
% найдем
%\[
%\hat{p}_{\MLE}=\frac{1}{1+0.8}\approx0.55.
%\]
%
%
%Посчитаем статистику $\chi^{2}$, найдя соответствующие $p_{i}$:
%\[
%p_{0}=\P_{\Geom(0.55)}(0)=0.55,\quad p_{1}\approx0.26,\quad p_{2}\approx0.11,\quad p_{3}\approx0.09.
%\]
% Тогда
%\[
%\chi^{2}=\sum_{i=1}^{k}\frac{\left(\nu_{i}-np_{i}\right)^{2}}{np_{i}}=\frac{25}{55}+\frac{16^{2}}{26}+\frac{81}{11}+\frac{1}{9}\approx17.77.
%\]
% Наконец, поскольку $\chi^{2}\xrightarrow[n\to\infty]{}\chi^{2}(k-r-1)$,
%\[
%\pv=1-\cdf_{\chi^{2}(2)}(17.77)\approx0.00014.
%\]
%\end{solution}


%\section{Согласие с нормальным распределением по $\chi^{2}$}
%
%Для проверки гипотезы $H_{0}:\Pcal_{\xi}=\N(a,\sigma^{2})$ также
%можно воспользоваться статистикой критерия $\chi^{2}$ для сложной
%гипотезы. В этом случае, нужно дискретизировать нормальное распределение,
%так, что
%\[
%\Pcal_{0}=\begin{pmatrix}x_{1}^{*} & \dots & x_{k}^{*}\\
%p_{1}(\hat{\thb}) & \dots & p_{k}(\hat{\thb})
%\end{pmatrix},\quad\hat{\thb}=\hat{\thb}_{\MLE}.
%\]
%Тем не менее, нужно иметь в виду две теоретических неточности:
%\begin{enumerate}
%\item Построение $\Pcal_{0}$ происходит случайно, в результате объединения
%элементов выборки после того, как она получена.
%\item Оценка параметров $\hat{\thb}_{\MLE}$ должна быть посчитана для $\Pcal_{0}$,
%а не для исходного (нормального) распределения --- не $\bar{x},s^{2}$.
%Однако на практике на этот момент не обращают внимания.
%\end{enumerate}
%Существует два возможных способа дискретизации:
%\begin{enumerate}
%\item Гистограмма: одинаковые интервалы, но разные вероятности.
%\item Неравные интервалы с равными вероятностями. %
%\begin{comment}
%@xio: это правильный алгоритм? mean?
%\end{comment}
%
%
%\begin{lyxcode}
%N~<-~length(xs)
%
%xs~<-~sort(xs)
%
%probs~<-~pnorm(xs,~mean=mean(xs),~sd=sd(xs))
%
%i~<-~1;~j~<-~i+1
%
%while~(N~{*}~(probs{[}j{]}~-~probs{[}i{]})~<~5)~\{
%
%~~j~<-~j+1
%
%\}
%
%mean(xs{[}i:j{]})~~\#~our~$x_{1}^{*}$;~$n_{1}=j-i+1$
%\end{lyxcode}
%
%Этот способ разбиения предпочтителен, потому что:
%\begin{itemize}
%\item Можно разбить максимально часто --- так, чтобы $np_{i}=5\ \forall i$,
%следовательно и мощность будет максимальна.
%\item Он оказывается точнее первого на практике.
%\item Получается единственное $p$-value.\end{itemize}
%\begin{rem*}
%Следует иметь в виду, что этот способ не годится для непрерывных,
%но плохо дискретизированных данных.
%\end{rem*}
%\end{enumerate}

\chapter{Критерий Колмогорова-Смирнова согласия с видом распределения}


\section{Произвольное абсолютно непрерывное распределение}

$H_{0}:\xi\sim\Pcal=\Pcal_{0}$.
\begin{claim*}
Для проверки гипотезы согласия с видом произвольного \emph{абсолютно
непрерывного }распределения с известными параметрами используется
точный критерий Колмогорова-Смирнова со следующей статистикой:
\[
D_{n}=\sup_{x\in\mathbf{x}}\left|\widehat{\cdf}_{n}(x)-\cdf_{0}(x)\right|,
\]
 где $\cdf_{0}$ --- функция распределения $\Pcal_{0}$ нулевой гипотезы. Распределение $D_{n}$, оно разное для разных $n$, но не зависит от распределения.

%\begin{comment}
Распределение не зависит от $\cdf_{0}$, так как любое распределение можно привести, например, к равномерному, монотонным преобразованием:
для любой $\cdf_{0}$  верно $\cdf_{0}^{-1}(\xi)\sim\U(0,1)$,
как будто, мы проверяем гипотезу $H_0: \cdf_{0}^{-1}(\xi)\sim\U(0,1)$.
%Значение супремума при этом не меняется, значит, не
%зависит от $F_{0}$.
%\end{comment}


Альтернатива только одна: $H_{1}:\xi\not\sim\Pcal_{0}$; $\Ascr_{\text{крит}}=(\qnt_{\text{K-S}}(1-\a),\infty).$\end{claim*}
\begin{rem*}
Критерий является \emph{точным}, не асимптотическим. Значит, им можно
пользоваться и при маленьких объемах выборки (мощность, при этом,
останется низкой все-равно).
\end{rem*}

\begin{rem*}
$\sqrt{n}\sup_{x}\left|\widehat{\cdf}_{n}(x)-\cdf_{0}(x)\right|\tod\Pcal_{\mathrm{K.S.}}$,
где $\Pcal_{\mathrm{K.S.}}$ --- распределение Колмогорова. Это удобно тем, что распределение такой статистики критерия не зависит от $n$. Значит,
при больших объемах выборки для такой статистики критерия можно пользоваться
таблицами распределения Колмогорова.\end{rem*}

%\end{document}
%\begin{xca*}
%Проверить гипотезу, что $\mathbf{x}=\left(0.1,0.2,0.4,0.3,0.1\right)$
%есть выборка из $\U[0,1]$.\end{xca*}
%\begin{solution}
%$D_{n}=0.6$, $\pv\approx0.05$ (по таблицам или компьютером). Таким
%образом, при $\a>0.05$ гипотеза отвергается, при $\a<0.05$ --- нет.
%\begin{lyxcode}
%>~ks.test(c(0.1,0.2,0.4,0.3,0.1),~'punif')
%
%
%
%~~~~~~~~One-sample~Kolmogorov-Smirnov~test
%
%
%
%data:~~c(0.1,~0.2,~0.4,~0.3,~0.1)
%
%D~=~0.6,~p-value~=~0.05465
%\end{lyxcode}
%\end{solution}
%\begin{rem*}
%Критерий Колмогорова-Смирнова консервативный --- значение $p$-value
%завышено. Поэтому если гипотеза отвергается, то наверняка.
%\end{rem*}

%\section{Нормальное распределение}
%
%Пусть $H_{0}:\Pcal_{\xi}\in\left\{ \N({a},\sigma^{2})\right\} $.
%Как известно, критерий Колмогорова-Смирнова используется для непрерывных
%непараметрических распределений. Им можно воспользоваться и для данной
%$H_{0}$, если вместо ${a},\sigma^{2}$ подставить соответствующие
%оценки --- в таком случае критерий будет консервативным. По аналогии
%с $\chi^{2}$ хотелось бы сделать поправку на количество параметров
%--- такая поправка осуществляется путем моделирования распределения
%тестовой статистики. Для $\N({a},\sigma^{2})$ и $\Exp(\lambda)$
%получаем распределение $D_{n}$, не зависящее от параметров (так что
%поправку можно делать вне зависимости от параметров; к примеру, $\N({a},\sigma^{2})$
%можно привести к $\N(0,1)$ непрерывным преобразованием):
%\begin{description}
%\item [{Критерий~Бартлетта}] есть критерий Колмогорова-Смирнова для $H_{0}:\Pcal_{\xi}=\Exp(\lambda)$.%
%\begin{comment}
%@xio: FIXME: что $\Exp$ делает в этом разделе
%\end{comment}
%
%\item [{Критерий~Лиллиефорса\footnote{Lilliefors.}}] для проверки $H_{0}:\Pcal_{\xi}=\N({a},\sigma^{2})$
%считается статистикой $D_{n}$ c $\cdf_{0}(x)=\cdf_{\N(\bar{x},s^{2})}(x)$,
%сходящейся к распределению Лиллиефорса (Колмогорова-Смирнова с учетом
%подстановки оценок).
%\item [{Критерий~Шапиро-Уилка}] $T\approx\rho^{2}$, т.е. ведет себя примерно
%как квадрат коэффициента корреляции на normal probability plot.\end{description}
%\begin{rem*}
%Распределения в случае критерия Лиллиефорса были получены путем
%моделирования.\end{rem*}
%\begin{example*}
%В R:
%\begin{lyxcode}
%>~require('nortest')
%
%>~xx~<-~rt(1000,~df=20)
%
%>~lillie.test(xx)
%
%~~~~~~~~Lilliefors~(Kolmogorov-Smirnov)~normality~test
%
%data:~~xx
%
%D~=~0.023412,~p-value~=~0.2032
%
%>~shapiro.test(xx)
%
%~~~~~~~~Shapiro-Wilk~normality~test
%
%data:~~xx
%
%W~=~0.99669,~p-value~=~0.03409
%
%
%\end{lyxcode}
%\end{example*}
%
%\chapter{Критерий типа $\omega^{2}$}
%\begin{defn*}
%Статистика
%\[
%Q=n\int_{\RR}\left(\cdf_{n}(x)-\cdf_{0}(x)\right)^{2}w(x)\d\cdf_{0}(x),
%\]
% где $w(x)$ --- весовая функция.\end{defn*}
%\begin{rem*}
%Статистика может быть проинтерпретирована как площадь разницы между
%соответствующими функциями распределения.\end{rem*}
%\begin{description}
%\item [{Cramer~von~Mises}] $Q$ с $w\equiv1$.
%\item [{Anderson-Darling}] $Q$ с
%\[
%w(x)=\frac{1}{\cdf_{0}(x)(1-\cdf_{0}(x))}.
%\]
%\end{description}
%\begin{rem*}
%Весовая функция критерия Anderson-Darling присваивает большой вес
%значениям на хвостах распределения, поэтому сам критерий является
%мощным против разницы на хвостах, но и менее мощным при сдвиге.
%\end{rem*}
%
%\begin{rem*}
%Все эти критерии точны.
%\end{rem*}
%
%\begin{rem*}
%Распределение статистики в каждом случае не зависит от $\cdf_{0}$
%и все эти критерии состоятельные против любой альтернативы, поэтому
%не очень мощные.%
%\begin{comment}
%@xio: Какие альтернативы важны? AD --- отличия на хвостах. KS-test,
%$t_{+},t_{-},t=\max(t_{+},t_{-})$. $t_{+}\rightsquigarrow F_{n}(x)>F_{0}(x),\ t_{-}\rightsquigarrow F_{n}(x)<F_{0}(x).$
%Пусть $F_{0}$ отличается от $\cdf_{\xi}$ только сдвигом, т.е. $\xi=\xi_{0}+a$.
%тогда $t_{+}=a,\ t_{-}=0$. <картиночка>
%
%Пусть $\xi=c\xi_{0}$, тогда <картиночка>
%\end{comment}
%
%\end{rem*}
%
%\begin{rem*}
%Из всех тестов для тестирования согласия с нормальным распределением
%наибольшей мощностью при любых объемах выборки почти всегда обладает
%Shapiro-Wilk, см. \url{http://www.de.ufpb.br/~ulisses/disciplinas/normality_tests_comparison.pdf}
%\end{rem*}

\chapter{Визуальное определение согласия с распределением}


\section{P-P plot}
\begin{defn*}
\emph{P-P plot} есть график
\[
\left\{ \left(\cdf_{0}(x_{i})+\frac{1}{2n},\widehat{\cdf}_{n}(x_{i})\right)\right\} _{i=1}^{n}.
\]
\end{defn*}
\begin{example*}
В R:
\begin{lyxcode}
pp.plot~<-~function(xs,~cdf.0=pnorm,~n.knots=1000)~\{

~~knots~<-~seq(min(xs),~max(xs),~length.out=n.knots)

~~plot(cdf.0(knots),~ecdf(xs)(knots))

~~abline(0,~1)

\}
\end{lyxcode}
\end{example*}

\section{Q-Q plot}
\begin{defn*}
\emph{Q-Q plot} есть график
\[
\left\{ \left(x_{i},\cdf_{0}^{-1}\left(\widehat{\cdf}_{n}(x_{i})+\frac{1}{2n}\right)\right)\right\} _{i=1}^{n}.
\]

\end{defn*}

\begin{defn*}
Частный случай Q-Q plot для $\cdf_{0}^{-1}=\cdf_{\N(0,1)}^{-1}$ называется
\emph{normal probability plot}.\end{defn*}
\begin{example*}
В R:
\begin{lyxcode}
qq.plot~<-~function(xs,~qf.0=qnorm,~n.ppoints=1000)~\{

~~qs~<-~ppoints(n.ppoints)

~~plot(qf.0(qs),~unname(quantile(xs,~probs=qs)))

~~abline(mean(xs),~sd(xs))

\}
\end{lyxcode}
\end{example*}
\begin{rem*}
Если $\hat{\Pcal}_{n}\to\Pcal_{\xi}$, то оба графика будут стремиться
к $y=x$. Референсной прямой normal probability plot будет $y=\sqrt{\widehat{\D\xi}}\cdot x+\widehat{\E\xi}$.
\end{rem*}

\begin{rem*}
Больше о различии Q-Q и P-P plots, см. \url{http://v8doc.sas.com/sashtml/qc/chap8/sect9.htm}
\end{rem*}

\begin{rem*}
Различные интерпретации параметров распределения по Q-Q plot можно
посмотреть в интерактивном приложении: \url{https://xiongge.shinyapps.io/QQplots/}
\end{rem*}

\chapter{Гипотеза о равенстве распределений}

$H_{0}:\Pcal_{\xi_{1}}=\Pcal_{\xi_{2}}.$

Возможно рассматривать два случая:
\begin{description}
\item [{Независимые~выборки}] Две группы индивидов, на которых измеряется
один и тот же признак. Формально: пусть $\zeta\in\left\{ 1,2\right\} $
--- номер группы, $\xi$ --- признак. Тогда $\xi_{1}\sim\Pcal_{\xi\mid\zeta=1},\ \xi_{2}\sim\Pcal_{\xi\mid\zeta=2}$
и $\xi_{1}\indep\xi_{2}$. В этом случае выборка имеет вид
\[
\left(\left(x_{1},x_{2},\ldots,x_{n_{1}}\right),\left(y_{1},y_{2},\ldots,y_{n_{2}}\right)\right)
\]
или


\begin{center}
\begin{tabular}{ccccccc}
\toprule
$\xi$ & $x_{1}$ & $\dots$ & $x_{n_{1}}$ & $y_{1}$ & $\dots$ & $y_{n_{2}}$\tabularnewline
\bottomrule
\end{tabular}
\par\end{center}


то есть одному признаку сопоставлено $n_{1}+n_{2}$ индивидов.

\item [{Зависимые~выборки}] Одна группа индивидов, на каждом из которых
измеряются две характеристики (либо же <<до>> и <<после>>). В
этом случае выборка имеет вид
\[
\left(\left(x_{1},y_{1}\right),\ldots,(x_{n},y_{n})\right)
\]
 или


\begin{center}
\begin{tabular}{cccc}
\toprule
$\xi$ & $x_{1}$ & $\dots$ & $x_{n}$\tabularnewline
\midrule
$\eta$ & $y_{1}$ & $\dots$ & $y_{n}$\tabularnewline
\bottomrule
\end{tabular}
\par\end{center}


то есть по строчкам стоят признаки, по столбцам --- индивиды.

Такие тесты называются парными.

\end{description}
\begin{rem*}
Для одной и той же гипотезы могут существовать разные критерии; их
возможно сравнить по мощности, но только если они состоятельны против
одной и той же альтернативы.
\end{rem*}

\begin{rem*}
Непараметрические критерии хороши тем, что основаны на рангах, значит
устойчивы к аутлаерам; плохи тем, что не используют всю информацию
о значении --- только порядок, из-за чего обладают меньшей мощностью.\\
Также непараметрические аналоги параметрических тестов могут проверять несколько другую гипотезу (точнее --- быть мощными против других альтернатив).%
\begin{comment}
Ранги в R:

\# cut(..., ordered=TRUE)

\# m <- c(\textquotedbl{}L\textquotedbl{}, \textquotedbl{}S\textquotedbl{},
\textquotedbl{}XL\textquotedbl{}, \textquotedbl{}XXL\textquotedbl{},
\textquotedbl{}S\textquotedbl{}, \textquotedbl{}M\textquotedbl{},
\textquotedbl{}L\textquotedbl{})

\# m.of <- ordered(factor(m), levels=c(\textquotedbl{}S\textquotedbl{},
\textquotedbl{}M\textquotedbl{}, \textquotedbl{}L\textquotedbl{},
\textquotedbl{}XL\textquotedbl{}, \textquotedbl{}XXL\textquotedbl{}))

a1 <- c(1:4, 4:5, 7, 7, 7, 9, 15, 17)

names(a1) <- rank(a1) \# позиция элемента в отсортированном массиве
(если элемент повторяется, то индекс позиции --- средний)
\end{comment}
\begin{comment}
Ranked data always require nonparametric analyses.
\end{comment}

\end{rem*}


\chapter{Равенство математических ожиданий для независимых выборок}


\section{Двухвыборочный $t$-критерий}

$H_{0}:\E\xi_{1}=\E\xi_{2}$.
\begin{defn*}
И для зависимых, и для независимых выборок используется\emph{ двухвыборочный
$t$-критерий}
\[
t=\frac{\bar{x}-\bar{y}}{\sqrt{\D(\bar{x}-\bar{y})}}\xrightarrow{\sim}\N(0,1).
\]

\end{defn*}
Пусть выборки \emph{независимы}\footnote{Случай зависимой выборки рассматривается в другом параграфе.},
$(x_{1},\ldots,x_{n_{1}}),\left(y_{1},\ldots,y_{n_{2}}\right),\ n=n_{1}+n_{2}$  (на самом деле, нужно говорить про независимость $\xi_1$ и $\xi_2$).
Значит $\D(\bar{x}-\bar{y})=\D\bar{x}+\D\bar{y}$.

\subsection{Двухвыборочный $t$-критерий для независимых выборок с $\sigma_{1}^{2}=\sigma_{2}^{2}$
(pooled $t$-test)}
\begin{prop*}
Если дисперсия известна,
\[
\D(\bar{x}-\bar{y})=\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}=\sigma^{2}\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right),
\]
 откуда
\[
t=\dfrac{\bar{x}-\bar{y}}{\sigma\sqrt{\dfrac{1}{n_{1}}+\dfrac{1}{n_{2}}}}\xrightarrow[n_{1},n_{2}\to\infty]{}\N(0,1).
\]
 Если данные нормальные, то
\[
t\sim\N(0,1).
\]

\end{prop*}

\begin{prop*}
Если дисперсия неизвестна,
\[
t=\dfrac{\bar{x}-\bar{y}}{\tilde{s}_{1,2}\sqrt{\dfrac{1}{n_{1}}+\dfrac{1}{n_{2}}}}\xrightarrow[n_{1},n_{2}\to\infty]{}\N(0,1).
\]
Если данные нормальные, то
\[
t\sim\t(n_{1}+n_{2}-2).
\]
\end{prop*}
\begin{proof}
Оценку дисперсии можно найти по объединенной и центрированной выборке
(т.е. если $H_{0}$ верна, то $\E\xi_{1}=\E\xi_{2}$ и можно думать
как про одну выборку):
\begin{eqnarray*}
s_{1,2}^{2} & = & \frac{\overbrace{\sum_{i=1}^{n_{1}}(x_{i}-\bar{x})^{2}}^{\sim\chi^{2}(n_{1}-1)}+\overbrace{\sum_{i=1}^{n_{2}}(y_{i}-\bar{y})^{2}}^{\sim\chi^{2}(n_{2}-1)}}{n_{1}+n_{2}}=\frac{n_{1}\cdot s_{1}^{2}}{n_{1}+n_{2}}+\frac{n_{2}\cdot s_{2}^{2}}{n_{1}+n_{2}}\\
\tilde{s}_{1,2}^{2} & = & \frac{\sum_{i=1}^{n_{1}}(x_{i}-\bar{x})^{2}+\sum_{i=1}^{n_{2}}(y_{i}-\bar{y})^{2}}{n_{1}+n_{2}-2}=\frac{(n_{1}-1)\tilde{s}_{1}^{2}}{n_{1}+n_{2}-2}+\frac{(n_{2}-1)\tilde{s}_{2}^{2}}{n_{1}+n_{2}-2},
\end{eqnarray*}
где в последнем случае оценка несмещенная и $\E\tilde{s}_{1,2}^{2}=\sigma^{2}$.
\begin{comment}
\[
\E\frac{(n-1)\tilde{s}^{2}}{\sigma^{2}}=n-1\implies\E\tilde{s}^{2}=\sigma^{2}.
\]
\end{comment}
\end{proof}
\begin{rem*}
Этот вариант более точен, чем в случае $\sigma_{1}\neq\sigma_{2}$.
\end{rem*}

\paragraph{Разбиение}
\begin{description}
\item [{$H_{1}:\E\xi_{1}\neq\E\xi_{2}$}] $\Ascr_{\text{крит}}=\RR\setminus\left(\qnt_{\t(n_{1}+n_{2}-2)}(\a/2),\qnt_{\t(n_{1}+n_{2}-2)}(1-\a/2)\right)$
\item [{$H_{1}:\E\xi_{1}>\E\xi_{2}$}] $\Ascr_{\text{крит}}=(\qnt_{\t(n_{1}+n_{2}-2)}(1-\a),\infty)$
\item [{$H_{1}:\E\xi_{1}<\E\xi_{2}$}] $\Ascr_{\text{крит}}=(-\infty,\qnt_{\t(n_{1}+n_{2}-2)}\a)$
\end{description}


\subsection{Двухвыборочный $t$-критерий для независимых выборок с $\sigma_{1}^{2}\protect\neq\sigma_{2}^{2}$
(Welch $t$-test)}
\begin{prop*}
Если дисперсия известна, $\D(\bar{x}-\bar{y})=\D\bar{x}+\D\bar{y}=\sigma_{1}^{2}/n_{1}+\sigma_{2}^{2}/n_{2}$
и
\[
t=\dfrac{\bar{x}-\bar{y}}{\sqrt{\dfrac{\sigma_{1}^{2}}{n_{1}}+\dfrac{\sigma_{2}^{2}}{n_{2}}}}\xrightarrow[n_{1},n_{2}\to\infty]{}\N(0,1).
\]
 Если данные нормальные, то
\[
t\sim\N(0,1).
\]

\end{prop*}

\begin{prop*}
Если дисперсия неизвестна, $\widehat{\D(\bar{x}-\bar{y})}=s_{1}^{2}/n_{1}+s_{2}^{2}/n_{2}$,
откуда
\[
t=\dfrac{\bar{x}-\bar{y}}{\sqrt{\dfrac{s_{1}^{2}}{n_{1}}+\dfrac{s_{2}^{2}}{n_{2}}}}\xrightarrow[n_{1},n_{2}\to\infty]{}\N(0,1).
\]
 \end{prop*}
\begin{rem*}
Точное распределение неизвестно, примерно равно $\t$ с дробным числом
степеней свободы (что вычисляется интерполяцией по соседним степеням).
Всегда ожидается, что если данные нормальны, то распределение известно.
Это противоречие носит название \emph{проблемы Беренса-Фишера}\footnote{Behrens-Fisher problem.}.%
\begin{comment}
@xio: FIXME: википедия определяет ее по-другому
\end{comment}

\end{rem*}

\paragraph{Разбиение}
\begin{description}
\item [{$H_{1}:\E\xi_{1}\neq\E\xi_{2}$}] $\Ascr_{\text{крит}}=\RR\setminus\left(z_{\a/2},z_{1-\a/2}\right)$
\item [{$H_{1}:\E\xi_{1}>\E\xi_{2}$}] $\Ascr_{\text{крит}}=(z_{1-\a},\infty)$
\item [{$H_{1}:\E\xi_{1}<\E\xi_{2}$}] $\Ascr_{\text{крит}}=(-\infty,z_{\a})$
\end{description}


%\subsection{Испытания Бернулли}
%
%Пусть $\xi_{i}\sim\Ber(p_{i})$, $i\in\left\{ 1,2\right\} $. Рассмотрим
%$H_{0}:p_{1}=p_{2}=p$ против $H_{1}:p_{1}\neq p_{2}$. Поскольку
%$\E\xi_{i}=p_{i}$, применим двух-выборочный $t$-критерий. Объедим
%выборки и запишем:
%\[
%\D\left(\bar{x}-\bar{y}\right)=\dfrac{\hat{p}(1-\hat{p})}{n_{1}+n_{2}}\implies t=\frac{\hat{p}_{1}-\hat{p}_{2}}{\sqrt{\frac{\hat{p}(1-\hat{p})}{n_{1}+n_{2}}}}\xrightarrow[n_{1},n_{2}\to\infty]{}\N(0,1),\quad\hat{p}=\frac{n_{1}\hat{p}_{1}+n_{2}\hat{p}_{2}}{n_{1}+n_{2}}.
%\]
%\begin{comment}
%\[
%\sqrt{\frac{n_{1}n_{2}}{n_{1}+n_{2}}}\dfrac{\hat{p}_{1}-\hat{p}_{2}}{\sqrt{p(1-p)}}
%\]
%\end{comment}
%
%
%
%\paragraph{Разбиение}
%
%Аналогично с $\qnt_{\N(0,1)}$.
%\begin{defn*}
%Выборка обладает \emph{сбалансированным дизайном}, если $n_{1}=n_{2}$.
%\end{defn*}
%\begin{comment}
%При построении эксперимента, есть свобода выбора $n_{1},n_{2}$.
%\end{comment}
%Если дизайн сбалансирован, то
%\[
%s^{2}\left(\frac{1}{n_{1}}+\frac{1}{n_{2}}\right)=\frac{s_{1}^{2}}{n_{1}}+\frac{s_{2}^{2}}{n_{2}},
%\]
% т.е. даже если дисперсии разные, результат одинаковый. Это остается
%справедливым даже при $n_{1}\approx n_{2}$.


\section{Непараметрический $t$-критерий}

Можно использовать обычный $t$-критерий, но примененный к рангам.

Пусть, как и прежде, дана выборка $\left(\mathbf{x},\mathbf{y}\right).$
Следующие два критерия --- Wilcoxon и Mann-Whitney --- проверяют гипотезу
$H_{0}:\P(\xi_{1}>\xi_{2})=\P(\xi_{1}<\xi_{2})$ или, альтернативно,
$H_{0}:\Pcal_{\xi_{1}}=\Pcal_{\xi_{2}}$ против $H_{1}:\Pcal_{\xi_{1}}\neq\Pcal_{\xi_{2}}$
(что выборки получены из одной <<генеральной совокупности>>) в случае
абсолютно непрерывных распределений.


\section{Критерии суммы рангов Wilcoxon}

Следует сопоставить каждой выборке соответствующие её элементам ранги
в \emph{объединенной выборке}:
\begin{eqnarray*}
\left(x_{1},\ldots,x_{n_{1}}\right) & \mapsto & \left(R_{1},\ldots,R_{n_{1}}\right)\\
\left(y_{1},\ldots,y_{n_{2}}\right) & \mapsto & \left(T_{1},\ldots,T_{n_{2}}\right).
\end{eqnarray*}
 Ясно, что если в целом элементы одной выборки окажутся больше другой,
то нельзя будет говорить об их однородности. Определим
\[
W_{1}:=\sum_{i=1}^{n_{1}}R_{i},\qquad W_{2}:=\sum_{i=1}^{n_{2}}T_{i}.
\]
В качестве статистики можно было бы использовать либо $W_{1}$, либо
$W_{2}$, однако, ни той, ни другой статистике невозможно априорно
отдать предпочтение. Поэтому используется статистика
\[
W:=\max\left(W_{1},W_{2}\right),
\]
 не имеющая аналитического выражения (но для которого посчитаны соответствующие
таблицы).%
\begin{comment}
Имеет смысл рассмотреть не $\max$, чтобы рассмотреть критическую
область с двух сторон (сумма может быть слишком маленькой или слишком
большой).
\end{comment}


Иногда в качестве статистики берут количество инверсий в объединенной
выборке. %
\begin{comment}
Алексеева--89
\end{comment}



\section{Критерий Mann-Whitney ($U$ test)}

Используется статистика
\[
U:=\max\left(n_{1}n_{2}+\frac{n_{1}(n_{1}+1)}{2}-W_{1},n_{1}n_{2}+\frac{n_{2}(n_{2}+1)}{2}-W_{2}\right).
\]


При верной $H_{0}$, $\P(\xi_{1}<\xi_{2})=1/2$. В этом случае,
\[
\E U=\frac{n_{1}n_{2}}{2},\qquad\D U=\frac{n_{1}n_{2}(n_{1}+n_{2}+1)}{12}.
\]
Асимптотически,
\[
\frac{U-\E U}{\sqrt{\D U}}\xrightarrow[n_{1},n_{2}\to\infty]{}\N(0,1),
\]
но для малых объемов выборки можно посчитать и точные распределения.


\begin{rem*}
Критерий состоятельный против альтернативы
\[
H_{1}:\P(\xi_{1}>\xi_{2})\ne\P(\xi_{1}<\xi_{2}).
\]
Если формы распределений одинаковы, то эта альтернатива обозначает
сдвиг. Для симметричных распределений это условие обозначает равенство
медиан (а для нормального --- математических ожиданий). %
\begin{comment}
Поэтому критерий Манна-Уинтни называют непараметрическим аналогом
t-test.
\end{comment}
{} Поэтому критерий устойчив к аутлаерам, хоть и за счет небольшой ($\approx5\%$)
потери мощности.
\end{rem*}

\begin{rem*}
Критерии Манна-Уитни и Вилкоксона \emph{эквивалентны} --- в том смысле,
что выделяют один и тот же $p$-value.
%Тем не менее, проверяют они разные гипотезы ($\E\xi$ не то же, что $\med\xi$).
\end{rem*}



\section{Критерий серий (runs)}

Следует объединить выборку и в качестве статистики выбрать количество
серий, т.е. подряд идущих элементов из одной выборки. Эта статистика
имеет специально подобранное распределение. %
\begin{comment}
Поправка на дискретность --- рассмотреть серединки в функции распределения.
\end{comment}

\begin{rem*}
Все %
\begin{comment}
@xio: ?
\end{comment}
эти критерии подразумевают отсутствие повторяющихся наблюдений для
избежания появления дробных рангов.
\end{rem*}

\section{Двухвыборочный тест Колмогорова--Смирнова}

Рассматривается $H_{0}:\Pcal_{\xi_{1}}=\Pcal_{\xi_{2}}$ против $H_{1}:\Pcal_{\xi_{1}}\neq\Pcal_{\xi_{2}}$
и оба распределения абсолютно непрерывны. В качестве статистики используется
\[
D=\sup_{x}\left|\widehat{\cdf}_{\xi_{1}}(x)-\widehat{\cdf}_{\xi_{2}}(x)\right|.
\]



\chapter{Равенство математических ожиданий для парных (зависимых) выборок}

Выборка представлена набором пар $\left\{ \left(x_{i},y_{i}\right)\right\} _{i=1}^{n}.$


\section{$t$-критерий}

Пусть $\xi_{1},\xi_{2}$ заданы на одном $\left(\Omega,\F,\P\right)$.
Тогда гипотезу $H_{0}:\E\xi_{1}=\E\xi_{2}$ можно свести к $H_{0}:\E(\xi_{1}-\xi_{2})=\E\eta=0$
использовать не-парный $t$-тест.
\begin{rem*}[Мощность и зависимость]
Сравним статистику для сбалансированного дизайна:
\begin{itemize}
\item Независимая~выборка
\[
t_{\text{indep}}=\dfrac{\bar{x}-\bar{y}}{\sqrt{\dfrac{\sigma_{1}^{2}}{n_{1}}+\dfrac{\sigma_{2}^{2}}{n_{2}}}}=\frac{\sqrt{n}\left(\bar{x}-\bar{y}\right)}{\sqrt{\sigma_{1}^{2}+\sigma_{2}^{2}}}.
\]

\item Зависимая~выборка:
\begin{eqnarray*}
\D(\bar{x}-\bar{y}) & = & \D\bar{x}+\D\bar{y}-2\cov(\bar{x}\bar{y})=\frac{\sigma_{1}^{2}}{n}+\frac{\sigma_{2}^{2}}{n}-2\rho\sqrt{\D\bar{x}}\sqrt{\D\bar{y}}\\
 & = & \frac{\sigma_{1}^{2}}{n}+\frac{\sigma_{2}^{2}}{n}-2\rho\frac{\sigma_{1}}{\sqrt{n}}\frac{\sigma_{2}}{\sqrt{n}}=\frac{1}{n}\left(\sigma_{1}^{2}+\sigma_{2}^{2}-2\rho\sigma_{1}\sigma_{2}\right),
\end{eqnarray*}
 откуда
\[
t_{\text{dep}}=\frac{\sqrt{n}\left(\bar{x}-\bar{y}\right)}{\sqrt{\sigma_{1}^{2}+\sigma_{2}^{2}-2\sigma_{1}\sigma_{2}\rho}}.
\]

\end{itemize}

При $\rho>0$, $t_{\text{dep}}>t_{\text{indep}}$. Значит, статистика
чаще попадает в критическую область и критерий лучше находит различия
(и мощность, следовательно, выше). Значит, тот же эксперимент на зависимых
выборках мощнее.

\end{rem*}
\begin{example*}
Проверяют гипотезу, что белый свет лам влияет на решение задач.
\begin{itemize}
\item При тестировании на разных индивидах, должна быть уверенность, что
они одинаковы по критичным параметрам (IQ, например).
\item При тестировании на одинаковых индивидах следует составлять разные,
но одинаковы по сложности задачи (второй раз одну и ту же задачу решать
не займет много времени!). Мощность этого эксперимента будет выше.
\end{itemize}
\end{example*}

\section{Непараметрический тест знаков (Sign test)}

$H_{0}:\P(\xi_{1}<\xi_{2})=\P(\xi_{1}>\xi_{2})$. Используется статистика
\[
W=\sum_{i=1}^{n}\psi_{i},\quad\psi_{i}=\begin{cases}
1 & x_{i}>y_{i}\\
0 & x_{i}<y_{i}.
\end{cases}
\]
 Если при подсчете статистики $x_{i}=y_{i}$, эта пара игнорируется
вместе с соответствующим уменьшением объема выборки.

Пусть после удаления всех пар, таких, что $x_{i}=y_{i}$, объем выборки
стал равен $m$. Тогда
\[
W\sim\Bin(m,0.5)
\]
 и для построения разбиения можно пользоваться $\qnt_{\Bin(m,0.5)}$.
\begin{rem*}
Критерий применим к порядковым признакам.
\end{rem*}

\begin{rem*}
Критерий очень устойчив к аутлаерам (но и очень низкомощен поэтому).
\end{rem*}

\section{Непараметрический критерий (Paired Wilcoxon; Wilcoxon signed-rank
test)}

Увеличить мощность предыдущего критерия можно, учтя больше информации:
\[
W:=\sum_{i=1}^{n}R_{i}\psi_{i},\quad R_{i}:=\rk\left|x_{i}-y_{i}\right|.
\]
Для симметрии можно рассмотреть статистику
\[
W=\sum_{i=1}^{n}R_{i}\sign(x_{i}-y_{i})
\]
 с идеальным значением 0. При верной $H_{0}$, распределение $W$
не имеет простого аналитического выражения (но может быть посчитана
по таблицам), при этом $\E W=0$, $\D W=n(n+1)(2n+1)/6$. Кроме того,
$W\tod\N(0,\D W)$, так что уже при $n\geq10$ можно полагать, что
$z=W/\sqrt{\D W}\tod\N(0,1)$ и строить разбиение соответственно.
\begin{rem*}
Критерий уже не применим к порядковым признакам.
\end{rem*}

\chapter{Равенство дисперсии для двух распределений}

$H_{0}:\D\xi_{1}=\D\xi_{2}$, $\xi_{1}\indep\xi_{2}$, $\xi_{i}\sim\N(\mu,\sigma_{i})^{2}.$


\section{Критерий Фишера}

$H_{0}:\sigma_{1}^{2}=\sigma_{2}^{2}$. Естественно использовать отношение
$s_{1}^{2}/s_{2}^{2}$ с идеальным значением 1. Поделив на число степеней
свободы, получим статистику
\[
F:=\frac{\tilde{s}_{1}^{2}}{\tilde{s}_{2}^{2}}\sim\mathrm{F}(\left|\mathbf{x}\right|-1,\left|\mathbf{y}\right|-1).
\]

\begin{rem*}
При отклонении от нормальности не становится асимптотическим. %
\begin{comment}
(если $H_{0}$ не верна?) почему?
\end{comment}

\end{rem*}

\section{Критерий Левена (Levene's test)}

Так как $\D\xi_{i}=\E\left(\xi_{i}-\E\xi_{i}\right)^{2}$, то критерий
о равенстве дисперсий можно было бы свести к критерию о равенстве
математических ожиданий; в этом случае применили бы $t$-критерий
(подразумевающий разные дисперсии) к выборкам $\{\left(x_{i}-\bar{x}\right)^{2}\}$
и $\{\left(y_{i}-\bar{y}\right)^{2}\}$. Однако при возведении
в квадрат распределение стало бы несимметричным %
\begin{comment}
Точно?
\end{comment}
и потребовался бы больший объем выборки. Кроме того, значительно бы
усилились аутлаеры.

Вместо этого используют гипотезу $H_{0}:\E\left|\xi_{1}-\E\xi_{1}\right|=\E\left|\xi_{2}-\E\xi_{2}\right|$
вместе с $t$-критерием, подразумевающим равенство дисперсий (для
нормальных данных; иначе с разными).


\section{Критерий Brown--Forsythe}

Критерий Brown--Forsythe --- это $t$-критерий для гипотезы $H_{0}:\E\left|\xi_{1}-\med\xi_{1}\right|=\E\left|\xi_{2}-\med\xi_{2}\right|.$
\begin{rem*}
Устойчив к аутлаерам из-за использования $\med\xi_{i}$.
\end{rem*}


\part{Корреляционный анализ}
\begin{defn*}
\emph{Мера зависимости} --- это функционал $r:\left(\xi,\eta\right)\mapsto x\in\left[-1,1\right]$
со свойствами:
\begin{enumerate}
\item $\left|r\right|\leq1$.
\item $\xi\indep\eta\implies r(\xi,\eta)=0.$
\item Если $\xi$ и $\eta$ <<максимально зависимы>>, то $|r(\xi,\eta)|=1$.
\end{enumerate}
\end{defn*}

\chapter{Вероятностная независимость}


\section{Визуальное определение независимости}
\begin{itemize}
\item Поскольку при $p_{\eta}(y_{0})\neq0$
\[
\xi\indep\eta\iff p_{\xi\mid\eta}(x\mid y_{0})=\frac{p_{\xi,\eta}(x,y_{0})}{p_{\eta}(y_{0})}=p_{\xi}(x),
\]
 то срезы графика совместной плотности при фиксированном $y_{0}$
после нормировки $p_{\eta}(y_{0})$ должны выглядеть одинаково для
всех $y_{0}$. %
\begin{comment}
@xio: FIXME: 3 картиночки
\end{comment}

\item Для выборки независимость можно попытаться определить по \emph{таблицам
сопряженности}: сгруппируем $\left\{ \left(x_{i},y_{i}\right)\right\} _{i=1}^{n}$
и сопоставим каждой уникальной паре абсолютную частоту $\nu_{ij}$:


\begin{center}
\begin{tabular}{cccc}
\noalign{\vskip0.5ex}
 & $y_{1}^{*}$ & $\cdots$ & $y_{s}^{*}$\tabularnewline[0.5ex]
\noalign{\vskip0.5ex}
$x_{1}^{*}$ & $\nu_{11}$ & $\cdots$ & $\nu_{1s}$\tabularnewline[0.5ex]
\noalign{\vskip0.5ex}
$\vdots$ & $\vdots$ & $\ddots$ & $\vdots$\tabularnewline[0.5ex]
\noalign{\vskip0.5ex}
$x_{k}^{*}$ & $\nu_{k1}$ & $\cdots$ & $\nu_{ks}$\tabularnewline[0.5ex]
\end{tabular}
\par\end{center}


Тогда признаки с большей чем случайной вероятностью будут независимы
при пропорциональных строчках / столбцах. Более формально, признаки
независимы, если
\[
\frac{\nu_{ij}}{\sum_{k}\nu_{kj}}=\frac{\nu_{ij}}{\nu_{\cdot j}}=\hat{p}_{i|j}\propto\hat{p}_{i|\ell},
\]
т.е. вероятности условного распределения не зависят от выбора строки.
\begin{example*}
Таблица сопряженности похожей на независимую выборки:

\begin{center}
\begin{tabular}{ccc}
\noalign{\vskip0.5ex}
1 & 3 & 2\tabularnewline[0.5ex]
\noalign{\vskip0.5ex}
2 & 5 & 3\tabularnewline[0.5ex]
\noalign{\vskip0.5ex}
9 & 20 & 11\tabularnewline[0.5ex]
\end{tabular}
\par\end{center}

\end{example*}
\end{itemize}

\section{Критерий независимости $\chi^{2}$}

По определению, для двумерных дискретных распределений, независимость
есть
\[
\xi\indep\eta\iff\underbrace{\P(\xi=i,\eta=j)}_{p_{ij}}=\underbrace{\P(\xi=i)}_{p_{i}}\underbrace{\P(\eta=j)}_{p_{j}}=\underbrace{\sum_{k=1}^{K}\P(\xi=i,\eta=k)}_{p_{i\cdot}}\cdot\underbrace{\sum_{s=1}^{S}\P(\xi=s,\eta=j)}_{p_{\cdot j}}.
\]


Проверим $H_{0}:\xi\indep\eta$.
\begin{claim*}
ОМП оценкой будет $\hat{p}_{i\cdot}=\nu_{i\cdot}/n$ и $\hat{p}_{\cdot j}=\nu_{\cdot j}/n$.
\end{claim*}
Следовательно,
\[
\xi\indep\eta\iff\hat{p}_{ij}=\frac{\nu_{ij}}{n}=\hat{p}_{i\cdot}\hat{p}_{\cdot j}=\frac{\nu_{i\cdot}}{n}\cdot\frac{\nu_{\cdot j}}{n}.
\]
Это равенство удается получить редко; важно определить, не является
ли это нарушение случайным.

Запишем статистику
\[
\chi^{2}=\sum_{i=1}^{K}\sum_{j=1}^{S}\frac{\left(\nu_{ij}-n\hat{p}_{ij}\right)^{2}}{n\hat{p}_{ij}}=\sum_{i=1}^{K}\sum_{j=1}^{S}\frac{\left(\nu_{ij}-\nu_{i\cdot}\nu_{\cdot j}/n\right)^{2}}{\nu_{i\cdot}\nu_{\cdot j}/n}\tod\chi^{2}((k-1)(s-1))
\]
 Количество параметров таково, потому что если $\xi\parallel\eta$,
то всего $ks-1$ параметров ($-1$ потому что $\sum_{ij}p_{ij}=1$);
если $\xi\indep\eta$, то $k+s-2$ ($-2$ потому что $\sum_{i}p_{ij}=1$
и $\sum_{j}p_{ij}=1$). Значит $ks-1-k-s+2=(k-1)(s-1)$.

\begin{comment}
\[
\chi^{2}=n\left(\sum_{i=1}^{r}\sum_{j=1}^{s}\frac{\nu_{ij}^{2}}{\nu_{i\cdot}\nu_{\cdot j}}-1\right).
\]
В случае $k=s=2$, частотах $\nu_{11}=a,\nu_{12}=b,\nu_{21}=c,\nu_{22}=d$
в качестве статистики используется
\[
n\left(\frac{a^{2}}{(a+b)(a+c)}+\dots+\frac{d^{2}}{(b+d)(c+d)}-1\right)=n\frac{(ad-bc)^{2}}{(a+b)(c+d)(a+c)(b+d)}
\]
\end{comment}
\begin{comment}
TODO: Точный критерий Фишера: Алексеева--46
\end{comment}
\begin{comment}
TODO: Критерий проверки равенства частот: Алексеева--48
\end{comment}
\begin{comment}
TODO: Таблицы сопряженности для зависимых выборок: Алексеева--51
\end{comment}

\begin{example*}
Дано $S$ кубиков. Проверить гипотезу, что кубики одинаковы.\end{example*}
\begin{solution}
Сводится к гипотезе о независимости, так как независимость эквивалентна равенству условных распределений.
\begin{comment}
Аналогично, но $k$ одномерных распределенений --- $\chi^{2}$ + добавить
признак --- номер кубика.
\end{comment}
\end{solution}
\begin{rem*}
На маленьких выборках ($n<50$ или $np_{ij}<5$) возникают проблемы со
сходимостью, потому что можно объединять только столбцы / строки и
каждый раз терять сразу $S-1$ ($K-1$) степень свободы. В этих случаях
используют критерием с перестановкой\footnote{\url{https://en.wikipedia.org/wiki/Resampling_(statistics)##Permutation_tests}}
или, в случае таблиц сопряженности $2\times2$, точным критерием Фишера.
\end{rem*}

\begin{rem*}
Критерий верен для количественных, порядковых и качественных признаков,
потому что нигде не участвуют значения из выборки. Однако есть требование дискретности (конечного числа значений).
\end{rem*}

\begin{rem*}
Критерий асимптотический, поэтому $\alpha_{\I}\to\alpha$.
\end{rem*}

\begin{rem*}
Статистика критерия не удовлетворяет 1-му пункту определения меры зависимости
($\chi^{2}\not\in\left[-1,1\right]$). Это обычно исправляют так:
рассматривают \emph{среднеквадратичную сопряженность}
\[
\hat{r}^{2}:=\frac{\chi^{2}}{n}
\]
или коэффициент сопряженности Пирсона
\[
\hat{p}^{2}:=\frac{\chi^{2}}{\chi^{2}+n} = \frac{\hat{r}^{2}}{\hat{r}^{2}+1}.
\]
 (тогда $1$ никогда не достигается).
\end{rem*}

Заметим, что $\hat{r}^{2}:=\frac{\chi^{2}}{n}$ является оценкой следующей меры зависимости (меры сопряженности) для двумерного дискретного распределения, задаваемого набором $p_{ij}$:
\[
r^2=\sum_{i=1}^{K}\sum_{j=1}^{S}\frac{\left(p_{ij}-p_{i\cdot}p_{\cdot j}\right)^{2}}{p_{i\cdot}p_{\cdot j}}
\]

\chapter{Линейная / нелинейная зависимость}

\section{Определение вида зависимости}

Пусть теперь $\xi,\eta$ --- количественные признаки.

Напомним, условное математическое ожидание $\E(\eta\mid\xi)$ является такой функцией от $\xi$, на которой достигается минимум $\min_{\hat\eta \in \{\varphi(\xi)\}}\E(\eta-\hat\eta)^2$.

\begin{defn*}
Определим функцию условного математического ожидания
\[
\phi(x):=\E\left\{ \eta\mid\xi=x\right\} .
\]
Тогда назовем зависимость \emph{линейной}, если $\phi(x)$ --- линейная
функция, \emph{квадратичной} --- если квадратичная и т.д.
\end{defn*}
\begin{minipage}{\textwidth}
  \centering
  \begin{minipage}{0.47\textwidth}
      \begin{figure}[H]
          \includegraphics[width=\textwidth]{fig/covar-hyperb.pdf}
			\caption{Нелинейная зависимость}
      \end{figure}
  \end{minipage}
  \hspace{0.02\textwidth}
  \begin{minipage}{0.47\textwidth}
      \begin{figure}[H]
          \includegraphics[width=\textwidth]{fig/covar-linear.pdf}
          \caption{Линейная зависимость}
      \end{figure}
  \end{minipage}
\end{minipage}


\section{Коэффициент корреляции Пирсона}
\begin{defn*}
Мера \emph{линейной} зависимости между случайным величинами $\xi$
и $\eta$ есть \emph{коэффициент корреляции Пирсона}
\[
\rho=\frac{\cov(\xi,\eta)}{\sqrt{\D\xi}\sqrt{\D\eta}}.
\]
\end{defn*}
\begin{rem*}
Про $\rho$ можно думать как про $\cos$ между векторами в соответствующем
пространстве.
\end{rem*}

\begin{rem*}[Важное]
\begin{eqnarray*}
\xi\indep\eta & \implies & \rho=0\\
\xi,\eta\sim\N(\mu,\sigma^{2}),\ \xi\indep\eta & \iff & \rho=0.
\end{eqnarray*}
\end{rem*}
\begin{prop*}
Для линейно зависимых данных, конечно, $\rho=\sign b$.\end{prop*}
\begin{proof}
Пусть $\eta=a+b\xi$; тогда
\begin{eqnarray*}
\rho(\xi,\eta) & = & \frac{\cov(\xi,a+b\xi)}{\sqrt{\D\xi}\sqrt{\D(a+b\xi)}}=\frac{\E\xi(a+b\xi)-\E\xi\E(a+b\xi)}{\sqrt{\D\xi}\sqrt{\D b\xi}}=\frac{\E\xi a+b\E\xi^{2}-\E\xi\E a-\E\xi b\E\xi}{|b|\sqrt{\D\xi}\sqrt{\D\xi}}=\\
 & = & \frac{a\E\xi+b\E\xi^{2}-a\E\xi-b(\E\xi)^{2}}{|b|\D\xi}=\frac{b(\E\xi^{2}-(\E\xi)^{2})}{|b|\D\xi}=\sign b.
\end{eqnarray*}

\end{proof}

\begin{prop*}
\[
\rho^2(\xi,\eta) = 1 - \frac{\min_{\hat\eta \in \{a+b\xi\}}\E(\eta-\hat\eta)^2}{\D\eta}.
\]
\end{prop*}

%\section{О соотношении $\rho$ и коэффициента линейной регрессии}
%
%По (\ref{claim:linreg-coeffs}), если линейная регрессия уравнением
%$y=kx+b$, то
%\[
%k=\rho\frac{\sigma_{\eta}}{\sigma_{\xi}}.
%\]
%
%
%В общем случае, по виду прямой линейной регрессии ничего нельзя сказать
%о зависимости между случайными величинами. Так, если $\eta=a+b\xi$
%есть линейная функция от $\xi$, то, по предыдущему, $\rho=\sign b$ и
%\[
%k=\sign b\cdot\frac{\sqrt{\D(a+b\xi)}}{\sqrt{\D\xi}}=b
%\]
% и прямая может иметь произвольный, в зависимости от $b$, наклон.
%\begin{rem*}
%В то же время, поскольку для
%\[
%\begin{pmatrix}\xi\\
%\eta
%\end{pmatrix}\sim\N(\boldsymbol{\mu},\Sigma),\quad\Sigma=\begin{pmatrix}\sigma_{\xi}^{2} & \cov(\xi,\eta)\\
%\cov(\xi,\eta) & \sigma_{\eta}^{2}
%\end{pmatrix}
%\]
%справедливо, что
%\[
%k=\rho\frac{\sigma_{\eta}}{\sigma_{\xi}}=\frac{\cov(\xi,\eta)}{\sigma_{\xi}\sigma_{\eta}}\cdot\frac{\sigma_{\eta}}{\sigma_{\xi}}=\frac{1}{\sigma_{\xi}^{2}}\cov(\xi,\eta),
%\]
% то $k=0\iff\cov(\xi,\eta)=0,$ а для стандартно нормальных данных
%$k=\rho=\cov(\xi,\eta)$.
%\end{rem*}

\subsection{Оценка коэффициента корреляции}

Оценка коэффициента корреляции строится стандартным методом подстановки в формулу для корреляции двумерного эмпирического распределения, в котором каждая пара значений $(x_i,y_i)^\mathrm{T}$, $i=1\ldots,n$, имеет вероятность $1/n$.

В знаменателе стоят дисперсии, поэтому они в оценке просто заменяются на выборочные дисперсии.

Для оценки ковариации $\cov(\xi,\eta) = \E(\xi-\E\xi) (\eta-\E\eta) = \E\xi\eta-\E\xi\E\eta$ можно использовать два варианта (одной и той же) оценки, поэтому получим:
\[
\hat{\rho}(\xi,\eta)=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})/n}{s_x s_y}=
\frac{\sum_{i=1}^n x_i y_i/n-\bar{x}\bar{y}}{s_x s_y}.
\]

\subsection{Значимость коэффициента корреляции}
\begin{defn*}
Коэффициент корреляции \emph{значим}, если отвергается $H_{0}:\rho=0$.
\end{defn*}

Пусть $\left(\xi,\eta\right)^{\T}\sim\N(\boldsymbol{\mu},\boldsymbol{\Sigma})$. Тогда при $H_{0}:\rho=0$
статистика критерия имеет вид и распределение:%
\begin{comment}
$\hat{\rho}\sim N$?
\end{comment}
\begin{comment}
По теореме Кохрана --- нетривиальный вывод
\end{comment}
{}
\[
T=\frac{\sqrt{n-2}\hat{\rho}_{n}}{\sqrt{1-\hat{\rho}_{n}^{2}}}\sim\t(n-2).
\]
Идеальное значение --- 0, критическая область двухсторонняя.
Если предположения о нормальности $\left(\xi,\eta\right)^{\T}$ нет, а гипотеза не о некоррелированности, а о независимости, то критерий становится асимптотическим (т.е. им все равно можно пользоваться).

Проверим теперь гипотезу $H_{0}:\rho=\rho_{0}$ (чаще проверяют $H_{0}:\rho>\rho_{0}$).
Тогда применяется $z$-преобразование Фишера
\[
z=\frac{1}{2}\ln\frac{1+\rho}{1-\rho},\quad z_{0}=\frac{1}{2}\ln\frac{1+\rho_{0}}{1-\rho_{0}}.
\]
Если $\left(\xi,\eta\right)^{\T}\sim\N(\boldsymbol{\mu},\boldsymbol{\Sigma})$,
\[
T=\sqrt{n-3}(z-z_{0})\tod\N(0,1).
\]

Этот критерий асимптотический даже в нормальной модели.

\section{Метод наименьших квадратов (Ordinary Least Squares)}

Пусть $\eta,\xi\in L^{2}(\F,\P)$ пространству $\F$-измеримых по
мере $\P$ функций с нулевым мат.ожиданием, конечным вторым моментом и скалярным произведением
$\left(\eta,\xi\right)=\E\eta\xi$.
По свойству УМО, случайная величина
\[
\hat{\eta}^{*}=\E\left( \eta\mid \xi\right)
\]
 будет ортогональной проекцией $\eta$ на $K$, где $K=\left\{ \phi(\xi), \phi \ \text{измерима} \right\}$, т.е. $\left(\eta-\hat{\eta}^{*},\hat{\eta}\right)=0\ \forall\hat{\eta}\in K$.
Значит, она минимизирует квадрат нормы расстояния от $\eta$ до $K$:
\[
\hat{\eta}^{*}=\argmin_{\hat{\eta}\in K}\left\Vert \eta-\hat{\eta}\right\Vert ^{2}=\argmin_{\hat{\eta}\in K}\E\left(\eta-\hat{\eta}\right)^{2}=\E\left( \eta\mid \xi\right) .
\]
 $\hat{\eta}^{*}$ называется \emph{наилучшим среднеквадратичным приближением
в классе} $K$.


\section{Происхождение и сравнение мер зависимости разного типа}
\label{sec:Pythagor}
Если $K$ --- линейное пространство,
то теорема Пифагора принимает вид
\[
\D\eta=\E\left(\eta-\E\eta\right)^{2}=\underbrace{\E\left(\hat{\eta}^{*}-\E\eta\right)^{2}}_{\text{объяснённая доля аппроксимации}}+\underbrace{\E\left(\eta-\hat{\eta}^{*}\right)^{2}}_{\text{ошибка аппроксимации}},
\]
где $\hat{\eta}^{*} = \argmin_{\hat\eta \in K}\E(\eta-\hat\eta)^2$.

 Откуда можно записать меру аппроксимации линейным пространством $K$ как
\[
\frac{\E(\hat{\eta}^{*}-\E\eta)^{2}}{\D\eta}=1-\frac{\E\left(\eta-\hat{\eta}^{*}\right)^{2}}{\D\eta}=1-\frac{\min_{\hat{\eta}\in K}\E\left(\eta-\hat{\eta}\right)^{2}}{\D\eta}.
\]

Если $K = \mathcal{L} = \{a\xi+b\}$, то полученная величина является квадратом коэффициентом корреляции $\rho^{2}$:
\[
\rho^{2}:=1-\frac{\min_{\hat{\eta}\in\mathcal{L}}\E\left(\eta-\hat{\eta}\right)^{2}}{\D\eta}.
\]
 $\rho$ --- коэффициент корреляции Пирсона.

 (Это будет доказано позже, в теме про парную регрессию.)

\begin{defn*}
В общем случае, если $K=\left\{ \phi(\xi)\mbox{ измеримые}\right\} $,
то полученная величина называется \emph{корреляционным отношением}:

\[
r_{\eta\mid\xi}^{2}:=1-\frac{\min_{\hat{\eta}\in K}\E\left(\eta-\hat{\eta}\right)^{2}}{\D\eta}=\frac{\D\E(\eta\mid\xi)}{\D\eta}.
\]
\end{defn*}

Сравнивая полученные формулы, мы получаем, что коэффициент корреляции измеряет, насколько хорошо случайную величину $\eta$ можно приблизить линейной функцией от $\xi$, а корреляционное отношение --- произвольной (измеримой) функцией от $\xi$ (в смысле метода наименьших квадратов).

%\end{document}

\subsection{Свойства корреляционного отношения}
\begin{enumerate}
\item $r_{\eta\mid\xi}^{2}\in\left[0,1\right]$.
\item $\eta\indep\xi\implies r_{\eta\mid\xi}^{2}=0$.
\item $\eta=\phi(\xi)\iff r_{\eta\mid\xi}^{2}=1.$
\item Вообще говоря, $r_{\eta\mid\xi}^{2}\neq r_{\xi\mid\eta}^{2}$ . К
примеру, для любой не монотонной функции (так, чтобы не существовала
обратная).
\item $r_{\eta\mid\xi}^{2}\geq\rho^{2}(\eta,\xi)$ (потому что минимум по
всем функциям меньше, чем лишь по линейным, значит $1-\min$ больше).
\item $\left(\xi,\eta\right)^{\T}\sim\N(\boldsymbol{\mu},\boldsymbol{\Sigma})\implies r_{\eta\mid\xi}^{2}=\rho^{2}(\eta,\xi)$.
\end{enumerate}

\subsection{Выборочное корреляционное отношение}
\label{sec:cor_ratio}
По разложению дисперсии,

\[
\D\eta=\E(\eta-\E\eta)^{2}=\underbrace{\E(\E(\eta\mid\xi)-\E\eta)^{2}}_{\D\E(\eta\mid\xi)}+\E(\eta-\E(\eta\mid\xi))^{2}.
\]
 Перейдем на выборочный язык. Пусть дана выборка
\[
\begin{pmatrix}x_{1}\\
y_{1}
\end{pmatrix},\ldots,\begin{pmatrix}x_{n}\\
y_{n}
\end{pmatrix}.
\]

Пусть $\xi$ --- дискретная случайная величина со значениями $\left(x_{1}^{*},\ldots,x_{k}^{*}\right)$.
Переобозначим элементы выборки:

\begin{center}
\begin{tabular}{c|ccc}
$x_{1}^{*}$ & $y_{11}$ & $\dots$ & $y_{1n_{1}}$\tabularnewline
$\vdots$ & $\vdots$ & $\ddots$ & $\vdots$\tabularnewline
$x_{k}^{*}$ & $y_{k1}$ & $\dots$ & $y_{kn_{k}}$\tabularnewline
\end{tabular}
\par\end{center}


Тогда, учитывая
\[
\bar{y}_{i}=\bar{y}_{i\cdot}=\frac{1}{n_{i}}\sum_{j=1}^{n_{1}}y_{ij}=\hat{\E}(\eta\mid\xi=x_{i}^{*}),
\]
на выборочном языке получаем (домножив на $n$):
\begin{eqnarray*}
\underbrace{\sum_{i=1}^{k}\sum_{j=1}^{n_{i}}(y_{ij}-\bar{y})^{2}}_{\text{total sum of squares}} & = & \underbrace{\sum_{i=1}^{k}n_{i}\left(\bar{y}_{i}-\bar{y}\right)^{2}}_{\text{межгрупповой разброс}}+\underbrace{\sum_{i=1}^{k}\sum_{j=1}^{n_{i}}(y_{ij}-\bar{y}_{i})^{2}}_{\text{внутригрупповой разброс}}\\
ns_{y}^{2} & = & ns_{y\mid x}^{2}+\sum_{i=1}^{k}\sum_{j=1}^{n_{i}}(y_{ij}-\bar{y}_{i})^{2}.
\end{eqnarray*}
 Отсюда, так как, $r_{\eta\mid\xi}^{2}=\D\E(\eta\mid\xi)/\D\eta$,

\[
\hat{r}_{\eta\vert\xi}^{2}=\hat{r}_{y\vert x}^{2}=\frac{s_{y\vert x}^{2}}{s_{y}^{2}}.
\]

\section{Множественный коэффициент корреляции}

Введенные выше меры обобщаются естественным образом и на случай зависимости не от одной случайной величины, а сразу от нескольких.

\begin{defn*}
\emph{Множественный коэффициент корреляции} определяется на основе метода МНК с $K=\left\{ \sum_{i=1}^{k}b_{i}\xi_{i}+b_{0}\right\} $.
\[
R^{2}(\eta,\xi_{1},\ldots,\xi_{k}):=1-\frac{\min_{\hat{\eta}\in K}\E\left(\eta-\hat{\eta}\right)^{2}}{\D\eta}.
\]
\end{defn*}
\begin{rem*}
$R^{2}\geq\rho^{2}$; если же $R^{2}(\eta,\xi_{1},\ldots,\xi_{k})=\rho^{2}(\eta,\xi_{1})$, то либо через $\xi_{1}$ линейно выражаются все остальные $\xi_{i}$ (но это вырожденный случай), либо $\eta$ и $\xi_i$, $i=2,\ldots,k$, независимы.\end{rem*}


\section{Приложение. Свойства условного математического ожидания\label{sec:cond_exp-props}}
\begin{enumerate}
\item $\E\left\{ a\eta+b\zeta\mid\xi\right\} =a\E\left\{ \eta\mid\xi\right\} +b\E\left\{ \zeta\mid\xi\right\} $.
\item $\E\E\left\{ \eta\mid\xi\right\} =\E\eta$.
\item $\xi\indep\eta\implies\E\left\{ \eta\mid\xi\right\} =\E\eta$.
\item $\eta=f(\xi)\implies\E\left\{ \eta\mid\xi\right\} =\E\left\{ f(\xi)\mid\xi\right\} =f(\xi)$.
\item $\E(\eta f(\xi)\mid \xi)=f(\xi)\E\left\{ \eta\mid\xi\right\} $.
\item $(\xi,\eta)^{\T}\sim\N(\boldsymbol{\mu},\boldsymbol{\Sigma})\implies\E(\eta\mid\xi)=a\xi+b$.

\begin{rem*}[Важное]
Таким образом, если выборка нормальная, то зависимость линейная \emph{всегда}.
\end{rem*}
\item $\argmin_{\hat{\eta}\in K=\left\{ \phi(\xi)\right\} }\E(\eta-\hat{\eta})^{2}=\E\left\{ \eta\mid\xi\right\}.$\end{enumerate}

\chapter{Частная корреляция}
\begin{defn*}
Частная корреляция случайных величин $\eta_{1},\eta_{2}$ за вычетом влияния
$\left\{ \xi_{1},\ldots,\xi_{k}\right\}$ есть
\[
\rho\left(\eta_{1},\eta_{2}\mid\left\{ \xi_{1},\ldots,\xi_{k}\right\} \right):=\rho\left(\eta_{1}-\hat{\eta}_{1}^{*},\eta_{2}-\hat{\eta}_{2}^{*}\right),\quad\text{где }\hat{\eta}_{i}^{*}=\argmin_{\hat{\eta}_{i}\in\left\{ \sum_{i=1}^{k}b_{i}\xi_{i}+b_{0}\right\} }\E\left(\eta_{i}-\hat{\eta}_{i}\right)^{2}.
\]

\end{defn*}
\begin{comment}
@xio: $\E(\eta\mid\xi_{1},\ldots,\xi_{k}):=$ наилучшее функциональное
приближение --- регрессия; наилучшее приближение по МНК --- линейная
регрессия.
\end{comment}


Если регрессия линейна, то
\[
\rho(\eta_{1},\eta_{2}\mid\xi_{1},\ldots,\xi_{k})=\rho(\eta_{1}-\E\left\{ \eta_{1}\mid\xi_{1},\ldots,\xi_{k}\right\} ,\eta_{2}-\E\left\{ \eta_{2}\mid\xi_{1},\ldots,\xi_{k}\right\} ).
\]

\begin{rem*}[Важное]
Пусть в эксперименте подсчитан ненулевой $\rho$. Это может означать,
что один из факторов является причиной, а другой следствием; чтобы
установить, что есть что, проводят эксперимент и смотрят, какой фактор
в реальности влияет на какой. Это может также означать, что влияет
сторонний фактор. Чтобы его исключить, считают частную корреляцию.\end{rem*}
%\begin{example*}
%Возможна ситуация, когда $\rho(\eta_{1},\eta_{2})\neq0$, но $\rho(\eta_{1},\eta_{2}\mid\xi)=0$.

Частная корреляция есть, по сути, корреляция между остатками от линейной регрессии (т.е. вычитается наилучшее линейное приближение по МНК).

\section{Примеры, когда $\xi$ имеет два состояния}
Рассмотрим пример, когда вычитается признак с двумя значениями. В этом случае функцию, задающую наилучшее приближение по МНК, всегда можно считать линейной функцией (так как через две точки всегда можно провести прямую). Поэтому вычитание наилучшего линейного приближения можно заменить на вычитание условного математического ожидания.

\begin{example*}

Это пример, в котором обнаружилась отрицательная корреляция между ростом и длиной волос.

\begin{minipage}{\textwidth}
  \centering
  \begin{minipage}{0.47\textwidth}
      \begin{figure}[H]
          \includegraphics[width=\textwidth]{fig/part_corr-mf.pdf}
			\caption{Исходные данные}
      \end{figure}
  \end{minipage}
  \hspace{0.02\textwidth}
  \begin{minipage}{0.47\textwidth}
      \begin{figure}[H]
          \includegraphics[width=\textwidth]{fig/part_corr_centered-mf.pdf}
          \caption{Данные после вычитания условного матем.ожидания; примерно нулевая корреляция}
      \end{figure}
  \end{minipage}
\end{minipage}
\end{example*}

\begin{example*}
Возможна и ситуация как на (\ref{fig:part_covar-bivar_gauss}), где, скорее всего (а если раздвинуть облака, то 
определенно) $\rho(\eta_{1},\eta_{2})>0$, но $\rho(\eta_{1},\eta_{2}\mid\xi)<0$.

\begin{center}
\begin{figure}[H]
\begin{centering}
\includegraphics[width=0.6\textwidth]{fig/part_corr-bivar_gauss}
\par\end{centering}

\caption{\label{fig:part_covar-bivar_gauss}$\rho(\eta_{1},\eta_{2})>0$, но
$\rho(\eta_{1},\eta_{2}\mid\xi)<0$}
\end{figure}

\par\end{center}

\end{example*}

Заметим, что если корреляции внутри групп разные, то результат будет бессмысленным.

\section{Более двух значений у $\xi$}

Для числа групп, большего, чем 2, все происходит примерно так же, но со след. замечанием. Здесь уже наилучшее функциональное приближение, вообще говоря, нелинейное. Поэтому вычитание условных мат.ожиданий может соответствовать частному корреляционному отношению, но не частной корреляции.

Если же центры групп лежат примерно на одной прямой, то подход через вычитание условного мат.ожидания (центров групп) по-прежнему работает. Заметим, что группы могут пересекаться, сливаться и пр., т.е. не обязаны быть раздельными, как в предыдущем примере.

Однако, наиболее общий случай, когда $\xi$ --- количественный признак и тогда никаких групп, вообще говоря, нет. В этом случае можно мысленно разбить значения $\xi$ на градации и все равно рассуждать в терминах групп.

\paragraph{Интерпретация}

Интерпретируем частную корреляцию мы так: какая была бы зависимость, если бы рассмотреть только объекты с зафиксированным значением признака $\xi$. Результат интерпретации будет осмысленным, если это действие (фиксирование значения $\xi$ и одновременное изменение $\eta_i$) осмысленно.

%\begin{rem*}
%По аналогии с предыдущим примером, если $\left|\im\xi\right|\to\infty$,
%то графики $\left(\eta_{1},\eta_{2}\right)$ при фиксированном $\xi$
%образуют эллипсоид (в этом случае с положительной корреляцией).
%\end{rem*}

%\begin{comment}

\section{Пример анализа данных CARDATA}

Давайте ответим на вопрос, почему цена не зависит от других характеристик
\begin{enumerate}
\item Смотрим на график pairs. 
\item Видим много распределений с хвостом вправо. Логарифмируем данные.

\begin{enumerate}
\item Длинные хвосты (вправо) укорачиваются
\item Можно больше доверять визуальному восприятию выбросов
\item Если признак lognormal, то log(lognormal) = normal и нелинейные зависимости
становятся линейными.
\item Ряд критериев становятся точными (те, что строятся в нормальной модели)
\end{enumerate}
\item Убираем аутлаеров в логданных. (Мерседесы?)
\item Ищем неоднородности в данных. По качественному признаку (origin) нельзя
считать частные корреляции, поэтому строим скаттерплоты, раскрашенные согласно значению origin. 
Видим,
что американские данные отличаются от европейских + японских. Можем
построить correlation matrix отдельно для каждого значения origin. Там видно, что для японских машин корреляции значительно увеличились. Однако, для американских этого не произошло.
\item Смотрим раскрашенные скаттерплоты по году (например, цена против мощности) и, эврика, видим, что год размывает
отчетливую тенденцию. Все облако горизонтальное, с нулевой корреляцией, но состоит из сдвинутых в противоположном направлении облаков с положительной корреляцией. 
\item Это означает в точности то, что частная корреляция за вычетом влияния года будет положительной, как и должно быть по смыслу. Напомним, что интерпретация частной корреляции как раз такая и есть --- какая будет корреляция внутри выборки, если фиксируем значение того признака(ов), влияние которых вычитаем.
\item Можно попробовать  раскрасить скаттерплот одновременно по году и origin, но там мало индивидов будет в группах, не знаю, что там останется.
\end{enumerate}
%\end{comment}


\begin{comment}
Преобразования данных:
\begin{itemize}
\item $\log(x+1)$
\item $1/(x+1)$ стабилизирует дисперсию
\item $\log\left(\frac{x}{1-x}\right)$ (logit) или $\arcsin\sqrt{x}$:
выпрямляет сигмоиды\end{itemize}
\end{comment}


\begin{comment}
Нормализация данных: scale(c(1, 3, 5))
\end{comment}



\chapter{Зависимость между порядковыми признаками}

Пусть признаки порядковые, т.е. их значения можно только сравнивать. Это означает, что нельзя читать, например, математическое ожидание, плотность и пр., но понятие функции распределения, основанное на сравнении, корректно.
%\[
%\begin{pmatrix}\xi\\
%\eta
%\end{pmatrix}\sim\begin{pmatrix}x_{1}\\
%y_{1}
%\end{pmatrix},\ldots,\begin{pmatrix}x_{n}\\
%y_{n}
%\end{pmatrix}
%\]
%о порядок. Тогда можем считать только эмпирическую функцию
%распределения.

Оценки значений функции распределения строятся по выборке на рангах случайных величин. Поэтому в случае порядковых признаков рассматриваются оценки, основанные на рангах. В частности, ранговые коэффициенты корреляции. Заметим, что ранговые характеристики
хорошо работают на выборках без совпадающих наблюдений.


\section{Ранговый коэффициент Спирмана}
\begin{defn*}
\emph{Ранговый коэффициент Спирмана} есть
\[
\rho_{S}=\rho(\cdf_{\xi}(\xi),\cdf_{\eta}(\eta)).
\]
\begin{comment}
@xio: как это понимать?
\end{comment}
\end{defn*}
\begin{rem*}
Для непрерывной функции распределения, $\cdf_{\xi}(\xi)\sim\U(0,1)$, потому что $\P(\cdf_{\xi}(\xi)<x)=\P(\xi<\cdf_{\xi}^{-1}(x))=\cdf_{\xi}(\cdf_{\xi}^{-1}(x))=x$.
\end{rem*}

\begin{defn*}
\emph{Ранг} элемента из выборки есть его порядковый номер в упорядоченной
выборке:
\[
\rk x_{(i)}=i.
\]
\end{defn*}
\begin{notation*}
$\rk x_{(i)}=:R_{i}$, $\rk y_{(i)}=:T_{i}$.
\end{notation*}
Можем ввести эмпирическое распределение
\[
\cdf_{\xi_{n}}(x_{i}+0)=\frac{\rk x_{i}}{n},\quad\cdf_{\eta_{n}}(y_{i}+0)=\frac{\rk y_{i}}{n}=\frac{T_{i}}{n}.
\]
 %
\begin{comment}
\[
\begin{pmatrix}\begin{pmatrix}R_{1}/n\\
T_{1}/n
\end{pmatrix} & \cdots & \begin{pmatrix}R_{n}/n\\
T_{n}/n
\end{pmatrix}\\
1/n & \cdots & 1/n
\end{pmatrix}.
\]
\end{comment}
Тогда будет справедливо следующее
\begin{defn*}
\emph{Выборочный коэффициент Спирмана} определяется как выборочный
коэффициент корреляции Пирсона $\hat{\rho}$, но с заменой значений
на ранги:
\[
\hat{\rho}_{S}
%\begin{pmatrix}\xi_{n}\\
%\eta_{n}
%\end{pmatrix}=\rho\begin{pmatrix}R_{n}\\
%T_{n}
%\end{pmatrix}
=\frac{1/n\cdot\sum_{i=1}^{n}R_{i}T_{i}-\bar{R}\bar{T}}{\sqrt{1/n\cdot\sum_{i=1}^{n}\left(R_{i}-\bar{R}\right)^{2}}\sqrt{1/n\cdot\sum_{i=1}^{n}\left(T_{i}-\bar{T}\right)^{2}}}.
\]


Если нет повторяющихся наблюдений, то знаменатель будет одним и тем
же у всех выборок объема $n$, значит его можно посчитать заранее.
В этом (и только этом) случае, справедлива более простая формула:
\[
\hat{\rho}_{S}=1-\frac{6\sum_{i=1}^{n}(R_{i}-T_{i})^{2}}{n^{3}-n}.
\]
\end{defn*}
\begin{rem*}
Из последней формулы хорошо видно, что если $x_{i},y_{i}$ все идут
в одном порядке, то $R_{i}-T_{i}=0\ \forall i$ и $\hat{\rho}_{S}=1$.
\end{rem*}

\begin{rem*}
$\rho_{S}$ для \emph{количественных} признаков есть мера монотонной
зависимости:
\[
\rho_{S}=1\iff(x_{i}>x_{i+1}\implies y_{i}>y_{i+1}\ \forall i)
\]
(даже если зависимость нелинейная и $\rho\neq1$). Иными словами,
$\rho_{S}>0$, если $y$ имеет тенденцию к возрастанию с возрастанием
$x$ (и $\rho_{S}<0$ иначе). Чем большое $|\rho_{S}|$, тем более явно
выражена зависимость $y$ от $x$ в виде некоторой монотонной функции.
\end{rem*}

\subsection{Согласованость $\rho$ и $\rho_{S}$}

Для количественных признаков, мера монотонной зависимости $\rho_{S}$ не согласована с мерой линейной зависимости $\rho$ в том же смысле, что $\rho$ и мера функциональной зависимости
$r_{\xi\mid\eta}$, где, в частности, $\rho \leq r_{\xi\mid\eta}$, а равенство достигается в случае линейной зависимости (линейного условного математического ожидания). %
\begin{comment}
@xio: FIXME: т.е.?
\end{comment}

\begin{claim*}
Если $(\xi,\eta)^{\T}\sim\N(\boldsymbol{\mu},\Sigma)$, то справедлива формула
\[
\rho=2\sin\left(\frac{\pi}{6}\rho_{S}\right).
\]
\end{claim*}
\begin{itemize}
\item С точностью до погрешности, по значению, ${\rho}$ и ${\rho}_{S}$
--- это одно и то же (см. \ref{fig:rho-rho_S})


\begin{figure}[H]
\begin{centering}
\includegraphics[width=0.6\textwidth]{fig/rho-rho_S}
\par\end{centering}

\captionbelow{\label{fig:rho-rho_S}${\rho}\approx{\rho}_{S}$}
\end{figure}


Значит, в случае нормального распределения можем сравнить оценки $\hat{\rho}$ и $\hat{\rho}_{S}$ между собой.

\item Выборочную дисперсию оценок сравнить довольно сложно.
Тем не менее, можем заметить, что $\hat{\rho}_{S}$ более устойчив
к аутлаерам (см. \ref{fig:rho-outlier}). Всегда можно добавить аутлаер
такой, что $\hat{\rho}=0$; $\hat{\rho}_{S}$ же поменяется не сильно.
Поэтому для нормальных данных, $\rho_{S}$ --- это оценка, что нет
аутлаеров.


\begin{figure}[H]
\begin{centering}
\includegraphics[width=0.6\textwidth]{fig/rho_outlier}
\par\end{centering}

\captionbelow{$\hat{\rho}$ до и после добавления аутлаера\label{fig:rho-outlier}}
\end{figure}


\item Монотонным преобразованием можем всегда сделать так, чтобы $\rho$
изменился (например, возведя в квадрат); при монотонном преобразовании,
однако, не меняется $\rho_{S}$ (см. \ref{fig:rho-monotone}). Значит,
чтобы узнать $\rho$ исходных (нормальных) данных, можно не выполнять
обратного преобразования, а сразу посчитать $\rho_{S}$.


\begin{figure}[H]
\begin{centering}
\includegraphics[width=0.6\textwidth]{fig/rho_monotone}
\par\end{centering}

\captionbelow{Монотонное преобразование нормальных данных\label{fig:rho-monotone}}
\end{figure}


\end{itemize}

\section{Ранговый коэффициент Кэндалла $\tau(\xi,\eta)$}
\begin{defn*}
Пусть $\left(\xi_{1},\eta_{1}\right)^{\T}\indep\left(\xi_{2},\eta_{2}\right)^{\T}\sim\Pcal_{\xi,\eta}\sim\left(\xi,\eta\right)^{\T}$;
тогда \emph{ранговым коэффициентом Кэндалла} называется
\[
\tau(\xi,\eta)=\rho(\sign(\xi_{2}-\xi_{1}),\sign(\eta_{2}-\eta_{1}))=\P((\xi_{2}-\xi_{1})(\eta_{2}-\eta_{1})>0)-\P((\xi_{2}-\xi_{1})(\eta_{2}-\eta_{1})<0).
\]


На выборочном языке, пусть дана выборка $(x_{1},y_{1}),\ldots,(x_{n},y_{n})$;
тогда
\[
\tau=\frac{\#(\text{одинаково упорядоченных пар})-\#(\text{по-разному упорядоченных пар})}{\#(\text{комбинаций пар})},
\]
 где пара $(x_{i},y_{i}),(x_{j},y_{j})$ считается одинаково упорядоченной,
если $\sign(x_{i}-x_{j})=\sign(y_{i}-y_{j})$, а $\#(\text{комбинаций пар})=C_{n}^{2}=n(n-1)/2$.\end{defn*}
\begin{claim*}
Если $(\xi,\eta)^{\T}\sim\N(\boldsymbol{\mu},\Sigma)$, то справедлива
формула
\[
\rho=\sin\left(\frac{\pi}{2}\tau\right).
\]

\end{claim*}
Из утверждения следует, что $\tau$ все время меньше $\rho$ и $\rho_{S}$
(по модулю).
\begin{figure}[H]
\begin{centering}
\includegraphics[width=0.6\textwidth]{fig/rho-tau}
\par\end{centering}

\captionbelow{$\rho$ и $\tau$}
\end{figure}

\begin{example*}[Проверка ряда на тренд]
Пусть $\xi$ --- номера точек, а $\eta$ --- значения ряда. Тогда
$H_{0}:\tau_{0}=0$ и если $H_{0}$ отвергается, то тренд присутствует.
\begin{comment}
@xio: FIXME: прояснить!
\end{comment}
\begin{comment}
Т.е. если тренд есть, то $x_{i}>x_{i+1}\implies y_{i}>y_{i+1}$ с
большой вероятностью.
\end{comment}

\end{example*}
\begin{comment}
Еще одна мера порядковых зависимостей:
\[
\gamma=\frac{\tau}{1-\P(\xi=\eta)}.
\]
\end{comment}



%\chapter{Корреляционные матрицы}
%
%Если признаков много, то их наглядно характеризуют корреляционные
%матрицы. Улучшить наглядность можно переупорядочив признаки так, чтобы
%на диагонали матрицы стояли блоки корреляций признаков из <<корреляционных
%плеяд>>.
%\begin{defn*}
%Пусть задано $\rho_{0}$; корреляционная плеяда есть множество признаков,
%таких, что их попарная корреляция больше $\rho_{0}$.
%\end{defn*}
%Можно выделить и несколько уровней $\rho_{i}:\rho_{0}<\rho_{1}<\dots$.
%Тогда сначала следует составить плеяду по $\rho_{0}$, затем внутри
%полученного по $\rho_{1}$ и т.д.
%

%\end{document}

\part{Дисперсионный анализ}

\chapter{Однофакторный дисперсионный анализ (One-way ANOVA\protect\footnote{ANalysis Of VAriance})}

Задача может быть поставлена двумя эквивалентными образами:
\begin{enumerate}
\item Пусть $\eta_{i}\sim\Pcal_{i},\ i\in1:k$. Проверить гипотезу, что
все распределения равны:
\[
H_{0}:\Pcal_{1}=\dots=\Pcal_{k}.
\]

\item Пусть дан двумерный вектор $\begin{pmatrix}\xi & \eta\end{pmatrix}^{\T}$,
причем $\xi$ (<<фактор>>) принимает $k$ значений $A_{1},\ldots,A_{k}$.
Рассмотрим $\eta_{i}\sim\Pcal_{i}=\Pcal_{\eta\mid\xi=A_{i}}$. Проверить
гипотезу
\[
H_{0}:\Pcal_{\eta\mid\xi=A_{1}}=\dots=\Pcal_{\eta\mid\xi=A_{k}}.
\]

\end{enumerate}
Пусть теперь $\eta_{i}\sim\N(\mu_{i},\sigma^{2})$. Разумеется,
\begin{multline*}
H_{0}:\mu_{1}=\dots=\mu_{k}\iff H_{0}:\E\eta_{1}=\dots=\E\eta_{k}\\
\iff H_{0}:\E(\eta\mid\xi=A_{1})=\dots=\E(\eta\mid\xi=A_{k})\iff H_{0}:\D\E(\eta\mid\xi)=0.
\end{multline*}


Для построения критерия, вспомним разложение дисперсии на выборочном
языке (раздел~\ref{sec:cor_ratio}, $y_{ij}$ --- $j$-й элемент из $i$-й группы):
\[
\underbrace{\sum_{i=1}^{k}\sum_{j=1}^{n_{i}}(y_{ij}-\bar{y})^{2}}_{Q=\widehat{\D\eta}}=
\underbrace{\sum_{i=1}^{k}n_{i}\left(\bar{y}_{i}-\bar{y}\right)^{2}}_{Q_{1}=
\widehat{\D\E(\eta\mid\xi)}}+\underbrace{\sum_{i=1}^{k}\sum_{j=1}^{n_{i}}(y_{ij}-\bar{y}_{i})^{2}}_{Q_{2}},
\]
 откуда в качестве критерия (следуя гипотезе) выберем $Q_{1}$ с идеальным
значением $0$. Однако $Q_{1}$ полезно отнормировать по $Q_{2}$
для учета различных внутригрупповых разбросов. Чтобы получить статистику
с известным распределением, вспомним, что по теореме Cochran, $Q_{1}\indep Q_{2},$
\begin{comment}
Почему?
\end{comment}
\[
\frac{Q}{\sigma^{2}}\sim\chi^{2}(n-1),\quad\frac{Q_{1}}{\sigma^{2}}\sim\chi^{2}(k-1),\quad\frac{Q_{2}}{\sigma^{2}}\sim\chi^{2}(n-k)
\]
{} и
\[
t=\frac{Q_{1}/(k-1)}{Q_{2}/(n-k)}\sim\mathrm{F}((k-1),(n-k)).
\]

\begin{rem*}
Это обобщение статистики для проверки гипотезы о равенстве математических
ожиданий независимых двумерных выборок с равными дисперсиями (с $k=2$,
то есть):
\[
t=\frac{\bar{x}-\bar{y}}{\tilde{s}_{1,2}\sqrt{1/n_{1}+1/n_{2}}}
\]
 с $\tilde{s}_{1,2}^{2}=Q_{2}/(n-2)$. Видим, что статистики критериев
распределены одинаковы --- по определению,
\[
t^{2}(n-2)=\mathrm{F}(1,n-2).
\]

\end{rem*}
Чтобы воспользоваться полученным критерием, нужно как-то убедиться, что
дисперсии одинаковые. Как и в случае $k=2$, это можно проверить по
тесту Левена, только многомерному, т.е. проверить равенство математических
ожиданий $\E\left|\xi-\E\xi_{i}\right|\ \forall i\in1:k$, $\left|y_{ij}-\bar{y}_{i}\right|$
--- опять же, через саму ANOVA.
\begin{rem*}
Если условия нормальности нарушаются, то критерий становится асимптотическим.
Тогда вместо $\mathrm{F}$ следует использовать $\chi^{2}$, так как
$\mathrm{F}(k,m)/m\xrightarrow[m\to\infty]{}\chi^{2}(k)$.\end{rem*}
\begin{example*}
Пусть дана выборка вида $\left\{ \left(\xi=\text{пол},\eta=\text{вес}\right)\right\} $.
Выдвинем $H_{0}:$ вес не зависит от пола. Очевидно, что $\xi$ ---
категориальная случайная величина, а $\eta$ --- количественная. Значения
$\xi$ разобьют всю выборку на две группы. Тогда проверка гипотезы
сведется к проверке равенства распределений в двух группах, $\Pcal_{\eta\mid\xi=g_{1}}=\Pcal_{\eta\mid\xi=g_{2}}.$
В предположении, что $\left(\eta\mid\xi=g_{i}\right)\sim\N(\mu_{i},\sigma^{2})$,
равенство распределений будет следовать из равенства математических
ожиданий.
\end{example*}

\chapter{Множественные сравнения}
\begin{example*}
Проблема множественных сравнений возникает, например, в следующих
ситуациях.
\begin{itemize}
\item Пусть одна группа испытуемых принимает лекарство, а вторая нет. По
завершению эксперимента две группы сравниваются по $m$ показателям.
Однако чем больше показателей сравнивается, тем больше вероятность
того, что \emph{хотя бы по одному} показателю будет совпадение (в
силу случайности).
%\item Испытывают $m=100$ монет на честность сериями по $n=10$ бросков:\\
% $\{(\xi_{1}^{(1)},\ldots,\xi_{10}^{(1)}),\ldots,(\xi_{1}^{(100)},\ldots,\xi_{10}^{(100)})\}$,
%иными словами $\left\{ \eta^{(1)},\ldots,\eta^{(100)}\right\} ,$
%где $\eta^{(i)}\sim\Bin(10,p)$. Проверить $m$ гипотез $H_{0}^{(i)}:\eta^{(i)}\sim\Bin(10,1/2)$,
%$i\in1:m$. Зафиксируем $\a=0.05$. Тогда, учитывая $\pmf_{\Bin(10,1/2)}(k)=C_{10}^{k}2^{-10}$,
%$\P_{H_{0}^{(i)}}(\eta^{(i)}\geq9)=10\cdot2^{-10}+1\cdot2^{-10}\approx0.0107,$
%однако уже $\P_{H_{0}^{(i)}}(\eta^{(i)}\geq8)\approx0.0546$. Так
%что критерием наибольшей мощности будет $\eta^{(i)}\geq9$:
%\[
%\a_{1}=\P_{H_{0}^{(i)}}(H_{0}^{(i)}\text{ отв})=\P_{H_{0}^{(i)}}(\eta^{(i)}\geq9)\approx0.0107\leq0.05.
%\]
% Но использование того же критерия для множественных сравнений сильно
%завышает $\a_{\I}$:%
%\begin{comment}
%FIXME: это делает критерий множественных сравнений радикальным? Дописать
%\end{comment}
%{}
%\begin{eqnarray*}
%\P\left(\bigvee_{i=1}^{100}H_{0}^{(i)}\text{ отв}\right) & = & 1-\P\left(\bigwedge_{i=1}^{100}H_{0}^{(i)}\text{ не отв}\right)=1-\left(1-\P(H_{0}^{(i)}\text{ отв})\right)^{100}\\
% & = & 1-(1-0.0107)^{100}\approx0.6589.
%\end{eqnarray*}
%
\end{itemize}
\end{example*}
Пусть проверяются гипотезы $H_{0}^{(1)},\ldots,H_{0}^{(m)}$. Возможны
такие ситуации:

\begin{center}
\begin{tabular}{ccc}
\toprule
 & Retain $H_{0}$ (отличие от $H_{0}$ не значимо) & Reject $H_{0}$ (отличие от $H_{0}$ значимо)\tabularnewline
\midrule
True $H_{0}$ & \# True Negative & \# False Positive (False Discovery)\tabularnewline
\midrule
False $H_{0}$ & \# False Negative & \# True Positive (True Discovery)\tabularnewline
\bottomrule
\end{tabular}
\par\end{center}

Используя обозначения таблички,
\[
\a_{\I}\approx\frac{\mathrm{FP}}{\mathrm{TN+FP}},\qquad\a_{\II}\approx\frac{\mathrm{FN}}{\mathrm{FN+TP}}.
\]

\begin{defn*}
Family-wise error rate (FWER):
\begin{equation*}
\mathrm{FWER}=\P(\text{хотя бы один раз отвергнута верная гипотеза}).%=\\=\P\left(\bigvee_{i=1}^{m}H_{0}^{(i)}\text{ отв}\right).
\end{equation*}
 Иными словами, $\FWER$ --- это вероятность ошибки первого рода для всей совокупности
экспериментов.
\end{defn*}
Требуется контролировать $\FWER$ на предзаданном уровне $\a$, т.е.
чтобы $\FWER\sim\a,$ где $\sim\in\left\{ =,\leq,\to\right\} $.

Выше мы предполагали, что все гипотезы верны и мы везде ошиблись.
В \emph{сильном} смысле
контроль $\FWER$ на уровне $\a$ должен гарантироваться для \emph{любой} конфигурации
верных и не верных $H_{0}^{(j)}$.
\begin{defn*}
Пусть $I:=\left\{ i:H_0^{(i)}\text{ верна}\right\}$. Тогда
\[
 \FWER_{I}=\P(\text{хотя бы один раз отвергнута верная гипотеза, если верны }H^{(i)},\ i\in I).
\]
\end{defn*}

\begin{defn*}
\[
\strong\,\FWER=\max_{I:I\subset\left\{ 1,\ldots,m\right\} }\FWER_{I}.
\]
\end{defn*}
Это осуществляется двумя процедурами:
\begin{itemize}
\item Single
\item Stepdown
\end{itemize}

\section{Single}

Каждая $H^{(i)}$ проверяется отдельно с уровнем значимости $\a_{1}$.
Задача сводится к тому, чтобы найти такое $\a_{1}$, что $\FWER\leq\a$
для какого-то нужного предзаданного $\a$. Пусть $T=1:m$, т.е. будто
все тесты верны; тогда
\[
\FWER_{\left\{ 1,\ldots,m\right\} }=\P\left(\bigcup_{i=1}^{m}\{H_{0}^{(i)}\text{ отв}\}\right)\leq\sum_{i=1}^{m}\P(H_0^{(i)}\text{ отв})=m\a_{1}=\a\implies\a_{1}:=\frac{\a}{m}.
\]

\begin{rem*}
Из-за неравенства тест консервативный, т.е. $\FWER\le\a$. Значит
не максимально мощный.
\end{rem*}

Вопрос: когда тест максимально консервативный?

\begin{comment}
FixME: почему?
\end{comment}

\begin{claim*}
$\strong\,\FWER  \le \alpha$
\end{claim*}
\begin{proof}
\begin{eqnarray*}
\strong\,\FWER & = & \max_{I\subset\left\{ 1,\ldots,m\right\} }\P(H_0^{(i)}\text{ отвергается},\ i\in I)\\
 & \leq & \sum_{i\in I}\P(H_0^{(i)}\text{ отвергается})=\left|\left\{ i:i\in I\right\} \right|\a_{1} \le \a.
\end{eqnarray*}
\end{proof}

\begin{defn*}
Поправка Бонферрони
\[
\a_{1}=\frac{\a}{m}.
\]

\end{defn*}
Тест нужно проверять не с $\a_{1}$, а с $\a/m$. Так критерий будет
консервативным (иначе --- радикальным, что хуже).
\begin{defn*}
Поправка Бонферрони для $\pv$:
\[
\pv<\frac{\a}{m}\implies\text{отвергаем}\iff mp<\a\implies\text{отвергаем}.
\]

\end{defn*}

\section{Stepdown (Holm's algorithm)}


Алгоритм Хольма/Холма (Holm algorithm, или Holm-Bonferroni algorithm):
\begin{enumerate}
\item Пусть $H_{1},...,H_{m}$ --- семейство нулевых гипотез и $p_{1},...,p_{m}$ --- соответствующие
вероятностные уровни (p-values).
\item Отсортируем их по возрастанию: $p_{(1)}\leq \ldots \leq p_{(m)}$, соответствующие отсортированным уровням
 нулевые гипотезы переобозначим как $H_{(1)} \ldots H_{(m)}$.
\item Для заданного уровня значимости $\alpha$ пусть $k$ --- минимальный индекс такой, что $p_{(k)} > \frac{\alpha}{m+1-k}$.
\item Отвергаем все нулевые гипотезы $H_{(1)} \ldots H_{(k-1)}$ и не отвергаем $H_{(k)} \ldots H_{(m)}$
\item Если $k=1$, то никакая нулевая гипотеза не отвергается, а если нет такого $k$, то все нулевые гипотезы отвергаются.
\end{enumerate}

\begin{claim*}
При множественном тестировании с помощью алгоритма Хольма $\strong\,\FWER \leq\alpha$.
\end{claim*}

\begin{proof}
Обозначим $I_{0}$ множество индексов, соответствующих верным нулевым гипотезам (неизвестно, каким),
$m_0 = |I_0|$ --- число верных нулевых гипотез.

Пусть $h$ --- номер первой (имеем в виду порядок гипотез, введенный в алгоритме Хольма) отвергнутой нулевой гипотезы $H_{(h)}$ среди всех верных гипотез. Так как если одна гипотеза не отверглась, то и следующие гипотезы не отвергаются, то все предыдущие гипотезы  $H_{(1)},\ldots,H_{(h-1)}$ тоже отвергались, но были неверными. Поэтому $m_0$ верных нулевых гипотез должны поместиться между $h$ и $m$; отсюда получаем $m_0 \leq m - h +1$ и поэтому $\displaystyle{\frac{1}{m-h+1}\leq \frac{1}{m_0}}$.

По определению, $FWER_{I_0} = \Pr(\exists i\in I_0: H_{i} \mbox{\ отвергается})$.
Так как события отвержения гипотез являются вложенными, т.е. не может отвергнуться гипотеза с бОльшим номером $(i)$, если не отверглась с меньшим, получаем $FWER_{I_0} = \Pr(H_{(h)} \mbox{\ отверглась})$.  ($\Pr(A) = \Pr(AB)$, если $B\subset A$.)

В силу неравенства для $m_0$, $$FWER_{I_0} = \Pr(p_{(h)} \leq \frac{\alpha}{m-h+1}) \leq \Pr(p_{(h)} \leq \frac{\alpha}{m_0}).$$

Так как для $h$ может быть не более $m_0$ вариантов, $$\Pr(p_{(h)} \leq \frac{\alpha}{m_0}) \leq \Pr(p_{i} \leq \frac{\alpha}{m_0} \mbox{\ for\ } i\in I_0)\leq \sum_{i\in I_0} \Pr(p_{i} \leq \frac{\alpha}{m_0}) = \alpha.$$

Таким образом, $FWER_{I_0}\leq \alpha$ для любого $I_0$. Поэтому $\strong\,\FWER\leq \alpha$.
\end{proof}

%Для увеличения мощности применяется <<Holm's algorithm>>:
%\begin{enumerate}
%\item считаются все $\pv$ $p_{1},\ldots,p_{m}$,
%\item упорядочиваются: $p_{(1)}\leq\ldots\leq p_{(m)}.$
%\item если $mp_{(1)}<\a$ то гипотеза отвергается, иначе и все последующие
%не отвергаются
%\item в общем, если
%\[
%p_{(j)}<\frac{\a}{m-j+1}
%\]
% то гипотеза отвергается, иначе и все последующие не отвергаются.\end{enumerate}
\begin{rem*}
Тест по алгоритму Хольма более мощный, чем с поправкой Боферрони.
\end{rem*}

\begin{rem*}
Процедуру сложно повторить, потому что при упорядочивании гипотезы
могут перемешиваться.\end{rem*}
%\begin{prop*}
%$\FWER\leq\a$.\end{prop*}
%\begin{proof}
%Упорядочим $\pv$: $p_{(1)}\leq\dots\leq p_{(j)}\leq\ldots\leq p_{(m)}.$
%Пусть $I=\left\{ i:H_{0}^{(i)}\text{ верна}\right\} $, $m_{0}=\left|I\right|$
%--- количество верных гипотез, $j=\min_{k\in I}$. $m_{0}$ должно
%<<поместиться до конца>>:
%\[
%j\leq m-m_{0}+1\implies\frac{\a}{m-j+1}\leq\frac{\a}{m_{0}}.
%\]
% Значит %
%\begin{comment}
%FIXME: почему?
%\end{comment}
%{}
%\begin{eqnarray*}
%\FWER_{I} & \leq & \P\left(p_{(j)}<\frac{\a}{m-j+1}\right)\leq\P\left(p_{(j)}<\frac{\a}{m_{0}}\right)\leq\P\left(\min_{i\in I}p_{i}<\frac{\a}{m_{0}}\right)\\
% & = & \P\left(\bigvee_{i\in I}p_{i}<\frac{\a}{m_{0}}\right)\leq\sum_{i\in I}\P\left(p_{i}<\frac{\a}{m_{0}}\right)=m_{0}\frac{\a}{m_{0}}=\a
%\end{eqnarray*}
%
%\end{proof}

\subsection{Частный случай}

Если все гипотезы и критерии независимы, то возможно точно посчитать
$\FWER$:
\begin{eqnarray*}
\FWER_{\left\{ 1,\ldots,m\right\} } & = & \P\left(\bigvee_{i=1}^{m}H_{0}^{(i)}\text{ отв}\right)=1-\P\left(\bigwedge_{i=1}^{m}H_{0}^{(i)}\text{ не отв}\right)\\
 & = & 1-(1-\a_{1})^{m}=\a\implies\a_{1}=1-\sqrt[m]{1-\a}
\end{eqnarray*}

FWER без поправки ($\alpha_1 = \alpha$):\\
\begin{tabular}{|c|c|c|c|c|}
\hline
1&	2&	5&	10&  20\\
\hline
$\alpha_1$&	$\alpha_2$&	$\alpha_5$&	$\alpha_{10}$&	$\alpha_{20}$\\
\hline
0.01&	0.02&	0.05&	0.10&  0.20\\
0.05&	0.10&	0.23&	0.40&  0.64\\
0.10&	0.19&	0.41&	0.65&  0.88\\
\hline
\end{tabular}

\begin{defn*}
\emph{Поправка Šidák'a}:
\[
\a_{1}=1-\sqrt[m]{1-\a}.
\]
\end{defn*}

Вопрос: как выглядит поправка Šidák'а для p-value?

Вопрос: что происходит с тестом, если поправку Šidák'а заменить на поправку Бонферрони?


\chapter{ANOVA Post-Hoc Comparison}

В случае отвержения гипотезы ANOVA, можно провести дополнительное
выборочное тестирование выделенных групп.


\section{Least Significant Difference (LSD)}

LSD test --- это просто попарный $t$-test:
\[
t=\frac{\bar{y}_{i}-\bar{y}_{j}}{\tilde{s}_{1,\ldots,k}\sqrt{1/n_{i}+1/n_{j}}}\sim\t(n-k),
\]
 где $\tilde{s}_{1,\ldots,k}$ --- это pooled по $k$ группам standard
deviation.
\begin{rem*}
Его стоит применять после множественного сравнения лишь к тем группам,
важность которых была зафиксирована экспериментатором до проведения
множественного сравнения.
\end{rem*}

\begin{rem*}
Критерий радикален. Значит, если он не нашел разницу, то и другие
критерии тоже не найдут.
\end{rem*}

\begin{rem*}
Если групп немного, то можно применить поправку Бонферрони.
\end{rem*}

\section{Распределение размаха}

Сопоставим $\xi_{1},\ldots,\xi_{d}$ $\iid$ с $\cdf_{\xi_{i}}(x)=F(x)$
вариационный ряд $\xi_{(1)},\ldots,\xi_{(d)}$.
\begin{defn*}
\emph{Размах} есть случайная величина
\[
w_{d}=\xi_{(d)}-\xi_{(1)}
\]
 с функцией распределения
\[
\P(w_{d}<w)=d\int_{\RR}(F(x+w)-F(x))^{d-1}\d F(x)
\]
 ($w_{d}<w\implies w_{i}<w$, $\P(w_{i}<w)=F(x+w)-F(x)$ --- $d-1$
штук таких, плюс перебор разных минимумов по $1:d$).\end{defn*}
\begin{rem*}
В частном случае $F(x)=\cdf_{\N(0,\sigma^{2})}(x)$, $\Phi(x)=\cdf_{\N(0,1)}(x)$
рассматривается \emph{стандартизированный размах}
\[
\P\left(\frac{w_{d}}{\sigma}<w\right)=d\int_{\RR}(\Phi(x+w)-\Phi(x))^{d-1}\d\Phi(x).
\]
 Если $\sigma$ неизвестна, то с подставленной оценкой $w/\tilde{s}$
называется стьюдентизированным размахом.\end{rem*}
\begin{defn*}
\label{claim:studentized_range}Пусть натуральное число $\ell$ и случайная величина $\eta$ такие, что  $\ell\eta^{2}/\sigma^{2}\sim\chi^{2}(\ell)$;
тогда
\[
\frac{w_{d}}{\eta}\sim q(d,\ell),
\]
т.е. имеет распределение стьюдентизированного размаха с параметрами $d$ и $\ell$. Это распределение
затабулировано.\end{defn*}
\begin{example*}[Проверка выборки на outliers]
В нормальной модели, $H_{0}:\text{нет outliers}.$ Статистика при $d=n$
\[
\frac{x_{(n)}-x_{(1)}}{\tilde{s}}\sim q(n,n-1)
\]
потому что, естественно,
\[
\frac{(n-1)\tilde{s}^{2}}{\sigma^{2}}\sim\chi^{2}(n-1).
\]
\end{example*}
\begin{rem*}
Полученный критерий не очень мощный --- если $H_{0}$ отвергается,
то есть аутлаеры присутствуют, то $x_{(n)}-x_{(1)}$ есть большая
величина, но аналогично большой является и $\tilde{s}$, поэтому всё
значение статистики вырастет незначительно по сравнению со случаем
не-отвержения $H_{0}$, когда аутлаеров нет. Мощность же тем больше,
чем больше (по модулю) значение статистики в случае, когда требуется
отвержение $H_{0}$. Это видно из того, что $\beta=\P_{H_{1}}(T(\mathbf{x})\in\mathscr{A}_{\text{крит}})$;
но мощность, как площадь под графиком плотности $H_{1}$ на критическом
луче (которые располагаются на хвостах плотности $H_{0}$), тем больше,
чем дальше плотность $H_{1}$ от $H_{0}$, т.е. чем больше значения
статистики $T$ в ситуации отвержения $H_{0}$.

Выход заключается в построении более устойчивых оценок для $\sigma^{2}$
--- например, на основе медианы и абсолютного отклонения.
\end{rem*}

\section{Tukey's Honest Significant Difference (HSD) Test}
\begin{assumption*}
Модель нормальная с дисперсией $\sigma_{0}^{2}$, и дизайн сбалансирован:
$\N(\mu_{i},\sigma_{0}^{2}),\ n_{0}=n_{i}\ \forall i\in1:k$.
\end{assumption*}
По определению стьюдентизированного размаха (с $\xi_i = \bar{{y}}_{(i)}$, $l=n-k$),
\[
t=\frac{\bar{{y}}_{(k)}-\bar{{y}}_{(1)}}{\sqrt{\tilde{s}_{1,\ldots,k}^{2}/n_{0}}}\sim q(k,n-k).
\]
 Тогда для проверки $H_{0}:\mu_{i}=\mu_{j}$ используется HSD статистика
\[
t_{ij}=\frac{\left|\bar{{y}}_{i}-\bar{{y}}_{j}\right|}{\tilde{s}_{1,\ldots,k}\sqrt{1/n_{0}}},
\]
 а $\pv$ считаются по $q(k,n-k)$ (таким образом, смотрят на каждую
пару $\left(\bar{{y}}_{i},\bar{{y}}_{j}\right)$ как
на пару из размаха). %
\begin{comment}
$\tilde{s}_{1,\ldots,k}\sqrt{2/n_{0}}=\SE$?
\end{comment}

\begin{rem*}
Если $k=2$, то HSD и LSD --- эквивалентные тесты, но статистики критерия отличаются в $\sqrt{2}$ раз и наличием/отсутствием модуля: в t-test
\[
t=\frac{\bar{{y}}_{1}-\bar{{y}}_{2}}{\tilde{s}_{1,2}\sqrt{2/n_{0}}},
\]
\end{rem*}

\begin{prop*}
$\FWER_{\left\{ 1:m\right\} } = \alpha$.
\end{prop*}
\begin{proof}
Действительно
\begin{eqnarray*}
\FWER_{\left\{ 1:m\right\} } & = & \P\left(\bigvee_{i=1}^{m}H_{0}^{(i)}\text{ отв}\right)=1-\P\left(\bigwedge_{i=1}^{m}H_{0}^{(i)}\text{ не отв}\right)=1-\P\left(t_{ij}<t_{\a}\ \forall i,j\right)\\
 & = & 1-\P\left(\max_{i,j}t_{ij}<t_{\a}\right)=1-\P\left(t_{k1}<t_{\a}\right)=1-\P\left(t_{k1}<F^{-1}(1-\a)\right)=1-(1-\a)=\a.
\end{eqnarray*}

\end{proof}

\section{Другие критерии}
\begin{description}
\item [{Newman-Keals}] stepdown вариант HSD.
\item [{Tukey-Cramer~HSD}] вариант Tukey для несбалансированного дизайна
\item [{Dunnett}] сравнивает все группы с контрольной
\end{description}

\section{Scheffé's Method}

ANOVA гипотезу $H_{0}:\mu_{1}=\dots=\mu_{k}$ можно записать как
\[
H_{0}:\sum_{i=1}^{k}c_{i}\mu_{i}=0,\quad\sum_{i=1}^{k}c_{i}=0,
\]
 где $\mathbf{c}=\left\{ c_{i}\right\} _{i=1}^{k}$ --- \emph{<<контраст>>}.
\begin{example*}
Пусть две группы принимают $k$ лекарств, в том числе --- первым номером
--- плацебо. Сравнить все лекарства с плацебо \emph{одним сравнением}
можно сравнив с ним среднее арифметическое всех лекарств, для чего
положить $c_{1}=1,\ c_{2}=\dots c_{k}=-1/(k-1)$.
\end{example*}
Полученную сумму следует отнормировать и получить статистику
\[
t=\frac{\sum_{i=1}^{k}c_{i}\bar{{y}}_{i}}{\sqrt{\D\left(\sum_{i=1}^{k}c_{i}\bar{{y}}_{i}\right)}}=
\frac{\sum_{i=1}^{k}c_{i}\bar{{y}}_{i}}{\sigma\sqrt{\sum_{i=1}^{k}c_{i}^{2}/n_{i}}}\sim\N(0,1).
\]
 При замене $\sigma$ на $\tilde{s}$, получают, \emph{как обычно},
$t\sim\t(n-k)$.

\begin{comment}
FIXME: дальше мутно
\end{comment}
Пусть $\mathbf{c}^{(1)},\ldots,\mathbf{c}^{(d)},\ d\leq k-1$ --- наборы ортогональных контрастов.
Тогда для любого вектора
\[
t_{j}=\frac{\sum_{i=1}^{k}c_{i}^{(j)}\bar{{y}}_{i}}{\sigma\sqrt{\sum_{i=1}^{k}(c_{i}^{(j)})^{2}/n_{i}}},\quad j\in1:d.
\]
 Линейная комбинация нормальных векторов с ортогональными коэффициентами
независима. Следовательно, можно использовать поправки Šidák.

Сколько бы ни захотелось проверить контрастов, хочется уверенности,
что $\FWER\leq\a$. Статистика
\[
\frac{t^{2}}{k-1}\sim\mathrm{F}(k-1,n-k).
\]

\begin{rem*}
В HSD можно каждую пару рассматривать как конкретный набор контрастов.
Следовательно, метод Шеффе менее мощный по сравнению с HSD (поскольку
проверяет все).
\end{rem*}

\section{Сравнение мощностей}

Статистики всех критериев можно свести к одной с разными критическими
значениями. Для примера, пусть $k=4,n=20,\a=0.05$; тогда

\begin{center}
\begin{tabular}{cc}
\toprule
\emph{Критерий} & \emph{Критическое значение}\tabularnewline
\midrule
LSD & 2.09\tabularnewline
\midrule
Dunnett & 2.54\tabularnewline
\midrule
Bonferroni с 3-мя плановыми сравнениями & 2.63\tabularnewline
\midrule
HSD & 2.8\tabularnewline
\midrule
Bonferroni с $6=C_{4}^{2}$ сравнениями & 2.93\tabularnewline
\midrule
Scheffé & 3.05\tabularnewline
\bottomrule
\end{tabular}
\par\end{center}

Чем больше критическое значение, тем ниже мощность, конечно.


\part{Регрессионный анализ}


\chapter{Регрессия}
\begin{defn*}
\emph{Регрессией} $\eta$ на $\bm\xi=(\bm\xi_1,\ldots,\bm\xi_p)^\T$ называется $\E\left\{ \eta\mid\bm\xi\right\} $.\end{defn*}
\begin{rem*}
Таким образом осуществляется предсказание $\eta$ по $\xi$ с минимальной
среднеквадратичной ошибкой. Детали можно посмотреть в разделе \ref{sec:Pythagor}.
\end{rem*}
\begin{defn*}
\emph{Функция регрессии} есть $f(\x)=\E\left\{ \eta\mid\bm\xi=\x\right\} $.\end{defn*}
\begin{rem*}
$f$ находится по МНК: $f(\bm\xi)=\argmin_{\hat\eta\in K}\E(\eta - \hat\eta)^2$ для $K=\left\{ \psi(\bm\xi):\psi\text{\ ---\ измеримая}\right\} $.
\end{rem*}

Если класс $K$ отличается от $\left\{ \psi(\bm\xi):\psi\text{\ ---\ измеримая}\right\}$, то результат тоже называют регрессией, добавляя, например, линейная регрессия c $K=\left\{ A^\T \bm\xi+ b\right\} $, полиномиальная регрессия и пр.

Если зависимость линейная, то линейная регрессия является действительно регрессией (условным мат.ожиданием), но в общем случае линейная регрессия --- наилучшее линейное приближение по МНК.

\paragraph{Виды регрессий}
\begin{itemize}
\item Парные (предсказывая величину по одной случайной величине) и множественные
(по многим).
\end{itemize}

\paragraph{Неслучайные регрессоры}
Есть другая постановка задачи, когда регрессоры не случайные, т.е. есть неслучайная матрица $\X$, состоящая из $n$ индивидов $\x=(x_1,\ldots,x_p)^\T$, а наблюдается только случайный вектор $\eta$ с компонентами
$\eta_i = \psi(\x_i)+\varepsilon_i$, $i=1,\ldots,n$, где $\E \varepsilon_i=0$. В этой постановке функция $\psi$ называется функцией регрессии, $\E\eta_i = \psi(\x_i)$ (просто математическое ожидание, не условное).
В этом случае ищется функция $\psi$ (или ее параметры), на которой достигается минимум
$\min_{\hat{y}_i = \psi(\x_i), \psi \in K}\E \sum_{i=1}^n (\eta_i - \hat{y}_i)^2$. При такой постановке задачи возможна только параметрическая модель регрессии, когда $\psi$ не просто измеримая функция, а задана числом параметров меньше $n$.

В этом случае линейная модель соответствует линейной функции $\psi$. Стандартные дополнительные предположения --- ошибки $\varepsilon_i$ независимы и одинаково распределены с дисперсией $\sigma^2$. Еще одно необязательное предположение --- это нормальное распределение ошибок; оно нужно только для дополнительных свойств оценок коэффициентов регрессии.


\chapter{Парная линейная регрессия}

Когда используют название <<линейная регрессия>> или <<линия регрессии>>, обычно даже не предполагают, что полученная прямая линия является регрессией (условным математическим ожиданием). Имеется в виду просто приближение линейной функцией по методу наименьших квадратов.

\begin{defn*}
Пусть $\xi,\eta\in L^{2}$ (с конечной дисперсии).\emph{ Парной линейной регрессией $\eta$
 на $\xi$} называется наилучшее среднеквадратичное приближение $h_{b_{1}^{*},b_{0}^{*}}(\xi)=b_{1}^{*}\xi+b_{0}^{*}$
в классе линейных по $\xi$ функций $K=\mathcal{L}=\left\{ b_{1}\xi+b_{0}\right\} $.
Иными словами,
\[
h_{b_{1}^{*},b_{0}^{*}}(\xi)=\argmin_{b_{1},b_{0}}\left\Vert \eta-h_{b_{1},b_{0}}(\xi)\right\Vert ^{2}
 =\argmin_{b_{1},b_{0}}\underbrace{\E(\eta-(b_{1}\xi+b_{0}))^{2}}_{\phi(b_{1},b_{0})}=b_{1}^{*}\xi+\b_{0}^{*}.
\]
\end{defn*}
\begin{rem*}
Найти минимум $\phi$ можно, как обычно, решив систему $\partial\phi/\partial\b_{i}=0$\footnote{См. \url{https://en.wikipedia.org/wiki/Simple_linear_regression}}.\end{rem*}
\begin{claim*}
\label{claim:linreg-coeffs}$b_{1}^{*},b_{0}^{*}$ таковы, что для $y=h_{b_{1}^{*},b_{0}^{*}}(x)$
\[
\frac{y-\E\eta}{\sqrt{\D\eta}}=\rho\frac{x-\E\xi}{\sqrt{\D\xi}}.
\]
 Это уравнение задает линию регрессии. Иными словами,
\[
y=\underbrace{\rho\frac{\sqrt{\D\eta}}{\sqrt{\D\xi}}}_{b_{1}^{*}}x+
\underbrace{\E\eta-\rho\frac{\sqrt{\D\eta}}{\sqrt{\D\xi}}\E\xi}_{b_{0}^{*}}.
\]


Отсюда можно получить соотношение между коэффициентом линейной регрессии
$b_{1}^{*}$ (наклоном регрессионной прямой) и коэффициентом корреляции:
\[
b_{1}^{*}=\rho\frac{\sigma_{\eta}}{\sigma_{\xi}}.
\]
\end{claim*}
\begin{rem*}
Подстановкой проверятся, что
\[
\phi(b_{0}^{*},b_{1}^{*})=\min_{\hat{\eta}\in K}\E\left(\eta-\hat{\eta}\right)^{2}=\D\eta(1-\rho^{2}),
\]
 откуда можно найти уже известное выражение для коэффициента корреляции
Пирсона
\[
\rho^{2}(\eta,\xi)=1-\frac{\phi(b_{1}^{*},b_{0}^{*})}{\D\eta}=1-\frac{\min_{\hat{\eta}\in \mathcal{L}}\E\left(\eta-\hat{\eta}\right)^{2}}{\D\eta},\quad\hat{\eta}:=h(\xi).
\]
\end{rem*}
\begin{defn*}
Линейная регрессия \emph{значима}, если $b_{1}^{*}\neq0\implies\rho\neq0$.
Значимость регрессии эквивалентна значимости (осмысленности) предсказания по ней. В случае отсутствия угла наклона предсказание не имеет смысла, так как равно $\E\eta$ и не зависит от значений $\xi$.
\end{defn*}

В случае, когда наблюдения происходят в фиксированных, а не случайных точках, линия регрессии находится из условия
\[
  \sum_{i=1}^n (\eta_i-(b_{1}x_i+b_{0}))^{2}.
\]
Заметим, что здесь $n$ относится не к объему выборки, а к числу измерений.

\section{Переход на выборочный язык}

Если модель была двумерная $(\xi,\eta)^\T$, то выборка состоит из $n$ реализаций $(x_i,y_i)^\T$ вектора $(\xi,\eta)^\T$.

Если измерения происходят в фиксированных точках $x_i$, то выборка является одной реализацией $(y_1,\ldots,y_n)$ вектора $\eta$.

Методом подстановки получаем формулу для линии регрессии (ее оценки), причем для формулы неважно, фиксированные иксы или случайные:
\[
\frac{y-\bar{x}}{s_y}=\hat{\rho}\frac{x-\bar{x}}{s_x};
\]
по этой формуле несложно выписать оценки коэффициентов линии регрессии.

Обозначим $\hat{y}^i = \hat{b}_1 x_i + \hat{b}_0$. Тогда оценкой дисперсии ошибки методом подстановки будет
$\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}/n = \text{SSE}/n$. Но рассматривают исправленную оценку, чтобы получить несмещенную оценку дисперсии ошибки\\
\[
\hat{\sigma^2} = \sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}/(n-2) = \text{SSE}/(n-2).
\]
Таким образом, у нас есть оценки всех параметров модели, включая мешающий параметр $\sigma^2$.

%Можно описать модель данных как
%\[
%y_{i}=b_{1}x_{i}+b_{0}+\epsilon_{i},\quad\epsilon_{i}\sim\N(0,\sigma^{2}),\ \epsilon_{i}\indep\epsilon_{j}.
%\]
%$\sigma^{2}$ --- мешающий параметр, который можно оценить через $\mathrm{SSR}/n$,
%где
%величина \emph{sum of squares residual} задана формулой
%\[
%\mathrm{SSR}=n\cdot\hat\phi(b_{1}^{*},b_{0}^{*})=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2},\quad\hat{y}_{i}
%=h_{b_{1}^{*},b_{0}^{*}}(x_{i}).
%\]
%
%Но если $\epsilon_{i}\sim\N(0,\sigma^{2})$, то можно показать, что $\mathrm{SSR}/\sigma^2$ имеет распределение $\chi^2$ с $n-2$ степенями свободы, поэтому
%\[
%\hat{\sigma}^{2}=\frac{\mathrm{SSR}}{n-2}
%\]
% есть несмещенная оценка $\sigma^{2}$. %

\begin{rem*}
\begin{comment}
FIXME: картинка с OLS и PCA
\end{comment}
МНК минимизирует разницу $y_{i}-\hat{y}_{i}$, что на графике соответствует
вертикальным отрезкам, соединяющим $y_{i}$ и $\hat{y}_{i}=h(x_{i})$.
Это не то же, что минимизация перпендикуляров от $y_{i}$ на $h(x)$
--- техники метода анализа главных компонент (<<PCA>>).
\end{rem*}

\section{Доверительные интервалы для параметров регрессии}

Рассмотрим модель с фиксированными неслучайными иксами
$\eta_i = b_1 x_i + b_0 +\varepsilon_i$, $i=1,\ldots,n$, где $\E \varepsilon_i=0$, $\D \varepsilon_i = \sigma^2$.

Как обычно, помимо точечной оценки $\hat{b}_{1}$ и $\hat{b}_{0}$,
интересуемся диапазоном значений, которые может принимать оценка с
заданной вероятностью, т.е. доверительными интервалами.

Удобнее заменить параметр ${b}_{0}$ на другой, переписав уравнение регрессии как
$y = b_1 (x_i-\bar{x}) + \tilde{b}_0$, где $\tilde{b}_0 = b_0 + b_1 \bar{x}$. Соответственно поменяются и оценки параметров.
Для них можно получить следующие формулы для дисперсии (это частный случай множественной регрессии, поэтому без доказательства):
\[
\D\hat{b}_{1}=\frac{\sigma^{2}}{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}} = \frac{\sigma^{2}}{n s_x^2} ,\qquad\D\hat{\tilde{b}}_{0}=\frac{\sigma^{2}}{n}.
\]
 Более того, если распределение ошибок нормальное, что оценки $\hat{b}_{1}$ и $\hat{\tilde{b}}_{0}$ являются независимыми.
%Просто доказывается, что оценки несмещенные:  $\E\hat{\b}_{i}=\b_{i}$. Поскольку в модели $y_{i}=\b_{1}x_{i}+\b_{2}+\epsilon_{i}$
%ошибка $\epsilon_{i}\sim\N(0,\sigma^{2})$ есть случайная величина,
%оценки $\hat{\b}_{i}$ --- тоже становятся случайными величинами:
%$\hat{\b}_{i}\sim\N(\b_{i},\D\hat{\b}_{i}).$ В курсе регрессионного
%анализа доказывается\footnote{См. \url{https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares}},
%что

% Кроме того,
%\[
%\SE(\hat{\b}_{1})=\sqrt{\D\hat{\b}_{1}}=\dfrac{\hat{\sigma}}{\sqrt{n}s_{x}}=\dfrac{\sqrt{\frac{\mathrm{SSR}}{n-2}}}{\sqrt{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}},\qquad\SE(\hat{\b}_{2})=\SE(\hat{\b}_{1})\cdot s_{x}
%\]
%
%\begin{prop*}
%Статистика
%\[
%t=\frac{\hat{\b}_{1}-\b_{1}}{\SE(\hat{\b}_{1})}=\dfrac{\hat{\b}_{1}-\b_{1}}{\sqrt{\dfrac{\frac{1}{n-2}\sum_{i=1}^{n}\hat{\epsilon}_{i}^{2}}{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}}}\sim\t(n-2).
%\]
%\end{prop*}
%\begin{proof}
%Известно,
%\[
%t\sim\t(m)\iff t=\frac{\xi}{\sqrt{\eta/m}},\quad\xi\sim\N(0,1),\ \eta\sim\chi^{2}(m).
%\]
% Ясно, что
%\[
%\dfrac{\hat{\b}_{1}-\b_{1}}{\dfrac{\sigma}{\sqrt{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}}}\sim\N(0,1),\qquad\frac{\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}}{\sigma^{2}}\sim\chi^{2}(n-2).
%\]
% Тогда
%\[
%\dfrac{\left(\dfrac{\hat{\b}_{1}-\b_{1}}{\left(\dfrac{\sigma}{\sqrt{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}}\right)}\right)}{\left(\dfrac{\left(\dfrac{\sqrt{\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}}}{\sigma}\right)}{\sqrt{n-2}}\right)}=\dfrac{\dfrac{(\hat{\b}_{1}-\b_{1})\sqrt{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}}{\sigma}}{\dfrac{\sqrt{\sum_{i=1}^{n}\epsilon_{i}^{2}}}{\sigma\sqrt{n-2}}}=\dfrac{\hat{\b}_{1}-\b_{1}}{\sqrt{\dfrac{\frac{1}{n-2}\sum_{i=1}^{n}\hat{\epsilon}_{i}^{2}}{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}}}\sim\t(n-2).
%\]
%
%\end{proof}
%Используя статистику $t$, введем доверительные интервалы с $c_{\gamma}=\cdf_{\t(n-2)}^{-1}\left((1+\gamma)/2\right)$:
%\[
%t\in\left(-c_{\gamma},c_{\gamma}\right)\iff\frac{\hat{\b}_{1}-\b_{1}}{\SE(\hat{\b}_{1})}\in(-c_{\gamma},c_{\gamma})\iff\b_{1}\in\left(\hat{\b}_{1}-c_{\gamma}\SE(\hat{\b}_{1}),\hat{\b}_{1}+c_{\gamma}\SE(\hat{\b}_{1})\right).
%\]
%
%
%Аналогично, для $\b_{2}$:
%\[
%\b_{2}\in\left(\hat{\b}_{2}-c_{\gamma}\SE(\hat{\b}_{2}),\hat{\b}_{2}+c_{\gamma}\SE(\hat{\b}_{2})\right).
%\]

Значимость регрессии определяется с помощью проверки гипотезы $H_0: \rho=0$. Если гипотезы отвергается, то регрессия называется значимой (предсказание имеет смысл в том смысле, что зависит от значения $x$).

\section{Предсказание по линейной регрессии}
На основе дисперсий оценок параметров, можно построить доверительный интервал для среднего предсказания $y = b_1x_i+b_0$ в точке $x$ с уровнем доверия $\gamma$:
\[
  \left((\hat{b}_1 x_i+\hat{b}_0) \pm c_\gamma \sigma \sqrt{1/n + \frac{(x-\bar{x})^2}{n s_x^2}}\right),
\]
где $c_\gamma = \Phi^{-1}((1+\gamma)/2)$ (если ошибки не имеют нормальное распределение или дисперсия ошибки неизвестна и вместо нее стоит ее оценка $\hat\sigma^2$, то доверительный интервал асимптотический).

Из формулы ясно видно, что чем дальше $x$ от среднего $\bar{x}$, тем больше доверительный интервал (менее точное предсказание).

\begin{rem*}
На картинке доверительные интервалы изображаются в виде <<рукавов>>
вокруг графика линейной регрессии --- т.е. область всевозможных положений
прямой при варьировании $b_{1},\tilde{b}_{0}$ в заданных интервалах.\end{rem*}
\begin{example*}

Линейная регрессия как предсказательная модель может быть использована
неправильно в следующих случаях:
\begin{itemize}
\item неправильная модель;
\item применение к неоднородным данным (аутлаер или неоднородность);
\item хотим построить предсказание в точке, далекой от данных (проблема
--- большая ошибка);
\item не знаем какая модель там, где данных нет.
\end{itemize}
\end{example*}

Если мы хотим построить предсказательный интервал для $y = b_1x_i+b_0 + \varepsilon_i$, то получим
\[
  \left((\hat{b}_1 x_i+\hat{b}_0) \pm c_\gamma \sigma \sqrt{1 + 1/n + \frac{(x-\bar{x})^2}{n s_x^2}}\right).
\]

Доверительный интервал строится на основе SE оценки среднего предсказания и показывает, что в среднем может быть при таком $x$. Его размер уменьшается с ростом $n$ (с улучшением точности оценивания параметров регрессии). Предсказательный интервал предсказывает, что вообще может случиться.

\chapter{Множественная линейная регрессия}
Будем рассматривать случай не случайных регрессоров,

\section{Псевдообратные матрицы}
\begin{defn*}
Матрица $\A^{-}$ называется \emph{обобщённо-обратной}, если
\begin{enumerate}
\item По аналогии с $\A^{-1}\A=\Ib\implies\A\A^{-1}\A=\A$ и $\A^{-1}\A\A^{-1}=\A^{-1}$,
выполняется
\[
\A\A^{-}\A=\A,\quad\A^{-}\A\A^{-}=\A^{-}.
\]

\item \emph{(Псевдообратная по Муру-Пенроузу)} если по аналогии с $\A^{-1}=\A^{\T}\implies(\A^{-1}\A)^{\T}=\A^{\T}\left(\A^{-1}\right)^{\T}=\A^{-1}\A$,
дополнительно выполняется
\[
\A^{-}\A=(\A^{-}\A)^{\T},\quad\A\A^{-}=(\A\A^{-})^{\T}.
\]

\end{enumerate}
\end{defn*}

\paragraph{Свойства}
\begin{enumerate}
\item Если столбцы $\A$ линейно-независимы, то существует $(\A^{\T}\A)^{-1}$
и
\[
\A^{-}=(\A^{\T}\A)^{-1}\A^{\T}.
\]

\item Пусть ищут решение $\X\bb=\y$ относительно $\bb$

\begin{enumerate}
\item Если уравнение не имеет решений, то на $\bb=\X^{-}\y$ достигается
минимум невязки между левой и правой частями:
\[
\bb^{*}=\X^{-}\y=\argmin_{\bb}\left\Vert \X\bb-\y\right\Vert ^{2}.
\]

\item Если решение не единственно, то $\bb=\X^{-}\y$ есть решение с минимальной
нормой.
\end{enumerate}
\end{enumerate}

\section{Проекторы на подпространства}

Пусть $\Ell_{d}\subset\RR^{m}$ --- линейное подпространство размерности
$d$ с базисом $\left\{ \p_{1},\ldots,\p_{d}\right\} $, $\Pb=[\p_{1}:\ldots:\p_{d}]$.
Тогда проектор на $\Ell_{d}$ будет задан как
\[
\proj_{\Ell_{d}}=\boldsymbol{\Pi}=\Pb(\Pb^{\T}\Pb)^{-1}\Pb^{\T}=\Pb\Pb^{-}.
\]
 Если $\left\{ \p_{i}\right\} _{i=1}^{d}$ --- ортонормированный базис,
то
\[
\boldsymbol{\Pi}=\Pb\Ib\Pb^{\T}=\Pb\Pb^{\T}.
\]
 Кроме того,
\[
\proj_{\Ell_{d}^{\perp}}=\underset{m\times m}{\Ib}-\Pb\Pb^{\T}.
\]
 (т.е., чтобы получить ортогональное пространство к проекции, нужно
из исходного вектора вычесть проекцию).


\paragraph{Свойства}
\begin{enumerate}
\item $\boldsymbol{\Pi}\boldsymbol{\Pi}=\boldsymbol{\Pi}$
\item $(\Ib-\boldsymbol{\Pi})(\Ib-\boldsymbol{\Pi})=\Ib-\boldsymbol{\Pi}$
\item $\boldsymbol{\Pi}^{\T}=(\Pb\Pb^{\T})^{\T}=\boldsymbol{\Pi}.$
\end{enumerate}

\section{Ordinary and Total Least Squares }

\begin{comment}
На генеральном языке

$\X,\y$ случайные, $\rk\X=m$, $\cov(\y\mid\X)=\sigma^{2}\I_{n}.$
$\E(\y\mid\X)=\X\mathbf{b}.$

По МНК, $\bb=(\X^{\T}\X)^{-1}\X^{\T}\mathbf{y}$.
\begin{itemize}
\item Несмещенность
\begin{eqnarray*}
\E\hat{\bb} & = & \E_{\X}\left\{ \E(\hat{\bb}\mid\X)\right\} =\E_{\X}\left\{ \E((\X^{\T}\X)^{-1}\X^{\T}\mathbf{y}\mid\X)\right\} =\E_{\X}\left\{ (\X^{\T}\X)^{-1}\X^{\T}\E(\mathbf{y}\mid\X)\right\} \\
 & = & \E_{\X}(\X^{\T}\X)^{-1}\X^{\T}\X\mathbf{b}=\E_{\X}\mathbf{b}=\mathbf{b}.
\end{eqnarray*}

\item $\cov\hat{\mathbf{b}}=\E_{\X}\left\{ \E((\hat{\bb}-\bb)(\hat{\bb}-\bb)^{\T}\mid\X)\right\} =\E_{\X}\sigma^{2}(\X^{\T}\X)^{-1}=\sigma^{2}\E(\X^{\T}\X)^{-1}$
\end{itemize}
Схема случайной выборки: $y,x_{1},\ldots,x_{m}$, $\E y=\mu_{0}$,
$\E x_{i}=\mu_{i}$, $\D y=\sigma_{0}^{2}$, $\cov\mathbf{x}=\mathbf{C}$.
$\E(y\mid\mathbf{x})=\mathbf{b}^{\T}\mathbf{x}$. Пусть $\D(y\mid\mathbf{x})=\E_{\x}\left\{ \E(y-\E(y\mid\x))^{2}\mid\x\right\} =\sigma^{2}=\const$
и неизвестно. По независимости $\{y_{t},\x_{t}\}$, $\bb^{\T}\x$
эквивалентно $\X\mathbf{b}.$
\begin{prop*}
ОМНК в схеме случайной выборки асимптотически нормальна.
\end{prop*}

\begin{prop*}
Для выборки из нормального распределения, ОМНК и ОМП совпадают.\end{prop*}
\end{comment}


Пусть
\[
\X=\begin{pmatrix}x_{11} & \dots & x_{1k}\\
\vdots & \ddots & \vdots\\
x_{1n} & \dots & x_{nk}
\end{pmatrix}
\]
матрица данных с $n$ индивидами \footnote{Также <<predictors>>, <<regressors>>, <<controlled variables>>,
<<explanatory variables>>, <<features>>, <<inputs>>.} по столбцам, каждый из которых описывается $k$ признаками;
\[
\y=\begin{pmatrix}y_{1}\\
\vdots\\
y_{n}
\end{pmatrix}
\]
 вектор наблюдений\footnote{Также <<regressands>>, <<response>>, <<explaining variables>>,
<<outcome>>, <<experimental variables>>.};
\[
\mathbf{b}=\begin{pmatrix}b_{1}\\
\vdots\\
b_{k}
\end{pmatrix}
\]
 вектор неизвестных коэффициентов.
\begin{description}
\item [{OLS}] Пусть $\y\in\RR^{n},\ \X\in\RR^{n\times m},\ \bb\in\RR^{m},\ \rk\X=m$.
Пусть допускаются ошибки в наблюдениях такие, что $\E\epsilon_{i}=0,\ \epsilon_{i}\indep\epsilon_{j},\ \D\epsilon_{i}=\sigma^{2}\implies\cov\eb=\sigma^{2}\Ib$.
Тогда в модели
\[
\y=\X\bb+\eb,
\]
найти
\[
\hat{\bb}=\argmin_{\bb}\left\Vert \X\bb-\y\right\Vert ^{2}=\argmin_{\tilde{\y}}\left\Vert \tilde{\y}-\y\right\Vert ^{2}=\X^{-}\y=(\X^{\T}\X)^{-1}\X^{\T}\y,\quad\tilde{\y}:=\X\bb.
\]
 Откуда регрессией будет\footnote{$\H$ --- <<hat matrix>>.}
\[
\hat{\y}=\X\hat{\bb}=\underbrace{\X(\X^{\T}\X)^{-1}\X^{\T}}_{{\displaystyle \proj_{\colspace\X}}}\y=\H\y.
\]
Можно посчитать остатки --- разницу между наблюдениями и предсказанием
по регрессии:
\[
\mathrm{residuals}=\y-\hat{\y}=\y-\H\y=(\Ib-\H)\y=\M\y.
\]

\item [{TLS}] Модель допускает ошибки $\Db$ также и в $\X$,
\[
\y=\left(\X+\Db\right)\bb+\eb
\]
 (где известны $\y,\ $ $\tilde{\X}:=\X+\Db$, а $\X$ --- нет). Найти
\[
\argmin_{\bb:\tilde{\y}=\tilde{\X}\bb}\left(\left\Vert \tilde{\X}-\X\right\Vert _{F}^{2}+\left\Vert \tilde{\y}-\y\right\Vert ^{2}\right),\quad\left\Vert \mathbf{A}\right\Vert _{F}^{2}=\sum_{i.j}a_{ij}^{2}.
\]

\end{description}
Дальше рассматривается OLS.


\section{Свободный член}

Видно, что $\X\bb=\y$ задает СЛАУ, где каждое уравнение --- прямая,
проходящая через 0. Чтобы иметь возможность описывать случаи не-центрированных
данных, пригодны два варинаты:
\begin{enumerate}
\item Ввести фиктивный столбец из единиц:
\[
\X=\begin{pmatrix}1 & x_{11} & \dots & x_{1k}\\
\vdots & \vdots & \ddots & \vdots\\
1 & x_{n1} & \dots & x_{nk}
\end{pmatrix}\in\RR^{n\times m},\quad m=k+1.
\]

\item Центрировать признаки.\end{enumerate}
\begin{prop*}
Оба способы эквиваленты.\end{prop*}
\begin{thm*}[О разбиении регрессоров, the Frisch–Waugh–Lovell theorem]
Пусть $\X$ матрица данных с признаками (<<регрессорами>>) по столбцам,
$\X=[\X_{1}:\X_{2}]$, $\hat{\bb}=(\hat{\bb}_{1},\hat{\bb}_{2})^{\T}$,
$\M_{1}=\Ib-\H_{1}$, $\H_{1}=\proj_{\colspace\X_{1}}$. Тогда $\hat{\bb}_{2}$
можно получить как регрессию $\M_{1}\y$ на $\M_{1}\X_{2}$. Остатки
регрессии $\M_{1}\y$ будут такими же как остатки исходной. \end{thm*}
\begin{proof}
Без доказательства.
\end{proof}
Пусть $\bb\in\RR^{m}$, $\hat{\bb}=\X^{-}\y=\left(\hat{b}_{0},\hat{b}_{1},\ldots,\hat{b}_{k}\right)^\T.$
Центрируем $\X$, вычитая среднее по каждому столбцу: $\X^{(c)}\in\RR^{n\times k}.$
Центрируем $\y$: $\y^{(c)}\in\RR^{n}$; тогда $\hat{\bb}^{(c)}=(\X^{(c)})^{-}\y^{(c)}$
и по теореме
\[
\hat{\bb}^{(c)}=\begin{pmatrix}\hat{b}_{1}^{(c)}\\
\vdots\\
\hat{b}_{k}^{(c)}
\end{pmatrix}=\begin{pmatrix}\hat{b}_{1}\\
\vdots\\
\hat{b}_{k}
\end{pmatrix},\quad\y-\hat{\y}=\y^{(c)}-\hat{\y}^{(c)}.
\]

\begin{cor*}
\begin{comment}
FIXME: Почему?
\end{comment}
\[
\hat{b}_{0}=\bar{\mathbf{y}}-\sum_{i=1}^{k}\hat{b}_{i}\bar{x}_{i}.
\]

\end{cor*}

\section{Стандартизованные признаки}

Если признаки изначально измерены в разных шкалах, то коэффициенты
перед признаками можно интерпретировать как <<важность>>.
\begin{defn*}
Чтобы стандартизировать наблюдения, следует поделить центрированные
столбцы на нормы каждого столбца, получится $\X^{(s)}\in\RR^{n\times k}$;
$\y^{(s)}=\y^{(c)}/\left\Vert \y^{(c)}\right\Vert .$ Тогда
\[
\hat{\bb}^{(s)}=(\X^{(s)})^{-}\y^{(s)}=\left(\left(\X^{(s)}\right)^{\T}\X^{(s)}\right)^{-1}\left(\X^{(s)}\right)^{\T}\y^{(s)}=\hat{\boldsymbol{\beta}},\quad\hat{\b}_{i}=\frac{\left\Vert \mathbf{x}_{i}^{(c)}\right\Vert }{\left\Vert \y^{(c)}\right\Vert }\hat{b}_{i}.
\]
 Вектор $\hat{\boldsymbol{\beta}}$ имеет такой вид, потому что по
ходу вычислений два раза поделили и один раз умножили на $\left\Vert \x_{i}^{(c)}\right\Vert $,
и умножили на $\left\Vert \y^{(c)}\right\Vert $.
\end{defn*}

\section{Свойства оценки $\hat{\protect\bb}$}
\begin{enumerate}
\item \emph{Несмещенность} (по $\E\eb=\mathbf{0}$):
\[
\E\hat{\bb}=\E(\X^{\T}\X)^{-1}\X^{\T}\y=(\X^{\T}\X)^{-1}\X^{\T}\E\y=(\X^{\T}\X)^{-1}\X^{\T}\E\left(\X\bb+\eb\right)=\bb.
\]

\item Ковариационная матрица:
\[
\cov\hat{\bb}=\cov(\X^{\T}\X)^{-1}\X^{\T}\y=(\X^{\T}\X)^{-1}\X^{\T}\cov\eb\,\X(\X^{\T}\X)^{-1}=\sigma^{2}(\X^{\T}\X)^{-1}.
\]

%\item \emph{Состоятельность}: если оценка несмещенная и состоятельная в
%среднеквадратичном смысле, то она несмещенная; однако ситуация
%\[
%\MSE\hat{\bb}=\D\hat{\bb}\xrightarrow[n\to\infty]{}0
%\]
% невозможна в текущей постановке, потому что $\X$ --- фиксированная
%матрица наблюдений.
%
%\begin{prop*}[О состоятельности оценки]
%Пусть $\X_{n}\in\RR^{n\times m}$ --- последовательность (случайных)
%матриц, $\eb_{n}\in\RR^{n}$; кроме того,
%\begin{enumerate}
%\item Выполняется сильная регулярность независимых переменных:
%\[
%\frac{1}{n}\X_{n}^{\T}\X_{n}\toP\A,\quad\A\text{ невырожденная}
%\]
%
%\item Ошибки независимы с регрессорами
%\[
%\frac{1}{n}\X_{n}^{\T}\eb_{n}\toP\mathbf{0}_{m}.
%\]
%
%\end{enumerate}
%Тогда оценка
%\[
%\hat{\bb}_{\OLS,n}\toP\bb
%\]
%является состоятельной.\end{prop*}
%\begin{proof}
%\[
%n\left(\X_{n}^{\T}\X_{n}\right)^{-1}=\A^{-1}\implies\E(\hat{\bb}_{n}-\bb)(\hat{\bb}_{n}-\bb)=\cov\hat{\bb}_{n}=\sigma^{2}(\X_{n}^{\T}\X_{n})^{-1}\toP\mathbf{0}.
%\]
% Значит, оценка состоятельна в среднеквадратичном, значит состоятельна.\end{proof}
%\begin{prop*}[Об асимптотической нормальности оценки]
%Пусть $\left\{ \epsilon_{i}\right\} $ $\iid$,
%\[
%\A_{n}=\frac{1}{\sigma}\left(\X_{n}^{\T}\X_{n}\right)^{-1/2}\X_{n}^{\T}.
%\]
%$\hat{\bb}_{\OLS}$ асимптотически нормальна тогда и только тогда,
%когда
%\[
%\max_{i}\left\{ \A_{ni1}^{2},\ldots,\A_{nin}^{2}\right\} \xrightarrow[n\to\infty]{}0
%\]
%
%\end{prop*}
\end{enumerate}

Заметим, что никакие асимптотические свойства невозможны, так как $\X$ фиксирована.

\begin{comment}
\[
\cov\hat{\bb}^{(c)}=\sigma^{2}(\X^{(c)\T}\X^{(c)})^{-1}=\sigma^{2}n(n\X^{(c)\T}\X^{(c)})^{-1}=\sigma^{2}n\S_{\x\x}\neq\frac{\sigma^{2}}{n}\S_{\x\x}
\]
\end{comment}



\section{Свойства $\hat{\protect\bb}^{(c)}$ и $\hat{\protect\bb}^{(s)}$}
\begin{enumerate}
\item $\left(\X^{(c)}\right)^{\T}\X^{(c)}/n=\S_{\mathbf{x}\mathbf{x}}$,
$\left(\X^{(c)}\right)^{\T}\y^{(c)}/n=\S_{\mathbf{x}\mathbf{y}}$
суть выборочные ковариационные матрицы (это можно вручную расписать
и убедиться); тогда в их терминах
\[
\hat{\bb}^{(c)}=(\hat{b}_{1},\ldots,\hat{b}_{k})^{\T}=\left(\left(\X^{(c)}\right)^{\T}\X^{(c)}/n\right)^{-1}\left(\X^{(c)}\right)^{\T}\y^{(c)}/n=\S_{\mathbf{x}\mathbf{x}}^{-1}\S_{\mathbf{x}\y}.
\]


%\begin{cor*}
%Чем более скоррелированы признаки, тем более пропорциональны столбцы
%$\S_{\mathbf{x}\mathbf{x}}$ и тем более вырождена $\S_{\mathbf{x}\mathbf{x}}$,
%значит <<больше>> $\S_{\mathbf{x}\mathbf{x}}^{-1}$, следовательно
%$\hat{\bb}^{(c)}$ и разница $\y^{(c)}-\hat{\y}^{(c)}=\y^{(c)}-\X^{(c)}\hat{\bb}^{(c)}$.
%\end{cor*}
\item Ковариационная матрица:
\[
\cov\hat{\bb}^{(c)}=\sigma^{2}(\left(\X^{(c)}\right)^{\T}\X^{(c)})^{-1}=\frac{\sigma^{2}}{n}\cdot\S_{\mathbf{x}\mathbf{x}}^{-1}\xrightarrow[n\to\infty]{}0
\]
\begin{comment}
\begin{enumerate}
\item
\begin{eqnarray*}
\cov\hat{\bb}^{(c)} & = & \cov\left(\S_{\mathbf{x}\mathbf{x}}^{-1}\S_{\mathbf{x}\y}\right)=\S_{\mathbf{x}\mathbf{x}}^{-1}\cov\left(\S_{\mathbf{x}\y}\right)\left(\S_{\mathbf{x}\mathbf{x}}^{-1}\right)^{\T}=\S_{\mathbf{x}\mathbf{x}}^{-1}\cov\left(n\left(\X^{(c)}\right)^{\T}\y^{(c)}\right)\S_{\mathbf{x}\mathbf{x}}^{-1}\\
 & = & \S_{\mathbf{x}\mathbf{x}}^{-1}\left(\X^{(c)}\right)^{\T}\,n\cov\left(\y^{(c)}\right)\X^{(c)}\S_{\mathbf{x}\mathbf{x}}^{-1}=\sigma^{2}\S_{\mathbf{x}\mathbf{x}}^{-1}n\left(\X^{(c)}\right)^{\T}\X^{(c)}\S_{\mathbf{x}\mathbf{x}}^{-1}\\
 & = & \sigma^{2}\S_{\mathbf{x}\mathbf{x}}^{-1}\S_{\x\x}\S_{\mathbf{x}\mathbf{x}}^{-1}=\sigma^{2}\S_{\mathbf{x}\mathbf{x}}^{-1}\neq\frac{\sigma^{2}}{n}\cdot\S_{\mathbf{x}\mathbf{x}}^{-1}\xrightarrow[n\to\infty]{}0.
\end{eqnarray*}
\end{enumerate}
\end{comment}

\item Аналогично,
\[
\hat{\bb}^{(s)}=\R_{\mathbf{x}\mathbf{x}}^{-1}\R_{\mathbf{x}\y}
\]
 и
\[
\cov\hat{\bb}^{(s)}=\frac{\sigma^{(s)2}}{n}\R_{\mathbf{x}\mathbf{x}}^{-1},\quad\sigma^{(s)}=\frac{\sigma}{\left\Vert \y^{(c)}\right\Vert }.
\]

\end{enumerate}

\section{Сравнение оценок}

По аналогии с одномерным случаем, \emph{наилучшая оценка} --- с минимально
возможной дисперсией; аналог дисперсии --- ковариационная матрица.
Порядок вводится следующим образом:
\begin{defn*}
$\A<\B\iff\A-\B$ отрицательно определена, т.е.
\[
\forall\boldsymbol{\gamma}\ \boldsymbol{\gamma}^{\T}(\A-\B)\boldsymbol{\gamma}<0.
\]
\end{defn*}
\begin{rem*}
Пусть $\boldsymbol{\gamma}^{(i)}=(0,\ldots,\underbrace{1}_{i},\ldots,0)^{\T}$;
тогда $a_{ii}<b_{ii}$.\end{rem*}
\begin{thm*}[Гаусс-Марков]
В условиях $\E\epsilon_{i}=0,\ \D\epsilon_{i}=\sigma^{2},\ \epsilon_{i}\indep\epsilon_{j}$,
$\hat{\bb}_{\OLS}$ является <<BLUE>>: <<best linear unbiased estimate>>. То есть $\hat{\bb}_{\OLS}$ обладает наименьшей ковариационной матрицей среди всех линейных (линейно зависящих от $\y$) несмещенных оценок.\end{thm*}
\begin{rem*}
Наименьшая ковариационная матрица гарантирует, что дисперсия оценки каждого коэффициента минимальна (следует из предыдущего замечания).
\end{rem*}

\begin{thm*}
Если $\eb\sim\N(\mathbf{0},\sigma^{2}\Ib)$, то
\[
\hat{\bb}_{\OLS}=\hat{\bb}_{\MLE}.
\]
\end{thm*}
\begin{proof}
Так как $\y \sim \N(\X \bb, \sigma^2 \Ib$), то
\begin{eqnarray*}
\hat{\bb}_{\MLE} = \argmax\limits_{\bb}\P(\y\mid\bb)
	& = & \argmax\limits_{\bb} \frac{1}{(2\pi)^{n/2}\sqrt{\det\sigma^{2}\Ib}}\exp\left\{ -\frac{1}{2}\left(\mathbf{y}-\X \bb\right)\sigma^{-2}\Ib(\y-\X \bb)^{\T}\right\} \\
    & = & \argmax\limits_{\bb} \exp\left\{ -\frac{1}{2 \sigma^2}\left(\mathbf{y}-\mb\right)(\y-\mb)^{\T}\right\} \\
    & = & \argmax\limits_{\bb} \exp\left\{ -\frac{1}{2\sigma^2} \left\Vert \X \bb - \y \right\Vert^2 \right\} \\
    & = & \argmin\limits_{\bb} \left\Vert \X \bb - \y \right\Vert^2  = \hat{\bb}_{\OLS}
\end{eqnarray*}

\end{proof}

\section{Разложение суммы квадратов и оценка $\sigma^{2}$}

Разложение суммы квадратов в случае модели линейной регрессии имеет вид (даже если модель не верна):
$$\sum_{i=1}^{n}(y_{i}-\bar{y})^{2}=\sum_{i=1}^{n}(\hat{y}_{i}-\bar{y})^{2}+\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}.$$

Обозначим $\SSE = \mathrm{SSError} = \sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}$.
Пусть модель верна и ошибки имеют нормальное распределение $N(0,\sigma^2)$.
Тогда, с помощью теореме Cochran можно получить (без док-ва):
\[
\frac{\SSE}{\sigma^{2}}\sim\chi^{2}(\underbrace{n-m}_{n-k-1})
\]
и оценкой методом подстановки для $\sigma^{2}$ будет $\SSE/n$; несмещенной
оценкой (с поправкой на число степеней свободы) будет
\[
\hat{\sigma}^{2}=\frac{\SSE}{n-m}.
\]

\section{Проверка значимости коэффициентов линейной регрессии и доверительные
интервалы}
\begin{defn*}
Коэффициент $b_{i}$ \emph{значим}, если отвергается $H_{0}:b_{i}=0$.
Если коэффициент значим, значит признак существенен для регрессии.
\end{defn*}
Для построения точного критерия, предполагают $\eb\sim\N(\mathbf{0},\sigma^{2}\Ib)$.
Значит, поскольку $\hat{\bb}=(\X^{\T}\X)^{-1}\X^{\T}\y=(\X^{\T}\X)^{-1}\X^{\T}(\X\boldsymbol{\beta}+\eb)$,
$\hat{\bb}$ имеет тоже нормальное распределение со средним $\mathbf{0}$
(по несмещенности), но какой-то ковариационной матрицей: $\hat{\bb}\sim\N(\mathbf{0},\Sb)$.
Тогда $\E\hat{b}_{i}=b_{i}=0,\ \D\hat{b}_{i}=\sigma_{i}^{2}$ и
\[
t=\frac{\hat{b}_{i}-b_{i}}{\sqrt{\D\hat{b}_{i}}}=\frac{\hat{b}_{i}}{\sqrt{\sigma^{2}((\X^{\T}\X)^{-1})_{ii}}}=\frac{\hat{b}_{i}}{\sqrt{\sigma^{2}/n\cdot\left(\S_{\mathbf{x}\mathbf{x}}^{-1}\right)_{ii}}}=\sqrt{n}\frac{\hat{b}_{i}}{\sigma\left(\S_{\mathbf{x}\mathbf{x}}^{-1}\right)_{ii}^{1/2}}\sim\N(0,1).
\]
Заметим, что в $(\X^{\T}\X)^{-1}$ нумерация идет от 0, а в $\S_{\mathbf{x}}$ от 1.
Подставляя оценку $\sigma$, получают
\[
t=\sqrt{n}\frac{\hat{b}_{i}}{\hat{\sigma}\left(\S_{\mathbf{x}\mathbf{x}}^{-1}\right)_{ii}^{1/2}}=\sqrt{n}\frac{\hat{b}_{i}}{{\displaystyle \sqrt{\frac{\SSE}{\left(n-m\right)}}}\left(\S_{\mathbf{x}\mathbf{x}}^{-1}\right)_{ii}^{1/2}}=\dfrac{{\displaystyle \frac{\sqrt{n}\hat{b}_{i}}{\sigma\left(\S_{\mathbf{x}\mathbf{x}}^{-1}\right)_{ii}^{1/2}}}}{{\displaystyle \sqrt{\frac{\SSE}{\left(n-m\right)\sigma^{2}}}}}=\frac{\N(0,1)}{{\displaystyle \sqrt{\frac{\chi^{2}(n-m)}{n-m}}}}\sim\t(n-m).
\]



\subsection{Расстояние Махаланобиса}

Если на прямой разброс удобно измерять стандартных отклонениях $\sigma$,
то в многомерном пространстве аналогом такой характеристики является
расстояние Махаланобиса (Mahalanobis distance).
\begin{defn*}
Пусть $\V$ --- неотрицательно определенная симметричная матрица;
тогда \emph{расстояние Махаланобиса} есть
\[
r_{M}^{2}(\x,\y;\V)=(\x-\y)^{\T}\V^{-1}(\x-\y).
\]
\end{defn*}
\begin{rem*}
Если $\xib\sim\N(\mb,\V)$, то
\[
\pdf_{\xib}(\x)=C\cdot\exp\left\{ -\frac{1}{2}(\x-\mb)^{\T}\V^{-1}(\x-\mb)\right\} =C\cdot\exp\left\{ -\frac{1}{2}r_{M}^{2}(\x,\mb;\V)\right\} .
\]
 Для любых двух $\x_{1},\x_{2}$ на линии уровня, $\pdf_{\xib}(\x_{1})=\pdf_{\xib}(\x_{2})$.
Значит, $r_{M}^{2}(\x_{1},\mb;\V)=r_{M}^{2}(\x_{2},\mb;\V)$, в то
время, как Евклидово расстояние не обязано быть одинаковым из-за разной
выраженности главных компонент. Однако $r_{M}^{2}(\x,\y;\Ib)=\left\Vert \x-\y\right\Vert _{2}^{2}$.
Таким образом, $r_{M}^{2}$ --- это Еквлидово расстояние с поправкой
на ковариацию, задаваемую $\V$.
\end{rem*}

\begin{prop*}
Если $\xib\sim\N(\mb,\V)$, то
\[
r_{M}^{2}(\xib,\mb;\V)=(\xib-\mb)^{\T}\V^{-1}(\xib-\mb)\sim\chi^{2}(m)
\]
 как сумма квадратов центрированных и нормированных нормальных случайных
величин.
\end{prop*}
Действительно,
\[
\etab=\V^{-1/2}(\xib-\mb)\sim\N(0,\Ib)\implies r_{M}^{2}(\xib,\mb;\V)=r_{M}^{2}(\etab,\mathbf{0};\Ib)=\etab^{\T}\etab\sim\chi^{2}(m).
\]



\subsection{Доверительный эллипсоид}

В одномерном случае симметричного распределения, область носителя,
где лежит $\gamma$ всех значений распределения определяется равенством
\[
\P(\left|\xi - \E\xi\right|<\sqrt{\D\xi}c_{\gamma})=\gamma.
\]
Т.е. как такое множество значений, что расстояние их от среднего с
учетом стандартного отклонения меньше квантиля уровня $\gamma$. В
случае оценки среднего $\mu_{0}$, например, получают стандартное с $\SE = \sqrt{\D\bar{x}}$
\[
\P\left(\frac{\left|\bar{x}-\mu_{0}\right|}{\SE}<c_{\gamma}\right)=
\P\left(-c_{\gamma}<\sqrt{n}\frac{\bar{x}-\mu_{0}}{\sigma}<c_{\gamma}\right),\quad\sqrt{n}\frac{\bar{x}-\mu_{0}}{\sigma}\sim\N(0,1)
\]
так что $c_{\gamma}=\qnt_{\N(0,1)}\gamma$.

Аналогично можно нарисовать $m$-мерный эллипсоид, в который помещается выборка
с точностью $\gamma$. %
\begin{comment}
$\SD$? $\SD$ --- матрица ковараций (корень ея)?
\end{comment}
{} Расстояние с учетом ковариации будет задаваться соответственно параметризованным
расстоянием Махаланобиса:
\[
\P(r_{M}^{2}(\xib,\mb;\SD)<c_{\gamma})=\gamma.
\]


В случае, если $\hat{\thb}_{n}\tod\N(\thb,\Sb)$%
\begin{comment}
$\thb$? не $\mathbf{0}$?
\end{comment}
, по предыдущему,
\[
r_{M}^{2}(\thb,\hat{\thb}_{n})\sim\chi^{2}(m).
\]
 Значит,
\[
\P(r_{M}^{2}(\thb,\hat{\thb}_{n})<c_{\gamma})=\gamma,\quad c_{\gamma}=\qnt_{\chi^{2}(m)}\gamma.
\]
\begin{comment}
Но ведь не симметричное?
\end{comment}



\section{Значимость регрессии}

$H_{0}:\bb^{(c)}=\mathbf{0}$. Напомним что $\bb^{(c)} = (b_1,\ldots,b_k)^\mathrm{T}$ и равенство его нулю означает то, что предсказание равно константе и не зависит от значений иксов.
Эту гипотезу можно проверить тремя способами:
\begin{enumerate}
\item Аналогично парной регрессии: критерий
\[
t=r_{M}^{2}\left(\hat{\bb}^{(c)},\mathbf{0};\cov(\hat{\bb}^{(c)})\right)\sim\chi^{2}(k)
\]
 а именно,
\[
t=\left(\hat{\bb}^{(c)}\right)^{\T}\left(\cov(\hat{\bb}^{(c)})\right)^{-1}\hat{\bb}^{(c)}=\left(\hat{\bb}^{(c)}\right)^{\T}\left(\frac{\sigma^{2}}{n}\cdot\S_{\mathbf{x}\mathbf{x}}^{-1}\right)^{-1}\hat{\bb}^{(c)}=\frac{n\left(\hat{\bb}^{(c)}\right)^{\T}\S_{\mathbf{x}\mathbf{x}}\hat{\bb}^{(c)}}{\sigma^{2}}.
\]
Неизвестный $\sigma^{2}$ следует оценить как
\[
s^{2}=\frac{\SSE}{n-(k+1)};
\]
 тогда
\[
\frac{n\left(\hat{\bb}^{(c)}\right)^{\T}\S_{\mathbf{x}\mathbf{x}}\hat{\bb}^{(c)}/k}{s^{2}}\sim\mathrm{F}(k,n-(k+1)).
\]

\item Через ANOVA (разложение дисперсии):
Разложение дисперсии
\[
\D\eta=\E(\eta-\E\eta)^{2}=\E(\hat{\eta}^*-\E\eta)^{2}+\E(\eta-\hat{\eta}^*)^{2},
\]
 где $\hat{\eta}^*$ --- наилучшее линейное приближение от $\xib$, на выборочном языке будет иметь вид
\[
\underbrace{\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}}_{\mathrm{SSTotal}}=
\underbrace{\sum_{i=1}^{n}(\hat{y}_{i}-\bar{y})^{2}}_{\mathrm{SSRegr}}+
\underbrace{\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}}_{\mathrm{SSError}}.
\]
В случае, когда регрессоры не случайны (есть неслучайная матрица данных $\X$ и случайный отклик $\y$, как у нас сейчас),
то же самое разложение имеет место.

\begin{rem*}
Иногда также пишут
\[
\mathrm{SSTotal}=\mathrm{SSEffect}+\mathrm{SSResidual},
\]
что ведет к неиллюзорной путанице!
\end{rem*}
Пусть $\eb\sim\N(\mathbf{0},\sigma^{2}\Ib)$. Тогда, с помощью теореме Cochran можно получить (без док-ва):
\begin{comment}
Почему?
\end{comment}
\[
\frac{\SST}{\sigma^{2}}\sim\chi^{2}(n-1),\quad\frac{\SSR}{\sigma^{2}}\sim\chi^{2}(\underbrace{m-1}_{k}),\quad\frac{\SSE}{\sigma^{2}}\sim\chi^{2}(\underbrace{n-m}_{n-k-1})
\]
{} и $\SSE\indep\SSR.$ %
\begin{comment}
Поскольку $\widehat{\D\eta}=1/n\cdot\sum_{i=1}^{n}\left(\eta_{i}-\bar{\mathbf{y}}\right)$,
можно было бы оценивать $\sigma^{2}$ как $\SSE/n$ --- но ведь $\hat{y}_{i}=\E(\eta\mid\xi)\neq\bar{\y}=\E\eta.$
\end{comment}
\begin{comment}
Выборка не повторная, так что нельзя считать дисперсию?
\end{comment}

\begin{rem*}
Утверждение про распределение $\SSE$ справедливо всегда при нормальном распределении
ошибок; про $\SST$ и $\SSR$ это верно только если $\bb^{(c)}=\mathbf{0}$.
Именно поэтому применяется $F$-критерий для проверки значимости регрессии.
\end{rem*}

Таким образом, в качестве статистики F-критерия можно взять, как и в дисперсионном анализе,
\[
t=\frac{\SSR/k}{\SSE/(n-(k+1))}\sim\mathrm{F}(k,n-(k+1))
\]

Критическая область, очевидно, справа, так как 'идеальное значение' --- 0.

\begin{rem*}
У этой статистики с предыдущей совпадает также и числитель, хотя, чтобы в этом убедиться, надо провести некоторые выкладки, так это не очевидно.
\end{rem*}
\item Через коэффициент детерминации регрессии: известно выражение для множественного
коэффициента корреляции:
\[
R^{2}(\eta;\xi_{1},\ldots,\xi_{k})=\frac{\E(\hat{\eta}^{*}-\E\eta)^{2}}{\D\eta},\quad\D\eta=\E(\eta-\E\eta)^{2}=\E(\hat{\eta}^{*}-\E\eta)^{2}+\E(\eta-\hat{\eta}^{*})^{2};
\]
\begin{comment}
однако в текущей постановке задачи $\eta$ не присутствует (?), поэтому
\end{comment}
{} на выборочном языке для множественной линейной регрессии получают
\begin{eqnarray}
\label{eq:determ}
R^{2} & = & \frac{\SSR}{\SST}=\frac{\SST-\SSE}{\SST}=1-\frac{\SSE}{\SST}.
\end{eqnarray}

Если матрица регрессоров $\X$ фиксирована (т.е. не является выборкой из распределения $\xib$),
то $R^{2}$, вычисленный по той же формуле \eqref{eq:determ}, называется коэффициентом детерминации.

\begin{rem*}
При удалении даже незначимого признака $R^{2}$ уменьшится; однако $\mathrm{adjusted\,}R^{2}$
\begin{eqnarray*}
\mathrm{adjusted\,}R^{2} & = & 1-\frac{\SSE/(n-(k+1))}{\SST/(n-1)}\xrightarrow[n\to\infty]{}R^{2}
\end{eqnarray*}
не обязательно в силу поправки $n-(k+1),$ действующей как штраф за
количество переменных.
\end{rem*}

Несложные манипуляции позволяют выписать статистику критерия ANOVA через коэффициент детерминации:
\[
t=\frac{{\displaystyle \frac{\SSR}{k}}}{{\displaystyle \frac{\SSE}{n-(k+1)}}}=\frac{{\displaystyle \frac{\SSR}{k\,\SST}}}{{\displaystyle \frac{{\displaystyle \frac{(\SST-(\SST-\SSE))}{\SST}}}{n-(k+1)}}}=\frac{R^{2}/k}{\left(1-R^{2}\right)/(n-(k+1))}.
\]


\end{enumerate}



\section{Анализ оценок коэффициентов}

Для анализа оценок коэффициентов можно посмотреть на попарные срезы
доверительного эллипсоида; точнее, на двумерные эллипсоиды. Для пары коэффициентов ${\beta}_{i},{\beta}_{j}$
его можно нарисовать (самостоятельно), в качестве центра взяв  точку $(\hat{\beta}_{i},\hat{\beta}_{j})^{\T}$,
наклон главной оси и вытянутость определив по величине $\corr(\hat{\beta}_{i},\hat{\beta}_{j}).$
%Если центр достаточно далек от 0, то граница доверительного эллипсоида не должны пересекать
%оси, потому что гипотеза $\b_{i}=\b_{j}=0$ отвергается.
\begin{itemize}
\item Чем дальше от начала координат эллипсоид, тем больше значимость
признаков.
\item Чем больше корреляция тем менее адекватно центр отражает ситуацию.
\item Возможны два случая: когда эллипсоид  перпендикулярен или сонаправлен
прямым $y=\pm x$; в первом случае (<<хорошем>>) коэффициенты значимы в
совокупности (даже если один близок к 0, то второй вполне далек и
наоборот), во втором случае эллипсоид может довольно близко подходить к точек (0,0), т.е. оба коэффициента могут быть как одновременно
малыми, так и большими (и, значит, и сильно, и слабо влиять на результат).
\end{itemize}

\subsection{Корреляция между оценками коэффициентов в двумерном случае}

При возрастании корреляции признаков:
\begin{itemize}
\item дисперсия оценок коэффициентов стремится к бесконечности;
\item становится сложно оценить вклад каждого признака в регрессию.\end{itemize}
\begin{example*}
Пусть $k=2$, $\eta=b_{0}+b_{1}\xi_{1}+b_{2}\xi_{2}$. Пусть также
матрица корреляций есть
\[
\R_{\x\x}=\begin{pmatrix}1 & \rho\\
\rho & 1
\end{pmatrix}.
\]
 Тогда
\[
\cov\hat{\boldsymbol{\beta}}=\frac{\sigma^{(s)2}}{n}\R_{\x\x}^{-1}=\frac{\sigma^{(s)2}}{n}\frac{1}{1-\rho^{2}}\begin{pmatrix}1 & -\rho\\
-\rho & 1
\end{pmatrix}.
\]
 Значит, $\D\hat{\beta}_{i}\xrightarrow[\rho\to1]{}\infty$. %
\begin{comment}
\[
\mathrm{corr}\,\hat{\boldsymbol{\beta}}=\begin{pmatrix}1 & -\rho\\
-\rho & 1
\end{pmatrix}.
\]
 так что исходные признаки положительно коррелированы $\implies$
оценки коэффициентов отрицательно.
\end{comment}
\begin{comment}
$\xi_{1}=\xi=\xi_{2}\implies\xi_{1}+\xi_{2}=2\xi\implies\xi=k\xi_{1}-(k-1)\xi_{2}$
???
\end{comment}

\end{example*}

Комментарий по поводу того, почему естественно, что знак корреляции между оценками коэффициентов регрессии обратен к знаку корреляции между признаками.

Пусть $\rho(\xi_1,\xi_2) = 1$, например, $\xi_1 = \xi$, $\xi_2 = \xi$. Пусть $\eta = 3\xi$.
Тогда возможны варианты:\\
\[
\begin{array}{lll}
\eta &= 2\cdot\xi_1 &+ 1\cdot\xi_2\\
\eta &= 1\cdot\xi_1 &+ 2\cdot\xi_2\\
\eta &= 0\cdot\xi_1 &+ 3\cdot\xi_2\\
\eta &= -1\cdot\xi_1 &+ 4\cdot\xi_2\\
...&
\end{array}
\]
Видно, что между коэффициентами регрессии явная отрицательная линейная зависимость.

\subsection{Супрессоры}
TODO

\subsection{Избыточность (redundancy) и ручное удаление признаков}
С этой проблемой можно бороться, удаляя подходящие признаки из анализа\footnote{Нет признака --- нет проблемы.}
по следующим критериям:
\begin{enumerate}
\item Множественный коэффициент корреляции
\[
R^{2}(\xi_{i};\left\{ \xi_{j},\ j\neq i\right\} ).
\]
Чем он больше, тем скорее $i$-й признак нужно удалить.
\item Допустимость $i$-го признака:
\[
\mathrm{tolerance}_{i}=1-R^{2}(\xi_{i};\{\xi_{j},\ j\neq i\}).
\]
\begin{comment}
$\mathrm{tolerance}=\min\mathrm{tolerance}_{i}$
\end{comment}
{} Чем он меньше, тем скорее $i$-й признак нужно удалить. Помимо предыдущего
соотношения справедливо
\[
\D\hat{b}_{i}=\frac{\sigma^{2}}{\sum_{\ell=1}^{n}(x_{\ell}-\bar{\mathbf{x}}_{i})^{2}}\frac{1}{\mathrm{tolerance}_{i}},\quad\frac{1}{\mathrm{tolerance}_{i}}\text{ --- Variance Inflation Factor},
\]
так что при маленькой допустимости дисперсия велика.
\item Частные корреляции
\[
\rho(\xi_{i},\eta\mid\left\{ \xi_{j},\ j\neq i\right\} )=\rho(\xi_{i}-\hat{\xi}_{i},\eta-\hat{\eta})
\]
 Чем $i$-я частная корреляция больше, тем больше вклад признака в
регрессию (тем менее он предпочтителен для удаления).
\item Полу-частные корреляции
\[
\rho(\xi_{i}-\hat{\xi}_{i},\eta).
\]

\end{enumerate}

\subsection{Проверка гипотезы о том, что набор признаков избыточен}

Пусть $\bb=(b_{0},\ldots,b_{k-r},\underbrace{b_{k-r+1},\ldots,b_{k}}_{r\text{ штук}})^{\T}$.
Если $H_{0}:\bb_{k-r+1,k}=\mathbf{0}$ не отвергается, значит последние
$r$ признаков не влияют на модель и следует выбрать более простую
модель --- без этих коэффициентов. Можно использовать расстояние Махаланобиса
до 0 в метрике $\cov(\bb_{k-r+1,k})$:
\begin{eqnarray*}
t & = & r_{M}^{2}(\hat{\bb}_{k-r+1,k},\mathbf{0};\cov(\bb_{k-r+1,k}))\sim\chi^{2}(r)\\
 & = & \hat{\bb}_{k-r+1,k}^{\T}\left(((\X^{\T}\X)^{-1})_{(\mathrm{IV})}\right)^{-1}\hat{\bb}_{k-r+1,k}/\sigma^{2},
\end{eqnarray*}
 где $((\X^{\T}\X)^{-1})_{(\mathrm{IV})}$ --- IV квадрант $(\X^{\T}\X)^{-1}$.
Если $\sigma^{2}$ неизвестна, то
\begin{eqnarray*}
t & = & \frac{\hat{\bb}_{k-r+1,k}^{\T}\left(((\X^{\T}\X)^{-1})_{(\mathrm{IV})}\right)^{-1}\hat{\bb}_{k-r+1,k}/r}{\hat{\sigma}^{2}}\sim\mathrm{F}(r,n-(k+1))\\
 & = & \frac{(R_{1,k}^{2}-R_{1,k-r}^{2})/r}{(1-R_{1,k}^{2})/(n-m)}.
\end{eqnarray*}

\subsection{Stepwise автоматическое удаление/добавление признаков}

Выбор оптимального набора признаков можно производить автоматически,
по одному добавляя признаки (<<Forward stepwise regression>>) или
убирая их (<<Backward>>). Пусть вариант Forward. На шаге $i$ добавляется
тот признак, что максимизирует
\[
R_{1,i+1}^{2}-R_{1,i}^{2};
\]
остановиться следует, когда $\left|R_{1,i+1}^{2}-R_{1,i}^{2}\right|$
достаточно мало. $H_{0}:R_{1,i+1}^{2}-R_{1,i}^{2}=0$, т.е. $b_{i+1}=0$
перед добавленным признаком.
\[
t=\frac{\hat{b}_{i}}{\SE(\hat{b}_{i})}\sim\t(n-m).
\]
 Тогда $k=i+1$, $r=1$ и статистика будет иметь вид
\[
t^2=\frac{(R_{1,i+1}^{2}-R_{1,i}^{2})}{(1-R_{1,i+1}^{2})/(n-(i+2))}\sim\mathrm{F}(1,n-(i+2)).
\]
 По сути, это есть перемасштабированное значение разницы $R_{1,i+1}^{2}-R_{1,i}^{2}$.
\begin{rem*}
\begin{comment}
?
\end{comment}
Однако признак выбран <<лучший>> (а не случайный), значит распределение
не $\mathrm{F}$.\end{rem*}
\begin{itemize}
\item Полное решение задачи --- выбрать $\ell$ признаков из $k$ перебором.
\item Жадный алгоритм --- последовательно выбирать наиболее подходящие признаки.
\end{itemize}
\begin{comment}
Комбинация полной задачи и жадного алгоритма\ldots{}
\end{comment}


В Statistica есть критерий автоматической остановке для stepwise отбора признаков.
F to enter в forward варианте --- это пороговое значение для F, если F < этого числа, то останов.
В backward варианте есть F to remove: если F > F to
remove, то STOP. Только F to remove должно быть больше F to enter (на самом деле, на каждом шаге проверяется, можно ли добавить признак, а потом какой-то другой удалить и неравенство нужно, чтобы процедура не зациклилась.)
Имеет смысл установить такие пороги, чтобы критерий остановки не сработал, а потом посмотреть  на таблицу Stepwise summary.

Можно нарисовать, как ведет себя коэффициент детерминации в варианте forward и backward.
Это монотонные функции, но необязательно вторая производная одного знака. Отсюда можно увидеть, что критерий остановке в варианте backward более безопасен.

\subsection{Выбор модели на основе информационных критериев AIC и BIC }

См. отдельный файл на wiki курса. Для информационных критериев нужна параметрическая модель, так как они строятся на основе функции правдоподобия и вводят штраф за число параметров.

Отдельно отметим, что информационные критерии показывают, какая модель лучше, но не утверждают, что лучшая модель является правильной.

Заметим, что в случае нормального распределения для всех случайных величин справедливо%
\[
\eta=\E(\eta\mid\xi_{1},\ldots,\xi_{i})+(\eta-\E(\eta\mid\xi_{1},\ldots,\xi_{i})),
\]
так как имеем ортогональность второго слагаемого первому и линейность ошибок. Значит все модели <<верны>> и можно среди них выбрать наилучшую.

\subsection{О множественном коэффициенте корреляции и саппрессорах}

Известно, что $\rho(\eta,\xi)$ есть косинус угла между $\eta$ и
$\xi$ в соответствующем пространстве. Аналогично можно думать, что
$R^{2}$ есть косинус между $\eta$ и линейным пространством, натянутым
на $\xi_{1},\ldots,\xi_{k}$:
\[
R^{2}=\cos^{2}(\eta,\mathcal{L}(\xi_{1},\ldots,\xi_{k})).
\]

Для коэффициента детерминации то же самое, только вместо случайных величин стоят вектора-признаки и косинус --- это обычный косинус угла между векторами.

Возможна ситуация, когда $\cos^{2}(\eta,\mathcal{L}(\xi_{1},\xi_{2}))=1=R^{2}(\eta; \xi_1, \xi_2)$
--- т.е. $\eta$ лежит в $\mathcal{L}(\xi_{1},\xi_{2})$ (и предсказание
абсолютно точно), но, тем не менее,  $\cor(\xi_{1},\eta)\approx 0$, $\cor(\xi_{2},\eta)\approx 0$. Это возможно, если $\cor(\xi_{1},\xi_{2})\approx \pm 1$ (почти
коллинеарны).
$\xi_{1}$ называется <<саппрессором>>  (suppressor) по отношению к $\xi_{2}$
(или наоборот). %
\begin{comment}
TODO: раскрыть тему
\end{comment}
Подробнее, см \url{https://stats.stackexchange.com/a/73876}.

\subsection{Как понять, что все хорошо}
Если stepwise регрессия вперед и назад дает примерно одинаковые результаты и вторая производная одного знака (отрицат.), если нет супрессоров, нет плохих доверительных эллипсоидов, нет коэффициентов регрессии (перед стандартизованными признаками) больше 1.

Но самый хороший вариант, конечно, если регрессоры (почти) независимы. Если удается найти набор слабо зависимых признаков, которые предсказывают лишь немного хуже, чем полный набор, то это удача.

Также, если данные (регрессоры) должны собираться, то добавляются еще неформальные характеристики признаков ---
признаки должны легко и дешево собираться и не иметь много пропусков.

\subsection{Заполнение пропусков}
К стандартным вариантам casewise и pairwise добавляет вариант заполнения пропусков средним значением.
Здесь такая опасность: если большое количество пропусков заполнить средними, то искусственно уменьшится
дисперсия признаков и, тем самым, ширина доверительных интервалов для оценок параметров (увеличится значимость).%

Есть еще варианты заполнения пропусков по регрессии на признаки с малым числом пропусков, но это нужно делать в ручном режиме.

\section{Анализ аутлаеров}
Выделяющиеся наблюдения всегда выделяются по отношению к какой-то закономерности. Одним из стандартных методов определения выделяющегося наблюдения по отношению к конкретному методу является сравнение результатов методв, которые получены с участием индивида и без его участия.

\subsection{Matrix plot}

Аутлаеров можно найти <<на глаз>> при помощи стандартного matrix
plot данных.


\subsection{Deleted residuals}

можно применить технику кросс-валидации: удалить признак, построить
модель, сравнить. Если индивид является аутлаером, то наблюдение $y_{i}$
на нём <<перетягивает>> на себя регрессионную прямую. Тогда явно
<<большой>> будет разница
\[
r_{i}^{(i)}=y_{i}-\hat{y}_{i}^{(i)}
\]
 между $\hat{y}_{i}^{(i)}$ --- предсказание по регрессии на $i$-м индивиде с помощью коэффициентов, оцененных
без этого индивида, и $y_{i}$ --- наблюдении на $i$-м индивиде. $r_{i}^{(i)}$
будет <<большой>> также по сравнению с $r_{i}=y_{i} - \hat{y}_{i}$.
Напротив, если $i$-й индивид аутлаером не является, то будет справедливо
приближенное равенство $r_{i}^{(i)}\approx r_{i}$. Deleted residuals (`удаленные остатки') всегда не больше (по модулю)
residuals, поэтому прямая $y=x$ для точек $(r_{i},r_{i}^{(i)})$ не получится.


\subsection{Studentized residuals}

А как просто посмотреть, остатки слишком большие или не слишком? Для этого нужно знать распределение остатков. Оно отличается от распределения ошибок с ковар.матрицей $\sigma^2 \I_n$

Справедливо
\[
\hat{\y}=\H\y\implies(\y-\hat{\y})=(\Ib-\H)\y
\]
 откуда
\[
\cov(\y-\hat{\y})=\cov(\Ib-\H)\y=(\Ib-\H)\cov\y(\Ib-\H)^{\T}=\sigma^{2}(\Ib-\H)^{\T}(\Ib-\H)=\sigma^{2}(\Ib-\H)
\]
 потому что $\Ib-\H$ --- матрица проектора. Тогда,
\[
\D(y_{i}-\hat{y}_{i})=\D r_{i}=\sigma^{2}(1-h_{ii}).
\]
\begin{comment}
Значит, остатки зависимы? в отличии от ошибок $\eb$
\end{comment}
Как следствие, $\D\epsilon_{i}=\sigma^{2}\geq\D r_{i}$.
\begin{defn*}
$h_{ii}$ --- рычаг\footnote{<<Leverage>>.}.
\end{defn*}
Чем больше $i$-й рычаг, тем меньше %
\begin{comment}
Меньше?
\end{comment}
{} ошибка на $i$-м индивиде, так как он перетягивает на себя.%
\begin{comment}
reflects the amount that an observation influences its own fit
\end{comment}

Получаем в нормальной модели $\frac{r_{i}}{\sqrt{\D r_{i}}}=\frac{r_{i}}{\sigma\sqrt{1-h_{ii}}} \sim \N(0,1)$.

\begin{defn*}
Стандартизированные (также называют internally studentized) остатки:
\[
\frac{r_{i}}{\sqrt{\widehat{\D r_{i}}}}=\frac{r_{i}}{\hat\sigma\sqrt{1-h_{ii}}}.
\]

\end{defn*}
Можно рассмотреть $\hat{\sigma}^{(i)}$ --- оценку дисперсии без $i$-го
индивида (т.е. на основе суммы квадратов не всех остатков, без $i$-го индивида); тогда, при нормально распределенных ошибках наблюдения,
\[
\frac{r_{i}}{\hat{\sigma}^{(i)}\sqrt{1-h_{ii}}}\sim\t(n-m-1)
\]
 (<<$-1$>> потому что меньше на одного индивида). Такие остатки называют стьюдентизированными (или externally studentized).
\begin{rem*}
Полученную величину рычага%
\begin{comment}
Да?
\end{comment}
{} можно сравнивать со <<средним>> значением рычага
\[
\frac{1}{n}\sum_{i=1}^{n}h_{ii}=\frac{1}{n}\tr\H=\frac{1}{n}\rk\H=\frac{k+1}{n}.
\]
 (как след идемпотентной матрицы, равный её рангу\footnote{\url{http://math.stackexchange.com/a/101515}}:
след есть сумма собственных чисел, однако у идемпотента два возможных
собственных числа: 0 и 1, а кратность 1 в точности равна рангу).
\end{rem*}

\subsection{Расстояние по Куку и расстояние Махаланобиса}

Пусть $\hat{\bb}^{(i)}$ --- оценка коэффициентов регрессии, полученная по выборке без $i$-го индивида. Если расстояние
между $\hat{\bb}^{(i)}$ и $\hat{\bb}$ <<большое>>, то $i$-й индивид
есть аутлаер:
\[
r_{M}^{2}(\hat{\bb},\hat{\bb}^{(i)};\cov\hat{\bb})=(\hat{\bb}-\hat{\bb}^{(i)})^{\T}\cov^{-1}(\hat{\bb})(\hat{\bb}-\hat{\bb}^{(i)})=\frac{1}{\sigma^{2}}(\hat{\bb}-\hat{\bb}^{(i)})\X^{\T}\X(\hat{\bb}-\hat{\bb}^{(i)})
\]
 так что расстояние по Куку определяется как
\[
\frac{(\hat{\bb}-\hat{\bb}^{(i)})\X^{\T}\X(\hat{\bb}-\hat{\bb}^{(i)})/m}{\hat{\sigma}^{2}}.
\]

Расстояние по Куку показывает выбросы по отношению к регрессии (outliers всегда по отношению к чему-то, какой-то закономерности).

Можно еще рассмотреть выбросы по отношению к распределению регрессоров (зависимая переменная тут не участвует).
Это делается стандартным способом, через расстояние Махаланобиса в пространстве независимых
признаков (регрессоров): если $x_{i}$ --- $i$-й индивид, $\bar{\mathbf{x}}$ ---
вектор средних, то аутлаером можно назвать индивида, для которого
велико
\[
r_{M}^{2}(x_{i},\bar{\mathbf{x}};\S_{\mathbf{x}\mathbf{x}}).
\]

Правда, тут мы незаметно перешли к пониманию матрицы $\X$ как выборки из многомерного распределения.
Если $\X$ --- детерминированная матрица, то в этом смысле выбросов быть не может (так как нет закономерности).

\begin{rem*}
Если индивид аутлаер по Махаланобису, то $\S_{\mathbf{x}\mathbf{x}}$ оценивается неправильно (если понимать ее как выборочную ковариационную матрицу) и все значимости/доверительные интервалы становятся неправильными.
\end{rem*}
\begin{center}
\begin{tabular}{>{\centering}m{0.33\textwidth}>{\centering}m{0.33\textwidth}>{\centering}m{0.25\textwidth}}
\toprule
 & Аутлаер по Куку & Не аутлаер по Куку\tabularnewline
\midrule
Аутлаер по Махаланобису & Далеко от линии регрессии, далеко от  $\bar{\mathbf{x}}$& Далеко от $\bar{\mathbf{x}}$, на линии регрессии\tabularnewline
\midrule
Не аутлаер по Махаланобису & Далеко от линии регрессии, недалеко от  $\bar{\mathbf{x}}$ & Недалеко от $\bar{\mathbf{x}}$, на линии регрессии\tabularnewline
\bottomrule
\end{tabular}
\par\end{center}


\section{Проверка правильности и выбор модели}

\begin{comment}
Если остатки независимы и нормально распределены, то статистики точные
(иное не критично при большом объеме выборки).
\end{comment}

\begin{itemize}
\item Если известно, что ошибки нормально распределены (например, в случае
измерений прибора), то если остатки не имеют нормального распределения,
то модель не является правильной.\\
Вообще, на нормальность остатков имеет смысл смотреть, если верно то, что написано ваше, но это особый специфичный случай, а также (более общий вариант) --- для того, чтобы понимать, как относиться к результатам критериев (точные они или асимптотические). Поэтому всегда имеет смысл посмотреть на гистограмму остатков и/или их normal probability plot.
\item Если исходные данные имеют нелинейную зависимость, то и расположение
остатков по линейной регрессии на графике будет отражать характер
этой зависимости.
Имеет смысл рассмотреть график, где по оси X откладывают предсказанные значения predicted $\hat{y}_i$, а по оси Y --- остатки residuals $r_i = y_i-\hat{y}_i$.
Обращаю внимание, что остатки всегда (!) ортогональны предсказанным значениям, по построению.
Поэтому по наклону линии парной регрессии на графике residuals vs predicted невозможно определить, правильная ли была модель регрессии.
Однако, по облаку точек это можно сделать, так как если зависимость
нелинейная, то в $\epsilon$ войдет кусочек $\xi$ и независимости residuals и predicted
не будет.
\\
\textbf{Упражнение} Нарисуйте, как будет выглядеть это график residuals vs predicted, если регрессия квадратичная типа $y = x^2$, $x>0$.

Замечание. Есть только один вариант, когда линия парной регрессии на графике residuals vs predicted может быть не горизонтальной --- когда выборка неправильная, что получается в случае пропущенных наблюдений и варианте pairwise.
\begin{comment}
Картиночка с разницей линии регрессии и квадратичных данных
\end{comment}

%\item Модель с наименьшим количеством параметров при прочих равных является
%предпочтительной, поэтому если заранее известно, $b_0$ по смыслу равно нулю (если на вход поступают нулевые значения, то и на выходе должен быть ноль), то
%свободный член из модели лучше положить равным нулю и его не оценивать.
\end{itemize}
%\begin{comment}
%???
%\end{comment}

\newpage
\section{Доверительные интервалы для среднего предсказания и предсказательные интервалы}

Предваряя следующие рассуждения, сразу скажу, что первое в названии строится на основе $\SE$, а второе --- на основе $\SD$. Смысл тот же.

Пусть $\x = \left(1,\z\right)^{\T}\in\RR^{k+1}$; тогда среднее предсказание (мат.ож., mean prediction) в модели $y = \x^\T \bb + \varepsilon$
имеет вид
\[
\bar{y}=\begin{pmatrix}1\\
\z
\end{pmatrix}^{\T}\bb,
\]
а его оценка ---
\[
\hat{\y}=\mbox{\ensuremath{\begin{pmatrix}1\\
 \z
\end{pmatrix}}}^{\T}\hat{\bb}.
\]
 Эта оценка несмещенная, $\E\hat{y}=\bar{y}$. Несложно увидеть, что
её дисперсия есть
\[
\D\hat{y}=\sigma^{2}\begin{pmatrix}1\\
\z
\end{pmatrix}^{\T}(\X^{\T}\X)^{-1}\begin{pmatrix}1\\
\z
\end{pmatrix}.
\]

Так как у матрицы $\X$ первый столбец состоит из единиц, технические выкладки дают более интерпретируемое выражение
\[
\D\hat{y} = \frac{\sigma^{2}}{n}+\frac{\sigma^{2}}{n}(\z-\bar{{z}})^{\T}\S_{\x\x}^{-1}(\z-\bar{{z}}),
\]
частный случай чего выписывался в случае парной регрессии как
\[
\D\hat{y}=\frac{\sigma^{2}}{n}+\frac{\sigma^{2}(z-\bar{{z}})^2}{\sum_{i=1}^{n}(z_{i}-\bar{{z}})^2}.
\]

Доверительным интервалом для $\bar{y}$ будет
\[
\left(\hat{y}\pm c_{\gamma}\SE\right)=\left(\bar{y}\pm c_{\gamma}\sqrt{\widehat{\D\hat{y}}}\right)=\left(\hat{y}\pm c_{\gamma}\hat{\sigma}\sqrt{\begin{pmatrix}1\\
\z
\end{pmatrix}^{\T}(\X^{\T}\X)^{-1}\begin{pmatrix}1\\
\z
\end{pmatrix}}\right),
\]
где $c_{\gamma}$ находится из распределения $\t(n-m)$ стандартным способом.
%В данной модели $y=\X\bb+\varepsilon$ --- значение вообще, а $\X\bb$ --- среднее.
Если же мы хотит предсказать не среднее значение, а построить диапазон значений, который может быть, т.е.
предсказательный интервал (prediction interval), то получим, добавляя дисперсию $\varepsilon$:
\[
\left(\hat{\y}\pm c_{\gamma}\hat{\sigma}\sqrt{1+\frac{1}{n}+
\frac{1}{n}(\z-\bar{{z}})^{\T}\S_{\x\x}^{-1}(\z-\bar{{z}})}\right).
\]



\section{Сведение нелинейной модели к линейной}

Тут мы смешаем случайные и неслучайные регрессоры. Если регрессоры случайны, то стандартно под регрессией понимаем условное математическое ожидание.

Существует три базовых модели, в которых функция регрессии линейная:
\begin{enumerate}
\item $\eta,\xi_{1},\ldots,\xi_{k}$ имеют нормальное распределение. %
\begin{comment}
TODO: дописать пруф по Ермакову--64
\end{comment}

\item $\y=\X\bb+\eb$, $\E\eb=\mathbf{0}$, $\cov\eb=\sigma^{2}\Ib$. Если регрессоры случайные, то модель имеет вид измерений со случайными ошибками: $\eta = \xib^\T \bb + \epsilon$, где $\xib$ и $\epsilon$ независимы.
\item одномерный случай $\eta = \phi(\xi)+\epsilon$, $\xi$ принимает всего два значения (возможно,
как качественный признак).
\end{enumerate}

Теперь рассмотрим случаи, когда регрессию можно свести к линейной.
Пусть
\[
\eta=\phi(\xi_{1},\ldots,\xi_{k})+\epsilon
\]
и $\phi$ --- нелинейная функция.
\begin{itemize}
\item $\phi$ --- многочлен. Можно свести к линейной, добавляя признаки
$\xi,\xi^{2},\ldots$ и для этих признаков строить модель (для неслучайных регрессоров аналогично).\\%
Опасность: может появиться сильная зависимость построенных регрессоров.

\item $\xi$ --- качественный признак, $A_{1},\ldots,A_{k}$ --- его градации. Можно ввести $k-1$ штук\footnote{При добавлении вектора из единиц к $k$ признакам получается вырожденная
матрица.} фиктивных признаков со значениями $\left\{ 0,1\right\} $, где $1$ стоит на месте $A_i$, $i=1,\ldots,k-1$, и для них
строить модель.

\end{itemize}


\chapter{Модификации линейной регрессии.}
\section{Взвешенная регрессия (Weighted Least Squares)}

Пусть $\W$ --- симметричная, положительно определенная матрица, тогда
\[
\hat{\bb}_{W}=(\X^{\T}\W\X)^{-1}\X^{\T}\W\y
\]
 есть <<взвешенная>> оценка. При $\W=\Ib$, $\hat{\bb}_{W}=\hat{\bb}$,
конечно.

Если $\E\eb=\mathbf{0}$, то
\[
\E\hat{\bb}_{W}=(\X^{\T}\W\X)^{-1}\X^{\T}\W\E\y=(\X^{\T}\W\X)^{-1}\X^{\T}\W\X\bb=\bb
\]
 и оценка несмещенная.
\begin{itemize}
\item Если $\cov\eb=\sigma^{2}\I$, то $\hat{\bb}$ --- BLUE и $\hat{\bb}_{W}$
уже не лучшая.
\item Если $\cov\eb=\C$ (то есть шум не белый) то нужно подобрать $\W$
такую, что $\hat{\bb}_{W}$ --- BLUE. Это делается операцией отбеливания:
пусть всё центрированное; тогда $\C^{-1/2}\eb$ --- центрированный
и нормированный белый шум, и
\[
\underbrace{\C^{-1/2}\y}_{\tilde{\y}}=\underbrace{\C^{-1/2}\X}_{\tilde{\X}}\bb+\C^{-1/2}\eb
\]
откуда
\[
\cov(\C^{-1/2}\y)=\left(\C^{-1/2}\right)^{\T}\cov(\X\bb+\eb)\C^{-1/2}=\Ib.
\]
Так как теперь шум белый, следующая оценка будет BLUE:
\begin{eqnarray*}
\hat{\tilde{\bb}} & = & \tilde{\X}^{-}\tilde{\y}=((\C^{-1/2}\X)^{\T}(\C^{-1/2}\X))^{-1}\left(\C^{-1/2}\X\right)^{\T}\C^{-1/2}\y\\
 & = & (\X^{\T}\left(\C^{-1/2}\right)^{\T}\C^{-1/2}\X)^{-1}\X^{\T}\left(\C^{-1/2}\right)^{\T}\C^{-1/2}\y
\end{eqnarray*}
Значит, следует положить $\W=\C^{-1}$.


Для $\W$ итеративный процесс: берем начальное значение, находим коэффициент,
оцениваем $\C$ и т.д.

\end{itemize}
\begin{example*}
Стандартный случай --- измерения с разной точностью, откуда
\[
\C=\begin{pmatrix}\sigma_{1}^{2} &  & \mathbf{0}\\
 & \ddots\\
\mathbf{0} &  & \sigma_{n}^{2}
\end{pmatrix}.
\]
Наблюдениям, таким образом, придается разный вес --- чем меньше точность
наблюдения, тем больше $\sigma_{i}^{2}$ и меньший, соответственно,
вес.\end{example*}
\begin{rem*}
$\W$ можно также назначить и руками.
\end{rem*}

\section{Гребневая (Ridge) регрессия}

Чтобы бороться с вырожденностью $\R_{\x\x}$ в оценке $\hat{\boldsymbol{\b}}=\R_{\x\x}^{-1}\R_{\x\y}$
рассматривают
\[
\hat{\boldsymbol{\beta}}=(\R_{\x\x}+\lambda\Ib)^{-1}\R_{\x\y}.
\]
 Получается смещенная оценка, но с меньшей дисперсией, что может привести к уменьшению MSE (например, если регрессоры линейно зависимы, то точно будет лучше, так как конечная дисперсия меньше бесконечной). Напомним, что MSE равно сумме дисперсии и квадрата смещения. Для поиска
$\lambda$ используют кросс-валидацию.

Эта процедура эквивалентна тому, как если бы мы к регрессорам (после стандартизации) добавили искусственный шум с дисперсией $\lambda$.

%\end{document}
\part{Материалы для курса `Вероятностные и статистические модели' (магистры 1 курса)}

\chapter{Робастные оценки, критерии, ...}
Робастными (robust) оценками и критериями называются те, которые слабо зависят от предположений о виде распределения и/или выбросов (outliers). Поэтому, вообще говоря, нужно уточнять, относительно чего роюастность рассматривается. Часто, по умолчанию, предполагается робастность по отношению к выбросам.

Очевидные робастные кандидаты --- оценки/критерии, основанные на рангах, когда вместо значений подставляются ранги индивидов (номера по порядку в упорядоченной выборке, ранг 1 у минимального значения, ранг $n$ у максимального). Так как ранги не меняются при монотонном преобразовании, такие оценки/критерии вообще не зависят от вида распределения (и называются непараметрическими). Также, так как выброс меняет каждый ранг не больше, чем на 1, то характеристики, основанные на рангах, устойчивы к выбросам.

Другой класс оценок/критериев, который относительно устойчив по отношению к виду распределения, это те, которые основаны на центральной предельной теореме (ЦПТ). Конечно, устойчивость зависит от распределения (от скорости сходимости в ЦПТ для случайных величин с таким распределением). Самый общий пример --- t-test для проверки гипотезы о математическом ожидании на основе выборочного среднего.
Если случайная величины не имеет нормальное распределение, распределение стандартизованного выборочного среднего по ЦПТ все равно сходится к нормальному распределению.

Отдельное замечание по поводу того, что такое выброс. Если это просто ошибка в данных (неправильно набрано число, к примеру), то проблем нет, это значение не рассматривается или заменяется на пропуск или заменяется на среднее по признаку. Но, вообще, к выбросам нужно относиться внимательно. Например, вы изучаете состояние организма человек и делаете о нем какие-то выводы. При анализе обнаружился выброс, его удалили. Получили выводы, применили к женщине (например, чтобы предсказать, здорова она или нет), и ошиблись (!). Оказалось, что удаленный выброс был женщиной, а остальные были мужчинами. Соответственно, метод годился только для мужчин, а был применен к женщине.
Этот пример случая, когда выбросы --- не ошибка, а просто данные из другого распределения (другой закономерности). Если бы таких точек было много, мы бы распознали неоднородность данных, а так просто выбросили и не заметили, что это точки из другой закономерности.

Также, не бывает просто выброса (outlier), нужно обязательно сказать, по отношению к чему, к какой закономерности, это выброс.

\section{Непараметрические оценки и критерии}

\subsection{Оценки}
Например, вместо выборочного среднего рассматривать выборочную медиану. Получаем более устойчивую оценку (не реагирует на выброс, не реагирует на монот.преобразование). Простейший пример (10 3 6 4 7) и (10 3 6 4 777). Но (!) того ли это оценка? Если интересует мат.ож., но распределение несимметричное. Тогда не того. А если интересует характеристика положения у логнормального распределения – то ровно того, что нужно (а выб.среднее – как раз не того, что нужно). За устойчивость оценки чаще всего платят точностью. Как видели на прошлом занятии, даже не всегда. ($\pi/2$ для $n$, но потом извлечь корень).

\subsection{Критерии}
Проверяем гипотезу, что два распределения равны.

t-Test, тест Манна-Уитни (Mann-Whitney or Wilcoxon, or тест суммы рангов). Та же история – вопрос, то же самое ли проверяют. (Переформулируем – мощный ли против той альтернативы, которая нам важна.) Здесь за устойчивость можем заплатить мощностью. 5\% для норм.распр. с одинаковой дисп.

\subsection{Корреляции}
Обычный коэффициент корреляции Пирсона и ранговый Спирмена.	Разное-одинаковое? Если двумерное распределение нормальное, то $\rho = 2 \sin (\pi \rho_S/6)$, т.е. коэффициенты примерно равны (синус вблизи нуля хорошо приближается линейной функцией $y=x$. А в других случаях даже если зависимость (условное мат.ож.) линейная, коэффициенты не обязаны быть равны даже примерно.

\section{$M$-оценки}

\textbf{$M$-оценки} являются обобщением оценок максимального правдоподобия (ОМП, MLE) в том смысле, что оценка ищется путем максимизации некоторого функционала, а точнее, ищется экстремум некоторой суммы по наблюдениям:\\
$\sum_{i=1}^n d(x_i, \theta) \rightarrow \min_{\theta}$.

\medskip
В случае ОМП это ...

В случае оценок по методу наименьших квадратов это ...

В случае оценок по методу наименьших абсолютных отклонений это ...\\

%\newpage
ОМП: $\sum_{i=1}^n (-\ln p_\xi(x_i;\theta)) $\\

МНК: $\sum_{i=1}^n (y_i - \phi(\x_i;\theta))^2 $ ($\phi$ --- модель).

\medskip
\paragraph{Мат.ож.}Рассмотрим два последних случая: обсуждали, что в случае МНК решение --- это среднее арифметическое, а в случае абсолютных отклонений - это выборочная медиана.

Поэтому методы, основанные на $d(x_i, \theta) = (x_i - \theta)^2$, не являются робастными, а методы, основанные на $d(x_i, \theta) = |x_i - \theta|$, являются робастными.

В случае нормального распределения, рассматривая робастные оценки, теряем в точности.

Идея --- сделать оценки, где сочетается точность одного и устойчивость другого (для симметричного распределения). При этом предполагается, что выбросы тоже расположены симметрично. Например, исходная модель $N(a,1)$, модель выбросов $N(a,10)$, данные имеют вид смеси $0.95\cdot N(a,1) + 0.05 \cdot N(a,10)$ или $0.95\cdot N(a,1) + 0.05 \cdot Lapl(a,10)$.

\medskip
Пусть $d(x_i, \theta) = f(x_i - \theta)$.

Huber's оценки:\\
$f(x) = \begin{cases}
0.5 x^2 \text{\ if\ } |x|<\delta\\
\delta(|x| - 0.5\delta) \text{\ otherwise}.
\end{cases}$

\includegraphics[height=70mm]{Huber_loss}

\medskip
Для оценивания мат.ож. симметричного распределения используются еще варианты trimmed или windsorized. trimmed --- это когда какое-то количество крайних значений (например, больше трех сигм) удаляется перевычислением среднего арифметического,  windsorized --- когда соотв.значения не удаляются, а устанавливаются равными крайним неудаленным.

Задание --- какой функции $f$ это соответствует?

Задать порог --- непросто, так как нужно оценивать дисперсию для стандартизацию. Часто особые правила (например, trimmed) применяют к заданному проценту крайних точек.

\bigskip
\paragraph{Регрессия}Есть и $M$-оценки для параметров регрессии, те же идеи.

Взвешенная регрессия, когда уменьшается вклад далеких элементов.
В задачах типа регрессии, выброс часто соотносят с ошибкой в регрессии ($y_i = a x_i + b +e_i$), т.е. все ошибки подчиняются $N(a, s^2)$, а выброс нет (модель выброса --– у него больше дисперсия). Тогда, как мы обсуждали (говоря про ), можно выбросам дать меньший вес. Как это определить? Итеративная процедура. iteratively reweighted least-squares, IRLS

\bigskip
Общая идея для построения робастных оценок --- замена в оптимизационных (!) задачах $L_2$-нормы на $L_1$-норму.
\emph{Не путайте с $L_1$- и $L_2$-регуляризацией.}

\section{Не про робастность (!), но про увеличение точности оценки}
\subsection{Variance-bias trade-off}

\begin{defn*}
\emph{Среднеквадратичная ошибка}
(mean squared error, MSE)
есть
\[
\MSE(\hat{\theta}_{n}):=\E(\hat{\th}_{n}-\th)^{2}.
\]
\end{defn*}
\begin{rem*}
Поскольку
\[
\D\hat{\th}_{n}=\D(\hat{\th}_{n}-\th)=\E(\hat{\th}_{n}-\th)^{2}-(\E(\hat{\th}_{n}-\th))^{2},
\]
то
\begin{equation}
\label{eq:MSE}
\underbrace{\E(\hat{\th}_{n}-\th)^{2}}_{\mathrm{MSE}}=\D\hat{\th}_{n}+\underbrace{(\E(\hat{\th}_{n}-\th))^{2}}_{\mathrm{bias}^{2}}.
\end{equation}
\end{rem*}

\subsection{$L_1$- и $L_2$-регуляризация}

Модель регрессии:\\
$y_i = \phi(\x_i; \theta) + \varepsilon_i$, $\E\varepsilon =0$, $\D\varepsilon =\sigma^2$.

\medskip
MНК: $\sum_{i=1}^n (y_i - \phi(\x_i; \theta))^2 \rightarrow \min_\theta$.

Для линейной регрессии (функция $\phi$ линейная функция) известно, то оценки параметров регрессии по методу наименьших квадратов являются BLUE --- best linear unbiased estimate.

Идея: разрешить оценке иметь смещение, но уменьшить MSE.

\includegraphics[height=70mm]{trade-off.jpg}

Испортим оптимизируемую функцию:\\
$\sum_{i=1}^n (y_i - \phi(\x_i; \theta))^2 + \lambda f(\theta) \rightarrow \min_\theta$, $\lambda\ge 0$.

\medskip
$f(\t) = \sum_i t_i^2$ ---  ridge regression;

$f(\t) = \sum_i |t_i|$ ---  Lasso regression.

Почему работает:\\
$\|\hat\theta\|^2 = \D\hat\theta + \|\theta\|^2$.\\

Чем больше портим (больше $\lambda$), тем меньше дисперсия оценки получается, но больше смещение.
И, наоборот, при нулевом $\lambda$ смещение нулевое, но дисперсия побольше.

\medskip
Хорошая ссылка:\\
\url{https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net}


\chapter{Доверительные интервалы}

\section{Мотивация и определение доверительных интервалов}

Точечные оценки не дают информации о том, насколько (количественно) настоящее значение далеко от оценки.

\begin{defn*}
$\left[b_{1},b_{2}\right]$ --- \emph{доверительный интервал} для
параметра $\theta$ с уровнем доверия $\gamma\in\left[0,1\right]$,
если $\forall\th$
\[
\P(\theta\in\left[b_{1},b_{2}\right])=\gamma,\quad\text{где }b_{1}=b_{1}(\mathbf{x}),b_{2}=b_{2}(\mathbf{x}),
\]
т.е. границы доверительного интервала --- это статистики (функции от выборки, случайные величины <<до эксперимента>>).
\end{defn*}

\begin{rem*}
Если выборка из дискретного распределения, то $b_{1},b_{2}$ --- тоже дискретны.
Поэтому наперед заданную точность получить может не получиться; в
таких случаях знак <<$=$>> заменяют <<$\geq$>>. Аналогично с
заменой на <<$\xrightarrow[n\to\infty]{}$>> для асимптотических доверительных интервалов, когда точные получить невозможно или трудоемко.
\end{rem*}

\section{Доверительный интервал для проверки гипотезы о значении параметра}

Зафиксируем $H_{0}:\th=\th_{0}$ и $\gamma=1-\alpha$, где $\a$ играет
роль уровня значимости. По определению доверительного интервала, $\P(\theta\in\left[a_{\gamma}(\mathbf{x}),b_{\gamma}(\mathbf{x})\right])=\gamma.$
Тогда
\[
\P(\theta\in\left[b_1(\mathbf{x}),b_2(\mathbf{x})\right])=
\gamma=1-\a \implies\a=1-\P(\theta\in\left[a_{\gamma}(\mathbf{x}),b_{\gamma}(\mathbf{x})\right])=
\P(\th\not\in\left[a_{\gamma}(\mathbf{x}),b_{\gamma}(\mathbf{x})\right]).
\]
Соответственно,
\[
\begin{cases}
\text{отвергаем\ } H_0 \text{, если\ }& \th_0\not\in\left[b_1(\mathbf{x}),b_2(\mathbf{x})\right]\\
\text{не отвергаем\ } H_0 \text{, если\ }& \th_0\in\left[b_1(\mathbf{x}),b_2(\mathbf{x})\right].
\end{cases}
\]
 Вероятность ошибки первого рода равна $\a$, что соответствует определению критерия.
 Заметим, что здесь мы пользуемся общим определением критерия (критическая область --- область значений выборки, вероятность которой равна $\a$), а не частным случаем, когда критерий строится через статистику критерия.

 \section{Асимптотический доверительный интервал для математического ожидания
в модели с конечной дисперсией}

Если модель неизвестна, но известно, что $\D\xi<\infty$, можно построить
доверительный интервал для $\E\xi={a}$, не задавая параметрическую модель.
Пусть $\left\{ x_{i}\right\} $
$\iid$, тогда
\[
t=\frac{\sqrt{n}\left(\bar{x}-{a}\right)}{\sigma}\xrightarrow[n\to\infty]{}\N(0,1).
\]
 Если заменить $\sigma$ на ее состоятельную оценку ($s$), то по модифицированной теореме Леви  сходимость не испортится. Тогда
\[
\P\left(a\in\left(\bar{x}\pm c_{\gamma}\frac{s}{\sqrt{n}}\right)\right)\xrightarrow[n\to\infty]{}\gamma,\quad c_{\gamma}=\cdf_{\N(0,1)}^{-1}\left(\frac{1+\gamma}{2}\right).
\]

Чтобы в случае, когда модель нормальная, доверительный интервал становился точным, рассматривают его модификацию:
\[
\P\left(\E\xi\in\left(\bar{x}\pm c_{\gamma}\frac{s}{\sqrt{n-1}}\right)\right)\xrightarrow[n\to\infty]{}\gamma,\quad c_{\gamma}=\cdf_{\t(n-1)}^{-1}\left(\frac{1+\gamma}{2}\right).
\]

\section{Доверительные интервалы для пропорций}
Модель для распределения $\xi$ --- испытания Бернулли с вероятностью успеха $p$ ($\E\xi = p$, $\D\xi = p(1-p)$). Задача --- оценить $p$.
Так как $p=\E\xi$, то ответ очевиден: $\hat{p} = \bar{x}$. То же самое получается, если искать ОМП.

Что такое $\bar{x}$, если выборка состоит из результатов испытаний Бернулли?

Итак, нас интересует процент успехов.

\begin{rem*}
Хотя речь о модели Бернулли и повторной независимой выборке объема $n$, часто говорят, что данные подчиняются биномиальной модели. В этом случае у нас модель $Bin(n,p)$ и всего одно испытание (выборка объема 1) из числа успехов. Если разделим на $n$, будет доля успехов.
\end{rem*}

Если с оценкой все очевидно, то с доверительными интервалами не так очевидно.

Первая идея --- воспользоваться асимптотическим дов. интервалом для мат.ож.

\textbf{Задание}: Построить доверительный интервал для $p$ с помощью аналогичной идеи, как для математического ожидания.

%\newpage
\[
\P\left(p\in\left(\hat{p}\pm c_{\gamma}\frac{\sqrt{\hat{p}(1-\hat{p})}}{\sqrt{n}}\right)\right)\xrightarrow[n\to\infty]{}\gamma,\quad c_{\gamma}=\cdf_{\N(0,1)}^{-1}\left(\frac{1+\gamma}{2}\right).
\]

Разница с общим случаем:
\begin{itemize}
\item
можно получить более точную оценку дисперсии, так как всего один параметр ($\D\xi = p(1-p)$), и тем самым улучшить сходимость (asymptotic c.i.).
\item можно получить более точный (все еще асимптотический) несимметричный доверительный интервал (он будет всегда внутри $[0,1]$), называется Wilson's confidence interval for propotions.
\item можно получить точный доверительный интервал (но не для всех доверительных уровней) (exact).
\end{itemize}

\url{https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval}

Wilson's confidence interval for proportions:
$$
  \left(\frac{1}{~1+\frac{\,c_{\gamma}^2\,}{n}~}\left( \hat p+\frac{\,c_{\gamma}^2\,}{2n} \right)
~ \pm ~
\frac{c_{\gamma}}{~1+\frac{c_{\gamma}^2}{n}~}\sqrt{\frac{\,\hat p(1-\hat p)\,}{n}+\frac{\,c_{\gamma}^2\,}{4n^2}~} \right) ~
$$
or the equivalent
$$
  \left(\frac{~ n_S + \tfrac{1}{2} c_{\gamma}^2 ~}{ n + c_{\gamma}^2 }
~ \pm ~ \frac{c_{\gamma}}{n + c_{\gamma}^2}
\sqrt{
  \frac{~n_S \, n_F~}{n} + \frac{c_{\gamma}^2}{4} ~
  }\right)~,
$$
где  $n_S$ --- число успехов в выборке, $n_F$ --- число неудач.

Построение точного доверительного интервала основано на том, что распределение
$n\bar{x}$ известно. Какое оно?

%\newpage
Это биномиальное распределение с мат.ож. $np$. Находим границы: $P(np - c_1 < n\bar{x} < np + c_2) \ge \gamma$ (интервал покороче), отсюда получаем интервал для $p$. Распределение дискретное, поэтому точные границы не найти.\\

\textbf{Примеры}:\\

(1) Интуитивно, сколько раз может выпасть орел на правильной монетке из 10 бросаний с вер. 0.95? Из 100?\\
Если $p \approx 0.5$, то $\sqrt{p(1-p)}\approx 0.5$. Получаем примерный доверительный интервал
$(\hat{p}\pm 0.5 c_\gamma/\sqrt{n})$, а для 95\% доверительного интервала примерно $(\hat{p}\pm 1/\sqrt{n})$.\\

(2) Про американскую вакцину было сказано, что доля случаев, когда она помогает --- 95\%. А про другие --- 90 и 92\%. Позволяет ли точность эксперимента говорить, что какая-то из вакцин лучше? Там было 95 заболевших, из них 90 из группы с плацебо и 5 из группы с вакциной, группы равные. (Если группы плацебо-вакцина делятся 1 к $k$, обычно плацебо дают меньшей по размеру группе, то для случаев, когда помогает, оценивается как \\ $(\text{больные среди плацебо} * k)/(\text{больные среди плацебо} *k + \text{больные c вакциной} )$. Например, $=16*3/(16*3+4)\approx 0.92$, т.е. у нас из 20 человек 4 человека заболели после вакцины. \\

(3) Каков доверительный интервал, если получили ноль успехов? Правило трех: 95\% доверительный интервал имеет примерный вид $[0, 3/n]$ (при больших $n$, $3\approx-\ln 0.05$) --- здесь имеется в виду односторонний доверительный интервал, так как с одной стороны возможные значения ограничены нулем. Был пример --- 10 человек перешли по мостику и не упали. Кажется, что переход по мостику безопасный. Но (!) с вероятностью 0.95 вероятность упасть может иметь значение из $[0,0.3]$, т.е. может иметь значение вплоть до 0.3, что немало.\\

(4) Подряд 10 дней случились практически одинаковые значения числа успехов (не долей, а именно числа). Так могло быть? Построим доверительный интервал для числа успехов (при маленьком $p$ и большом $n$). Оказывается, он не зависит от $n$ и $p$ по отдельности!

\medskip
Воспользуемся тем, что $p(1-p)\approx p$. Получим:
\[
\P\left(np\in\left(n\hat{p}\pm c_{\gamma}\sqrt{n\hat{p}}\right)\right)\xrightarrow[n\to\infty]{}\gamma,\quad c_{\gamma}=\cdf_{\N(0,1)}^{-1}\left(\frac{1+\gamma}{2}\right),
\]
здесь $np$ --- теоретическое число успехов, а $n\hat{p}$ --- эмпирическое число успехов.

Если построить, к примеру, 50\% доверительный интервал, то вероятность того, что 10 дней подряд число успехов будет лежать в нем, равно $0.5^{10}\approx 0.001$ (а если внутри 20-процентного, то $10^{-7}$).

%\pagebreak{}

\chapter{Множественные тесты}

%\textbf{Задание}. Напишите мне в vk, как вы поняли комикс с точки зрения проверки гипотез.

\medskip
Когда мы в тестах говорим об ошибке первого рода (ложно отвергнуть $H_0$), то мы выбираем эту ошибку так, как будто в единичном эксперименте эта ошибка не может произойти (примерно так можно представлять себе допустимую ошибку).

Однако, если наши эксперименты повторяются, то это не так.
Если проводить эксперименты долго, то ошибка произойдет неминуемо.
Пусть $\alpha_1$ --- ошибка в одном эксперименте.
Нас интересует так называемая групповая ошибка (FWER, family-wise error rate) $$\text{FWER} = \alpha_m = \P(\text{хотя бы 1 раз из\ } m \text{\ экспериментов произойдет ошибка}).$$

\section{Независимые тесты}

Пусть у нас тесты проводятся независимо (как с леденцами). (Чему там равно $m$?)

%\textbf{Задание}
Давайте получим формулу для FWER в случае, когда все гипотезы справедливы. Это упражнение по теорверу. Решается довольно просто --- нас интересует событие, дополнение к которому `событие не произойдет ни разу'.

%\vspace*{5cm}
$FWER = \alpha_m = 1 - (1-\alpha_1)^m$. При $m\rightarrow \infty$ FWER стремится к 1.\\

\begin{tabular}{|c|c|c|c|c|}
\hline
1&	2&	5&	10&  20\\
\hline
$\alpha_1$&	$\alpha_2$&	$\alpha_5$&	$\alpha_{10}$&	$\alpha_{20}$\\
\hline
0.01&	0.02&	0.05&	0.10&  0.20\\
0.05&	0.10&	0.23&	0.40&  0.64\\
0.10&	0.19&	0.41&	0.65&  0.88\\
\hline
\end{tabular}

\bigskip
Но мы хотим ограничивать именно групповую ошибку. Что делать?\\

Идея --- поменять $\alpha_1$, т.е., уровень значимости для единичного эксперимента.

\medskip
%\textbf{Задание}
Напишем формулу, как задать $\alpha_1$, чтобы получить $FWER = \alpha$.

%\newpage
Очевидно, ответ:

$$FWER = 1 - (1-\alpha_1)^m = \alpha.$$

Следовательно,
$$\alpha_1 = 1 - \sqrt[m]{1-\alpha}.$$

\medskip
Мало кто любит извлекать корень степени $m$, особенно это было сложно раньше, когда вычисления делались вручную.

\medskip
Есть выход.
Для малых $x$ приближение для корня такое: $\sqrt[m]{1-x} \approx 1-x/m$. Тогда $\alpha_1 \approx \alpha/m$.

\medskip
\fbox{
\parbox{12cm}{
Поправка уровня значимости \v{S}idak'а: $\alpha_1 = 1 - \sqrt[m]{1-\alpha}$.

Поправка уровня значимости Бонферрони (Bonferroni): $\alpha_1 = \alpha/m$.}}

\medskip
Однако неудобно менять уровень значимости. Гораздо удобнее менять $p$-value.

Одна гипотеза отвергается, если $p < \alpha_1$. \\
%\textbf{Задание}
Получим эквивалентное неравенство $... < \alpha$.

%\newpage
%\vspace*{5cm}
\fbox{
\parbox{12cm}{
Поправка $p$-value \v{S}idak'а: $1 - (1-p)^m$ вместо $p$.

Поправка $p$-value Бонферрони (Bonferroni): $m p$ вместо $p$.}}

\medskip
Поправка Бонферрони очень удобна в использовании. По-прежнему, если 'поправленный p-value' в гипотезе меньше заданного уровня значимости $\a$, эта гипотеза отвергается.

\section{Зависимые тесты, общий случай}
Докажем, что поправка Бонферрони для $p$-value в каждом отдельном тесте приводит к тому, что групповая ошибка ограничена $\alpha$.

Каждая $H^{(i)}$ проверяется отдельно с уровнем значимости $\a_{1}$.
Задача сводится к тому, чтобы найти такое $\a_{1}$, что $\FWER\leq\a$
для выбранного группового уровня значимости $\a$. Имеем:
\[
\FWER=\P\left(\bigcup_{i=1}^{m}\{H_{0}^{(i)}\text{ отв}\}\right)\leq\sum_{i=1}^{m}\P(H^{(i)}\text{ отв})=m\a_{1}=\a\implies\a_{1}:=\frac{\a}{m}.
\]

Таким образом, деля уровень значимости на число тестов или, наоборот, умножая p-value на число тестов, мы получаем консервативный тест.

\begin{rem*}
Из-за неравенства тест консервативный, т.е. может быть, что $\FWER\ll\a$. Значит, тест может быть маломощным.
\end{rem*}

\textbf{Вопрос}. В каком случае поправка Бонферрони приведет к максимально консервативному тесту?
Если все тесты полностью зависимы (выдают одинаковые $p$-values).

\begin{rem*}
Есть множественный тест с гарантированной групповой ошибкой, у которого мощность больше, чем у теста с поправками Боферрони. Это тест Хольма (Holm).  Увеличение мощности удается получить за счет того, что гипотезы проверяются не параллельно, а в определенном порядке, причем поправка на каждом шаге разная.
\end{rem*}

\section{Множественное тестирование переходом к многомерному случаю}

\subsection{Одна группа, много признаков}

До сих пор мы обсуждали, как проверить гипотезу, что $\E\xi = a_0$. Для этого использовался t-test в случае, если предполагали, что $\xi$ имеет нормальное распределение, и его асимптотические вариант в общем случае.
Статистика критерия
\[
t=\frac{\sqrt{n-1}\left(\bar{x}-{a_0}\right)}{s}\xrightarrow[n\to\infty]{}\N(0,1).
\]

Доверительный интервал:
\[
\P\left(\E\xi\in\left(\bar{x}\pm c_{\gamma}\frac{s}{\sqrt{n-1}}\right)\right)\xrightarrow[n\to\infty]{}\gamma,\quad c_{\gamma}=\cdf_{\t(n-1)}^{-1}\left(\frac{1+\gamma}{2}\right).
\]

\bigskip
Предположим, что нам нужно проверить одновременно равенство каким-то значениям (например, нулю) сразу нескольких математических ожиданий для зависимых признаков.
И пусть нас интересует именно групповая ошибка.

\medskip
Есть обобщение t-test, который называется тестом Хотеллинга для гипотезы:
$$H_0: \left(
\begin{array}{c}
\E\xi_1\\
\E\xi_2\\
\ldots\\
\E\xi_p
\end{array}
\right)= \mathbf{a}_0.
$$
статистика имеет вид
\begin{equation*}
 T^2 = (\bar{\x} - \mathbf{a}_0)^{\mathrm{T}} \Big (\frac{\tilde{\mathbb{S}}}{n})^{-1} (\bar{\x} - \mathbf{a}_0) \xrightarrow[n
\rightarrow \infty]{\sim} \chi^2 (p),
\end{equation*}
%
где $\bar{\x}$ ---  вектор из выборочных средних, $\mathbb{S}$ --- выборочная ковариационные матрицы.

\textbf{Задание} Где критическая область?

\subsubsection{Доверительные интервалы/области}
Этот подход распространяется на проверку гипотез о параметрах, например,
$$H_0: \left(
\begin{array}{c}
\theta_1\\
\theta_2\\
\end{array}
\right)= \left(\begin{array}{c}
\theta_1^{(0)}\\
\theta_2^{(0)}\\
\end{array}\right).
$$

В свою очередь, мы знаем, что проверку гипотез про параметры можно проводить на основе доверительных интервалов, попадает туда значение, предполагаемое в нулевой гипотезе, или нет.

\medskip
Но также у нас речь шла о проблеме с множественным тестирование. Наверняка, как-то эта проблема должна отразиться и на доверительных интервалах (если параметр не одномерный, то говорят о доверительных областях).

\bigskip
Определение доверительной области: $$\P\left(\left(
\begin{array}{c}
\theta_1\\
\theta_2\\
\end{array}
\right)
\in A(\x)\right) = (\text{или} \ge)\ \gamma.
$$


Пусть мы умеем строить доверительный интервал отдельно для $\theta_1$ и отдельно для $\theta_2$.

\[
\P(\theta_i\in\left[b_{\text{low}}^{(i)}(\mathbf{x};\gamma),b_{\text{up}}^{(i)}(\mathbf{x};\gamma)\right])=
\gamma \text{\ для любого\ } \gamma.
\]

\textbf{Задание} Построить доверительную область с уровнем доверия $\gamma$ для вектора из параметров $$\left(
\begin{array}{c}
\theta_1\\
\theta_2\\
\end{array}
\right).
$$
Сначала рассмотрите случай, когда доверительные интервалы независимые (их границы независимые для разных параметров случайные величины). А потом, если получится, попробуйте использовать поправки Бонферрони для общего случая.

\medskip
В общем случае доверительный интервал на основе поправок Бонферрони получается слишком большим (вероятность для параметра попасть туда больше заданного уровня доверия). Опять же, в конкретных случаях можно строить (асимптотически) точные доверительные области. Например, в случае проверки гипотезы о значении вектора из математических ожиданий это будет эллипсоид.

\subsection{Две группы, много признаков}
Предположим, что нам нужно проверить одновременно равенство сразу нескольких математических ожиданий для зависимых признаков.  Т.е. у нас есть две группы и нужно проверить гипотезу о том, что они в среднем не различаются.

Пример: есть две группы, кто занимается физическими упражнениями (например, фитнесом) и кто нет. Признаки --- давление, пульс, вес, мешки под глазами...
Гипотеза: все равно, заниматься или нет.

Пример: есть 2 группы людей. Снимаются показания по росту, длине ног, длине рук и т.д. Необходимо сравнить так называемые ``средние размеры''.

\bigskip
Мы упоминали постановку задачи классификации. Чтобы строить правило, по которому группы различаются, нужно, чтобы они различались (иначе можно оооочень долго строить такое правило).

Здесь нас интересует именно групповая ошибка, так как гипотеза, что все мат.ож. равны, а альтернатива ---  что хотя бы по одному из признаков мат.ож. не равны.

\medskip
Статистика асимптотического варианта критерия в случае $p$ признаков (многомерный аналог $t$-критерия, называется тестом Хотеллинга) получается переходом к гипотезе, что разность значений равна нулю:
%
\begin{equation*}
T^2 = (\bar{\x}^{(1)} - \bar{\x}^{(2)})^{\mathrm{T}} \Big (\frac{\tilde{\mathbb{S}}_1}{n_1} +
\frac{\tilde{\mathbb{S}}_2}{n_2} \Big )^{-1} (\bar{\x}^{(1)} - \bar{\x}^{(2)}) \xrightarrow[n_1, n_2
\rightarrow \infty]{\sim} \chi^2 (p),
\end{equation*}
%
где $\bar{\x}^{(1)}$ и $\bar{\x}^{(1)}$ ---  вектора из выборочных средних в двум группам, $\mathbb{S}_1$ и $\mathbb{S}_2$ --- выборочные ковариационные матрицы.


\begin{rem*}
Есть групп больше двух (пусть их $k$), то у теста если обобщение, которое называется one-way (M)ANOVA, ANalysis Of VAriance). В этом случае, если ковариационные матрицы в группах одинаковые, то можно проверять гипотезу, что вектора из мат.ож. равны для всех $k$ групп.
\end{rem*}

\textbf{Задание} Пусть есть две группы и два признака. Поэтому мы можем выборку нарисовать на плоскости. Нарисуйте два случая (группы разными значками) --- ковариационные матрицы одинаковые и ковариационные матрицы разные.


\section{Post-hoc сравнения, много групп, один признак}

В ANOVA рассматривается модель, когда $\mathcal P(\eta_i) = \mathcal N(\mu_i, \sigma^2)$, $i=1,\ldots,k$.
В рамках этой модели гипотеза о равенстве $k$ распределений равносильна следующему:
\begin{gather}
    \label{ANOVA}
    \mathrm H_0: \mu_1 = \mu_2 = \ldots = \mu_k.
%    \mathrm H_0^*: \mathbb E (\eta \, | \, \xi = A_1) = \ldots = \mathbb E (\eta \, | \, \xi = A_k).
\end{gather}

Это просто одна гипотеза, здесь нет множественных сравнений. Но пусть эта гипотеза отверглась. Какой вопрос сразу возникает?

\fbox{Возникает вопрос, между какими группами есть разница. }

\medskip
\textbf{Вопрос}: А это сколько нужно сделать сравнений, чему равно число тестов, если групп $k$?

\bigskip
Если не обращать внимание на то, что проверяем сразу много тестов, а просто сравнивать каждое среднее с каждым по обычному t-test, то групповая ошибка будет большой. Такой тест называется \textbf{LSD (Least significant difference)}.

В этом случае тест будет радикальным. Шанс, что гипотеза про равенство средних для групп, где минимальное и максимальное выборочное среднее, отвергнется, гораздо больше, чем для двух случайно взятых групп.

\bigskip
Есть специальные тесты, с помощью которых можно построить точный тест (т.е. $FWER = \alpha$). Для равных объемов выборки в разных группах это \textbf{HSD (Honestly significant difference)} тест Тьюки (Tukey).

\bigskip
Какой бы честный тест не был, но его мощность будет падать при увеличении числа групп. Поэтому всегда стараются уменьшить число одновременно проверяемых тестов. Это можно сделать путем \textbf{плановых сравнений}. Для этого заранее (!), до эксперимента, определяются важные сравнения. Например, есть контрольная группа с плацебо и группы людей, которые принимают разные лекарства. Можно принять решения, что будут сравниваться только с группой плацебо.

Общее число сравнений --- $k(k-1)/2$, а если сравнивать только с контрольной группой, то сравнений $k-1$.

\bigskip
\textbf{Пример-задание}.
Ниже представлены результаты тестов LSD и HSD ($p$-values).

\includegraphics[width=\textwidth]{MT}

На их основе напишите, что (какие сравнения) будет отвергнуто и что не отвергнуто при групповом уровне значимости $\alpha = 0.05$ и как относиться к результатам тестов (a) LSD, (b) HSD, (c) Все сравнения, с поправками Бонферрони (d) Плановые сравнения всех групп с контрольной, с поправками Бонферрони.

Задача --- исследовать влияние картофельного вируса на вес выращенной картошки. FF --- один вирус, СС --- другой, FC --- оба вируса сразу, OO --- здоровый картофель (с ней и надо сравнивать в пункте (d)).


\newpage
\section{Одна группа, $p$ признаков}

Часто хочется проверить гипотезу о равенстве средних всех $p$ признаков. Конечно, для этого нужно, чтобы признаки были на одну тему. Часто это измерение одного и того же в разные моменты времени.

Например, люди сидят на модной диете (когда можно есть сколько угодно, но не всё) и измеряют свой вес раз в месяц.
Гипотеза: такая диета не влияет на вес:
\begin{gather}
    \label{ANOVA}
    \mathrm H_0: \mu_1 = \mu_2 = \ldots = \mu_p,
%    \mathrm H_0^*: \mathbb E (\eta \, | \, \xi = A_1) = \ldots = \mathbb E (\eta \, | \, \xi = A_k).
\end{gather}
где $\mu_i = \E\xi_i$. Выглядит так же, как в случае ANOVA, но там один признак и много групп, а здесь одна группа и много признаков (называется Repeated Measures ANOVA).

\medskip
Идея: свести к тому, что мы уже умеем делать (проверять гипотезу про многомерный вектор из мат.ожиданий критерием типа тест Хотеллинга, обобщение t-test.)

\medskip
Эквивалентная гипотеза:\\
$$H_0: \C \left(
\begin{array}{c}
\mu_1\\
\mu_2\\
\ldots\\
\mu_p
\end{array}
\right)= 0
$$
где требуется, чтобы матрица контрастов $\C$ размера $p-1$ на $p$ была полного ранга и сумма по строкам была равна нулю. Мы проверяем $p-1$ гипотез про то, что разные линейные комбинации (строка матрицы $\C$ состоит из коэффициентов одной линейной комбинации) равны нулю, причем сумма коэффициентов равна нулю.

\medskip
Например, если мы хотим проверить, что $\mu_2-\mu_1=0$, то коэффициенты (строка матрицы контрастов) будут выглядеть как
$(-1,1,0,0,\ldots,0)$.

\bigskip
Разные матрицы контраста --- разные критерии. Критерии отличаются чем? Они более мощные против разных альтернатив (хотя состоятельные против любого неравенства).

\medskip
\textbf{Задание}:
Напишите, какой матрице соответствует случай, когда (a) нам важно сравнить всё с первым моментом времени
(b) важно сделать акцент на динамике, т.е. сравнивать последовательные моменты времени. Пусть число признаков $p=4$.


\appendix
\begin{comment}
\begin{itemize}
\item Casewise --- не рассматривается вся точка, если отсутствует хотя бы
одна координата. Сохраняет свойства корреляционной матрицы (положительная
неопределенность, \ldots{}), потому что считается на одном и том
же вероятностном пространстве.
\item Pairwise --- не рассматривается только пропущенная координата. Свойства
корреляционной матрицы могут не сохранятся. Зато можно извлечь больше
информации из данных.
\end{itemize}
Полезно нормировать данные по строкам. Пусть данные неоднородные ---
два кластера. Внутри каждого кластера нет зависимости, но кластеры
расположены со сдвигом по отношению друг к другу. У индивидов могут
быть разные внутренние шкалы.

Полезно начинать с matrix plot.
\end{comment}

\end{document}
